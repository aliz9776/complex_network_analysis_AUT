{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogi5UJ2Jn10Q",
        "outputId": "6d1f134b-d611-4251-e2af-a5414661a5a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf9QNoPJr7NC",
        "outputId": "f314ed4b-9c1f-4ee3-a7fc-f351e6dbb3da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842425a6-b806-4719-823e-8a8cb9058e9c",
        "id": "GuU5ZiX3SQ6v"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpiB408fj9if",
        "outputId": "0f34f273-8f9d-4c31-c187-7c58320a1097"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "4ynbTYiVoXaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load citeseer**"
      ],
      "metadata": {
        "id": "wEvJbzGIR3Nz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0JY7caPEjvAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3b9594-a3ca-4cf9-ba73-85b64447d0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "\n",
        "\n",
        "citeseer_dataset = Planetoid(root='', name='CiteSeer')\n",
        "\n",
        "citeseer = citeseer_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load CoraFull**"
      ],
      "metadata": {
        "id": "jlV8WuThSBVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import CoraFull\n",
        "\n",
        "\n",
        "root = './CoraFull'\n",
        "\n",
        "\n",
        "CoraFull_dataset = CoraFull(root)\n",
        "\n",
        "CoraFull_dataset.download()\n",
        "CoraFull_dataset.process()\n",
        "\n",
        "CoraFull =  CoraFull_dataset[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppnuLvEY_CVp",
        "outputId": "61280791-f4d3-444e-b586-7c540a5d44ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/abojchevski/graph2gauss/raw/master/data/cora.npz\n",
            "Processing...\n",
            "Done!\n",
            "Using existing file cora.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split dataset**"
      ],
      "metadata": {
        "id": "VGRM6eBU283L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **split cora full**"
      ],
      "metadata": {
        "id": "5spl5OsX_blo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import RandomNodeSplit\n",
        "\n",
        "transform = RandomNodeSplit(num_train_per_class=int(CoraFull.num_nodes * 0.7), num_val=int(CoraFull.num_nodes * 0.1), num_test=int(CoraFull.num_nodes * 0.2))\n",
        "CoraFull = transform(CoraFull)"
      ],
      "metadata": {
        "id": "D3iaM0-22xJD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **split citeseer**"
      ],
      "metadata": {
        "id": "CWzjNh6n_ZzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = RandomNodeSplit(num_train_per_class=int(citeseer.num_nodes * 0.7), num_val=int(citeseer.num_nodes * 0.1), num_test=int(citeseer.num_nodes * 0.2))\n",
        "citeseer = transform(citeseer)"
      ],
      "metadata": {
        "id": "eYYIj54J2MV_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **important functions**"
      ],
      "metadata": {
        "id": "qMtwDUTKC-WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    data = data.to(device)\n",
        "    out = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def validate(model, criterion, data):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
        "    return val_loss.item()\n",
        "\n",
        "def test(model, criterion, data):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        _, pred = torch.max(out, dim=1)\n",
        "        correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "        acc = correct / data.test_mask.sum().item()\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "TXzM1GOJC-WL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **D) implement GAT**"
      ],
      "metadata": {
        "id": "EsQShX44g-V4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Two layer GAT**"
      ],
      "metadata": {
        "id": "nGrbVegunuMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch_geometric.nn import GATConv\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TwoLayerGAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, dropout=0.4, negative_slope=0.2):\n",
        "        super(TwoLayerGAT, self).__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=num_heads, dropout=dropout, negative_slope=negative_slope)\n",
        "        self.gat2 = GATConv(hidden_dim * num_heads, output_dim, heads=1, dropout=dropout, negative_slope=negative_slope)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.gat2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Msj1TMc6oKtQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Corafull dataset(two layer)**"
      ],
      "metadata": {
        "id": "JaW37ET8oL9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "hidden_dims = [16 , 32, 64, 128, 256]\n",
        "\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_num_heads = None\n",
        "\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    model = TwoLayerGAT(input_dim=CoraFull.num_node_features, hidden_dim= hidden_dim , output_dim=CoraFull_dataset.num_classes, num_heads = 4).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Hidden Dimension: {hidden_dim}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Hidden Dimension: {hidden_dim}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_hidden_dim = hidden_dim\n",
        "\n",
        "print('Best Hidden Dimension: {}'.format(best_hidden_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWiBuz88ykr6",
        "outputId": "ab682409-05b7-46c1-b17f-ac15d2760e9b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 16, Epoch: 0, Loss: 4.2540, Validation Loss: 4.0407 \n",
            "Hidden Dimension: 16, Epoch: 1, Loss: 4.0489, Validation Loss: 3.7696 \n",
            "Hidden Dimension: 16, Epoch: 2, Loss: 3.8117, Validation Loss: 3.4700 \n",
            "Hidden Dimension: 16, Epoch: 3, Loss: 3.5546, Validation Loss: 3.1713 \n",
            "Hidden Dimension: 16, Epoch: 4, Loss: 3.3187, Validation Loss: 2.9201 \n",
            "Hidden Dimension: 16, Epoch: 5, Loss: 3.0925, Validation Loss: 2.6899 \n",
            "Hidden Dimension: 16, Epoch: 6, Loss: 2.8957, Validation Loss: 2.4650 \n",
            "Hidden Dimension: 16, Epoch: 7, Loss: 2.7306, Validation Loss: 2.2754 \n",
            "Hidden Dimension: 16, Epoch: 8, Loss: 2.5764, Validation Loss: 2.1198 \n",
            "Hidden Dimension: 16, Epoch: 9, Loss: 2.4124, Validation Loss: 1.9802 \n",
            "Hidden Dimension: 16, Epoch: 10, Loss: 2.2947, Validation Loss: 1.8614 \n",
            "Hidden Dimension: 16, Epoch: 11, Loss: 2.1900, Validation Loss: 1.7619 \n",
            "Hidden Dimension: 16, Epoch: 12, Loss: 2.1035, Validation Loss: 1.6785 \n",
            "Hidden Dimension: 16, Epoch: 13, Loss: 2.0177, Validation Loss: 1.6061 \n",
            "Hidden Dimension: 16, Epoch: 14, Loss: 1.9500, Validation Loss: 1.5429 \n",
            "Hidden Dimension: 16, Epoch: 15, Loss: 1.8830, Validation Loss: 1.4879 \n",
            "Hidden Dimension: 16, Epoch: 16, Loss: 1.8351, Validation Loss: 1.4419 \n",
            "Hidden Dimension: 16, Epoch: 17, Loss: 1.7878, Validation Loss: 1.4014 \n",
            "Hidden Dimension: 16, Epoch: 18, Loss: 1.7269, Validation Loss: 1.3655 \n",
            "Hidden Dimension: 16, Epoch: 19, Loss: 1.6906, Validation Loss: 1.3315 \n",
            "Hidden Dimension: 16, Epoch: 20, Loss: 1.6582, Validation Loss: 1.3025 \n",
            "Hidden Dimension: 16, Epoch: 21, Loss: 1.6240, Validation Loss: 1.2807 \n",
            "Hidden Dimension: 16, Epoch: 22, Loss: 1.5951, Validation Loss: 1.2626 \n",
            "Hidden Dimension: 16, Epoch: 23, Loss: 1.5593, Validation Loss: 1.2453 \n",
            "Hidden Dimension: 16, Epoch: 24, Loss: 1.5310, Validation Loss: 1.2309 \n",
            "Hidden Dimension: 16, Epoch: 25, Loss: 1.5142, Validation Loss: 1.2191 \n",
            "Hidden Dimension: 16, Epoch: 26, Loss: 1.5079, Validation Loss: 1.2080 \n",
            "Hidden Dimension: 16, Epoch: 27, Loss: 1.4685, Validation Loss: 1.1957 \n",
            "Hidden Dimension: 16, Epoch: 28, Loss: 1.4639, Validation Loss: 1.1845 \n",
            "Hidden Dimension: 16, Epoch: 29, Loss: 1.4489, Validation Loss: 1.1747 \n",
            "Hidden Dimension: 16, Epoch: 30, Loss: 1.4382, Validation Loss: 1.1659 \n",
            "Hidden Dimension: 16, Epoch: 31, Loss: 1.4308, Validation Loss: 1.1575 \n",
            "Hidden Dimension: 16, Epoch: 32, Loss: 1.4034, Validation Loss: 1.1497 \n",
            "Hidden Dimension: 16, Epoch: 33, Loss: 1.4066, Validation Loss: 1.1431 \n",
            "Hidden Dimension: 16, Epoch: 34, Loss: 1.3798, Validation Loss: 1.1385 \n",
            "Hidden Dimension: 16, Epoch: 35, Loss: 1.3709, Validation Loss: 1.1347 \n",
            "Hidden Dimension: 16, Epoch: 36, Loss: 1.3623, Validation Loss: 1.1302 \n",
            "Hidden Dimension: 16, Epoch: 37, Loss: 1.3558, Validation Loss: 1.1258 \n",
            "Hidden Dimension: 16, Epoch: 38, Loss: 1.3446, Validation Loss: 1.1210 \n",
            "Hidden Dimension: 16, Epoch: 39, Loss: 1.3434, Validation Loss: 1.1169 \n",
            "Hidden Dimension: 16, Epoch: 40, Loss: 1.3151, Validation Loss: 1.1150 \n",
            "Hidden Dimension: 16, Epoch: 41, Loss: 1.3079, Validation Loss: 1.1126 \n",
            "Hidden Dimension: 16, Epoch: 42, Loss: 1.3083, Validation Loss: 1.1099 \n",
            "Hidden Dimension: 16, Epoch: 43, Loss: 1.2975, Validation Loss: 1.1071 \n",
            "Hidden Dimension: 16, Epoch: 44, Loss: 1.2973, Validation Loss: 1.1036 \n",
            "Hidden Dimension: 16, Epoch: 45, Loss: 1.2809, Validation Loss: 1.0993 \n",
            "Hidden Dimension: 16, Epoch: 46, Loss: 1.2846, Validation Loss: 1.0957 \n",
            "Hidden Dimension: 16, Epoch: 47, Loss: 1.2843, Validation Loss: 1.0934 \n",
            "Hidden Dimension: 16, Epoch: 48, Loss: 1.2663, Validation Loss: 1.0914 \n",
            "Hidden Dimension: 16, Epoch: 49, Loss: 1.2739, Validation Loss: 1.0906 \n",
            "Hidden Dimension: 16, Epoch: 50, Loss: 1.2623, Validation Loss: 1.0883 \n",
            "Hidden Dimension: 16, Epoch: 51, Loss: 1.2420, Validation Loss: 1.0864 \n",
            "Hidden Dimension: 16, Epoch: 52, Loss: 1.2483, Validation Loss: 1.0856 \n",
            "Hidden Dimension: 16, Epoch: 53, Loss: 1.2575, Validation Loss: 1.0834 \n",
            "Hidden Dimension: 16, Epoch: 54, Loss: 1.2409, Validation Loss: 1.0817 \n",
            "Hidden Dimension: 16, Epoch: 55, Loss: 1.2350, Validation Loss: 1.0787 \n",
            "Hidden Dimension: 16, Epoch: 56, Loss: 1.2276, Validation Loss: 1.0753 \n",
            "Hidden Dimension: 16, Epoch: 57, Loss: 1.2196, Validation Loss: 1.0745 \n",
            "Hidden Dimension: 16, Epoch: 58, Loss: 1.2238, Validation Loss: 1.0729 \n",
            "Hidden Dimension: 16, Epoch: 59, Loss: 1.2200, Validation Loss: 1.0728 \n",
            "Hidden Dimension: 16, Epoch: 60, Loss: 1.2188, Validation Loss: 1.0732 \n",
            "Hidden Dimension: 16, Epoch: 61, Loss: 1.2054, Validation Loss: 1.0715 \n",
            "Hidden Dimension: 16, Epoch: 62, Loss: 1.2036, Validation Loss: 1.0696 \n",
            "Hidden Dimension: 16, Epoch: 63, Loss: 1.1950, Validation Loss: 1.0689 \n",
            "Hidden Dimension: 16, Epoch: 64, Loss: 1.1995, Validation Loss: 1.0684 \n",
            "Hidden Dimension: 16, Epoch: 65, Loss: 1.1805, Validation Loss: 1.0665 \n",
            "Hidden Dimension: 16, Epoch: 66, Loss: 1.1960, Validation Loss: 1.0638 \n",
            "Hidden Dimension: 16, Epoch: 67, Loss: 1.1977, Validation Loss: 1.0602 \n",
            "Hidden Dimension: 16, Epoch: 68, Loss: 1.1966, Validation Loss: 1.0570 \n",
            "Hidden Dimension: 16, Epoch: 69, Loss: 1.1688, Validation Loss: 1.0584 \n",
            "Hidden Dimension: 16, Epoch: 70, Loss: 1.1879, Validation Loss: 1.0614 \n",
            "Hidden Dimension: 16, Epoch: 71, Loss: 1.1837, Validation Loss: 1.0612 \n",
            "Hidden Dimension: 16, Epoch: 72, Loss: 1.1822, Validation Loss: 1.0614 \n",
            "Early stopping at epoch 73 \n",
            "\n",
            " Hidden Dimension: 16, Validation Loss: 1.0632 \n",
            "\n",
            "Hidden Dimension: 32, Epoch: 0, Loss: 4.2550, Validation Loss: 3.8171 \n",
            "Hidden Dimension: 32, Epoch: 1, Loss: 3.8545, Validation Loss: 3.3061 \n",
            "Hidden Dimension: 32, Epoch: 2, Loss: 3.4637, Validation Loss: 2.9582 \n",
            "Hidden Dimension: 32, Epoch: 3, Loss: 3.1065, Validation Loss: 2.6045 \n",
            "Hidden Dimension: 32, Epoch: 4, Loss: 2.7741, Validation Loss: 2.2858 \n",
            "Hidden Dimension: 32, Epoch: 5, Loss: 2.5271, Validation Loss: 2.0307 \n",
            "Hidden Dimension: 32, Epoch: 6, Loss: 2.3321, Validation Loss: 1.8434 \n",
            "Hidden Dimension: 32, Epoch: 7, Loss: 2.1570, Validation Loss: 1.6907 \n",
            "Hidden Dimension: 32, Epoch: 8, Loss: 2.0053, Validation Loss: 1.5739 \n",
            "Hidden Dimension: 32, Epoch: 9, Loss: 1.8890, Validation Loss: 1.4887 \n",
            "Hidden Dimension: 32, Epoch: 10, Loss: 1.7854, Validation Loss: 1.4193 \n",
            "Hidden Dimension: 32, Epoch: 11, Loss: 1.7023, Validation Loss: 1.3598 \n",
            "Hidden Dimension: 32, Epoch: 12, Loss: 1.6507, Validation Loss: 1.3072 \n",
            "Hidden Dimension: 32, Epoch: 13, Loss: 1.5613, Validation Loss: 1.2662 \n",
            "Hidden Dimension: 32, Epoch: 14, Loss: 1.5189, Validation Loss: 1.2321 \n",
            "Hidden Dimension: 32, Epoch: 15, Loss: 1.4873, Validation Loss: 1.2041 \n",
            "Hidden Dimension: 32, Epoch: 16, Loss: 1.4353, Validation Loss: 1.1825 \n",
            "Hidden Dimension: 32, Epoch: 17, Loss: 1.4132, Validation Loss: 1.1639 \n",
            "Hidden Dimension: 32, Epoch: 18, Loss: 1.3701, Validation Loss: 1.1481 \n",
            "Hidden Dimension: 32, Epoch: 19, Loss: 1.3404, Validation Loss: 1.1338 \n",
            "Hidden Dimension: 32, Epoch: 20, Loss: 1.3311, Validation Loss: 1.1219 \n",
            "Hidden Dimension: 32, Epoch: 21, Loss: 1.3011, Validation Loss: 1.1114 \n",
            "Hidden Dimension: 32, Epoch: 22, Loss: 1.2822, Validation Loss: 1.1025 \n",
            "Hidden Dimension: 32, Epoch: 23, Loss: 1.2780, Validation Loss: 1.0973 \n",
            "Hidden Dimension: 32, Epoch: 24, Loss: 1.2524, Validation Loss: 1.0893 \n",
            "Hidden Dimension: 32, Epoch: 25, Loss: 1.2343, Validation Loss: 1.0833 \n",
            "Hidden Dimension: 32, Epoch: 26, Loss: 1.2195, Validation Loss: 1.0824 \n",
            "Hidden Dimension: 32, Epoch: 27, Loss: 1.2226, Validation Loss: 1.0782 \n",
            "Hidden Dimension: 32, Epoch: 28, Loss: 1.2057, Validation Loss: 1.0740 \n",
            "Hidden Dimension: 32, Epoch: 29, Loss: 1.1897, Validation Loss: 1.0716 \n",
            "Hidden Dimension: 32, Epoch: 30, Loss: 1.1823, Validation Loss: 1.0696 \n",
            "Hidden Dimension: 32, Epoch: 31, Loss: 1.1927, Validation Loss: 1.0689 \n",
            "Hidden Dimension: 32, Epoch: 32, Loss: 1.1758, Validation Loss: 1.0670 \n",
            "Hidden Dimension: 32, Epoch: 33, Loss: 1.1655, Validation Loss: 1.0654 \n",
            "Hidden Dimension: 32, Epoch: 34, Loss: 1.1457, Validation Loss: 1.0631 \n",
            "Hidden Dimension: 32, Epoch: 35, Loss: 1.1601, Validation Loss: 1.0622 \n",
            "Hidden Dimension: 32, Epoch: 36, Loss: 1.1332, Validation Loss: 1.0592 \n",
            "Hidden Dimension: 32, Epoch: 37, Loss: 1.1351, Validation Loss: 1.0564 \n",
            "Hidden Dimension: 32, Epoch: 38, Loss: 1.1260, Validation Loss: 1.0522 \n",
            "Hidden Dimension: 32, Epoch: 39, Loss: 1.1256, Validation Loss: 1.0509 \n",
            "Hidden Dimension: 32, Epoch: 40, Loss: 1.1135, Validation Loss: 1.0500 \n",
            "Hidden Dimension: 32, Epoch: 41, Loss: 1.1175, Validation Loss: 1.0490 \n",
            "Hidden Dimension: 32, Epoch: 42, Loss: 1.0935, Validation Loss: 1.0478 \n",
            "Hidden Dimension: 32, Epoch: 43, Loss: 1.1044, Validation Loss: 1.0476 \n",
            "Hidden Dimension: 32, Epoch: 44, Loss: 1.0954, Validation Loss: 1.0456 \n",
            "Hidden Dimension: 32, Epoch: 45, Loss: 1.0960, Validation Loss: 1.0451 \n",
            "Hidden Dimension: 32, Epoch: 46, Loss: 1.0873, Validation Loss: 1.0439 \n",
            "Hidden Dimension: 32, Epoch: 47, Loss: 1.0631, Validation Loss: 1.0421 \n",
            "Hidden Dimension: 32, Epoch: 48, Loss: 1.0676, Validation Loss: 1.0432 \n",
            "Hidden Dimension: 32, Epoch: 49, Loss: 1.0578, Validation Loss: 1.0438 \n",
            "Hidden Dimension: 32, Epoch: 50, Loss: 1.0545, Validation Loss: 1.0421 \n",
            "Hidden Dimension: 32, Epoch: 51, Loss: 1.0576, Validation Loss: 1.0402 \n",
            "Hidden Dimension: 32, Epoch: 52, Loss: 1.0621, Validation Loss: 1.0409 \n",
            "Hidden Dimension: 32, Epoch: 53, Loss: 1.0491, Validation Loss: 1.0405 \n",
            "Hidden Dimension: 32, Epoch: 54, Loss: 1.0616, Validation Loss: 1.0370 \n",
            "Hidden Dimension: 32, Epoch: 55, Loss: 1.0322, Validation Loss: 1.0373 \n",
            "Hidden Dimension: 32, Epoch: 56, Loss: 1.0419, Validation Loss: 1.0386 \n",
            "Hidden Dimension: 32, Epoch: 57, Loss: 1.0335, Validation Loss: 1.0380 \n",
            "Hidden Dimension: 32, Epoch: 58, Loss: 1.0365, Validation Loss: 1.0373 \n",
            "Early stopping at epoch 59 \n",
            "\n",
            " Hidden Dimension: 32, Validation Loss: 1.0392 \n",
            "\n",
            "Hidden Dimension: 64, Epoch: 0, Loss: 4.2499, Validation Loss: 3.6503 \n",
            "Hidden Dimension: 64, Epoch: 1, Loss: 3.7017, Validation Loss: 3.0772 \n",
            "Hidden Dimension: 64, Epoch: 2, Loss: 3.1648, Validation Loss: 2.5544 \n",
            "Hidden Dimension: 64, Epoch: 3, Loss: 2.7033, Validation Loss: 2.1132 \n",
            "Hidden Dimension: 64, Epoch: 4, Loss: 2.3065, Validation Loss: 1.8184 \n",
            "Hidden Dimension: 64, Epoch: 5, Loss: 2.0330, Validation Loss: 1.6239 \n",
            "Hidden Dimension: 64, Epoch: 6, Loss: 1.8374, Validation Loss: 1.4775 \n",
            "Hidden Dimension: 64, Epoch: 7, Loss: 1.6978, Validation Loss: 1.3840 \n",
            "Hidden Dimension: 64, Epoch: 8, Loss: 1.5954, Validation Loss: 1.3105 \n",
            "Hidden Dimension: 64, Epoch: 9, Loss: 1.4945, Validation Loss: 1.2451 \n",
            "Hidden Dimension: 64, Epoch: 10, Loss: 1.4228, Validation Loss: 1.2068 \n",
            "Hidden Dimension: 64, Epoch: 11, Loss: 1.3586, Validation Loss: 1.1811 \n",
            "Hidden Dimension: 64, Epoch: 12, Loss: 1.3393, Validation Loss: 1.1582 \n",
            "Hidden Dimension: 64, Epoch: 13, Loss: 1.2733, Validation Loss: 1.1361 \n",
            "Hidden Dimension: 64, Epoch: 14, Loss: 1.2404, Validation Loss: 1.1130 \n",
            "Hidden Dimension: 64, Epoch: 15, Loss: 1.2134, Validation Loss: 1.0912 \n",
            "Hidden Dimension: 64, Epoch: 16, Loss: 1.1704, Validation Loss: 1.0745 \n",
            "Hidden Dimension: 64, Epoch: 17, Loss: 1.1806, Validation Loss: 1.0626 \n",
            "Hidden Dimension: 64, Epoch: 18, Loss: 1.1318, Validation Loss: 1.0537 \n",
            "Hidden Dimension: 64, Epoch: 19, Loss: 1.1191, Validation Loss: 1.0506 \n",
            "Hidden Dimension: 64, Epoch: 20, Loss: 1.1122, Validation Loss: 1.0472 \n",
            "Hidden Dimension: 64, Epoch: 21, Loss: 1.0977, Validation Loss: 1.0448 \n",
            "Hidden Dimension: 64, Epoch: 22, Loss: 1.0762, Validation Loss: 1.0441 \n",
            "Hidden Dimension: 64, Epoch: 23, Loss: 1.0741, Validation Loss: 1.0399 \n",
            "Hidden Dimension: 64, Epoch: 24, Loss: 1.0517, Validation Loss: 1.0396 \n",
            "Hidden Dimension: 64, Epoch: 25, Loss: 1.0445, Validation Loss: 1.0391 \n",
            "Hidden Dimension: 64, Epoch: 26, Loss: 1.0513, Validation Loss: 1.0369 \n",
            "Hidden Dimension: 64, Epoch: 27, Loss: 1.0270, Validation Loss: 1.0366 \n",
            "Hidden Dimension: 64, Epoch: 28, Loss: 1.0161, Validation Loss: 1.0346 \n",
            "Hidden Dimension: 64, Epoch: 29, Loss: 1.0179, Validation Loss: 1.0328 \n",
            "Hidden Dimension: 64, Epoch: 30, Loss: 1.0112, Validation Loss: 1.0337 \n",
            "Hidden Dimension: 64, Epoch: 31, Loss: 1.0076, Validation Loss: 1.0368 \n",
            "Hidden Dimension: 64, Epoch: 32, Loss: 0.9987, Validation Loss: 1.0345 \n",
            "Hidden Dimension: 64, Epoch: 33, Loss: 0.9932, Validation Loss: 1.0293 \n",
            "Hidden Dimension: 64, Epoch: 34, Loss: 0.9802, Validation Loss: 1.0290 \n",
            "Hidden Dimension: 64, Epoch: 35, Loss: 0.9931, Validation Loss: 1.0264 \n",
            "Hidden Dimension: 64, Epoch: 36, Loss: 0.9924, Validation Loss: 1.0295 \n",
            "Hidden Dimension: 64, Epoch: 37, Loss: 0.9868, Validation Loss: 1.0354 \n",
            "Hidden Dimension: 64, Epoch: 38, Loss: 0.9644, Validation Loss: 1.0376 \n",
            "Hidden Dimension: 64, Epoch: 39, Loss: 0.9478, Validation Loss: 1.0373 \n",
            "Early stopping at epoch 40 \n",
            "\n",
            " Hidden Dimension: 64, Validation Loss: 1.0356 \n",
            "\n",
            "Hidden Dimension: 128, Epoch: 0, Loss: 4.2505, Validation Loss: 3.4643 \n",
            "Hidden Dimension: 128, Epoch: 1, Loss: 3.5768, Validation Loss: 3.0225 \n",
            "Hidden Dimension: 128, Epoch: 2, Loss: 3.0597, Validation Loss: 2.3772 \n",
            "Hidden Dimension: 128, Epoch: 3, Loss: 2.4773, Validation Loss: 1.9040 \n",
            "Hidden Dimension: 128, Epoch: 4, Loss: 2.1211, Validation Loss: 1.6400 \n",
            "Hidden Dimension: 128, Epoch: 5, Loss: 1.8425, Validation Loss: 1.4596 \n",
            "Hidden Dimension: 128, Epoch: 6, Loss: 1.6651, Validation Loss: 1.3722 \n",
            "Hidden Dimension: 128, Epoch: 7, Loss: 1.5489, Validation Loss: 1.2892 \n",
            "Hidden Dimension: 128, Epoch: 8, Loss: 1.4404, Validation Loss: 1.2329 \n",
            "Hidden Dimension: 128, Epoch: 9, Loss: 1.3482, Validation Loss: 1.2042 \n",
            "Hidden Dimension: 128, Epoch: 10, Loss: 1.3178, Validation Loss: 1.1581 \n",
            "Hidden Dimension: 128, Epoch: 11, Loss: 1.2345, Validation Loss: 1.1309 \n",
            "Hidden Dimension: 128, Epoch: 12, Loss: 1.1929, Validation Loss: 1.1144 \n",
            "Hidden Dimension: 128, Epoch: 13, Loss: 1.1626, Validation Loss: 1.1046 \n",
            "Hidden Dimension: 128, Epoch: 14, Loss: 1.1264, Validation Loss: 1.0980 \n",
            "Hidden Dimension: 128, Epoch: 15, Loss: 1.1100, Validation Loss: 1.0810 \n",
            "Hidden Dimension: 128, Epoch: 16, Loss: 1.0674, Validation Loss: 1.0671 \n",
            "Hidden Dimension: 128, Epoch: 17, Loss: 1.0687, Validation Loss: 1.0563 \n",
            "Hidden Dimension: 128, Epoch: 18, Loss: 1.0392, Validation Loss: 1.0564 \n",
            "Hidden Dimension: 128, Epoch: 19, Loss: 1.0301, Validation Loss: 1.0675 \n",
            "Hidden Dimension: 128, Epoch: 20, Loss: 1.0310, Validation Loss: 1.0629 \n",
            "Hidden Dimension: 128, Epoch: 21, Loss: 1.0099, Validation Loss: 1.0514 \n",
            "Hidden Dimension: 128, Epoch: 22, Loss: 1.0140, Validation Loss: 1.0442 \n",
            "Hidden Dimension: 128, Epoch: 23, Loss: 0.9926, Validation Loss: 1.0422 \n",
            "Hidden Dimension: 128, Epoch: 24, Loss: 1.0081, Validation Loss: 1.0459 \n",
            "Hidden Dimension: 128, Epoch: 25, Loss: 0.9731, Validation Loss: 1.0395 \n",
            "Hidden Dimension: 128, Epoch: 26, Loss: 0.9676, Validation Loss: 1.0368 \n",
            "Hidden Dimension: 128, Epoch: 27, Loss: 0.9686, Validation Loss: 1.0367 \n",
            "Hidden Dimension: 128, Epoch: 28, Loss: 0.9494, Validation Loss: 1.0400 \n",
            "Hidden Dimension: 128, Epoch: 29, Loss: 0.9533, Validation Loss: 1.0442 \n",
            "Hidden Dimension: 128, Epoch: 30, Loss: 0.9444, Validation Loss: 1.0420 \n",
            "Early stopping at epoch 31 \n",
            "\n",
            " Hidden Dimension: 128, Validation Loss: 1.0406 \n",
            "\n",
            "Hidden Dimension: 256, Epoch: 0, Loss: 4.2569, Validation Loss: 3.5249 \n",
            "Hidden Dimension: 256, Epoch: 1, Loss: 3.6569, Validation Loss: 2.9790 \n",
            "Hidden Dimension: 256, Epoch: 2, Loss: 3.0136, Validation Loss: 2.3854 \n",
            "Hidden Dimension: 256, Epoch: 3, Loss: 2.4602, Validation Loss: 1.9457 \n",
            "Hidden Dimension: 256, Epoch: 4, Loss: 2.1281, Validation Loss: 1.8257 \n",
            "Hidden Dimension: 256, Epoch: 5, Loss: 1.9374, Validation Loss: 1.6786 \n",
            "Hidden Dimension: 256, Epoch: 6, Loss: 1.7915, Validation Loss: 1.5250 \n",
            "Hidden Dimension: 256, Epoch: 7, Loss: 1.6317, Validation Loss: 1.4029 \n",
            "Hidden Dimension: 256, Epoch: 8, Loss: 1.5011, Validation Loss: 1.3129 \n",
            "Hidden Dimension: 256, Epoch: 9, Loss: 1.4097, Validation Loss: 1.2803 \n",
            "Hidden Dimension: 256, Epoch: 10, Loss: 1.3538, Validation Loss: 1.2353 \n",
            "Hidden Dimension: 256, Epoch: 11, Loss: 1.2968, Validation Loss: 1.1760 \n",
            "Hidden Dimension: 256, Epoch: 12, Loss: 1.2292, Validation Loss: 1.1502 \n",
            "Hidden Dimension: 256, Epoch: 13, Loss: 1.1983, Validation Loss: 1.1424 \n",
            "Hidden Dimension: 256, Epoch: 14, Loss: 1.1850, Validation Loss: 1.1176 \n",
            "Hidden Dimension: 256, Epoch: 15, Loss: 1.1499, Validation Loss: 1.0877 \n",
            "Hidden Dimension: 256, Epoch: 16, Loss: 1.1115, Validation Loss: 1.0799 \n",
            "Hidden Dimension: 256, Epoch: 17, Loss: 1.0879, Validation Loss: 1.0834 \n",
            "Hidden Dimension: 256, Epoch: 18, Loss: 1.0910, Validation Loss: 1.0727 \n",
            "Hidden Dimension: 256, Epoch: 19, Loss: 1.0672, Validation Loss: 1.0554 \n",
            "Hidden Dimension: 256, Epoch: 20, Loss: 1.0217, Validation Loss: 1.0551 \n",
            "Hidden Dimension: 256, Epoch: 21, Loss: 1.0322, Validation Loss: 1.0542 \n",
            "Hidden Dimension: 256, Epoch: 22, Loss: 1.0310, Validation Loss: 1.0404 \n",
            "Hidden Dimension: 256, Epoch: 23, Loss: 0.9925, Validation Loss: 1.0346 \n",
            "Hidden Dimension: 256, Epoch: 24, Loss: 0.9790, Validation Loss: 1.0375 \n",
            "Hidden Dimension: 256, Epoch: 25, Loss: 0.9759, Validation Loss: 1.0423 \n",
            "Hidden Dimension: 256, Epoch: 26, Loss: 0.9593, Validation Loss: 1.0453 \n",
            "Hidden Dimension: 256, Epoch: 27, Loss: 0.9551, Validation Loss: 1.0363 \n",
            "Early stopping at epoch 28 \n",
            "\n",
            " Hidden Dimension: 256, Validation Loss: 1.0367 \n",
            "\n",
            "Best Hidden Dimension: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  cora full dataset(two layer)**"
      ],
      "metadata": {
        "id": "m8hBVxjgQEIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = TwoLayerGAT(input_dim= CoraFull.num_node_features ,hidden_dim=best_hidden_dim , output_dim= CoraFull_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ac89a6-0e6e-465e-8b83-d5afbd4333eb",
        "id": "6HpvE21QQEIk"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 4.2531, Validation Loss: 4.0613 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 4.0674, Validation Loss: 3.7771 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 3.8324, Validation Loss: 3.4404 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 3.5733, Validation Loss: 3.1708 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 3.3280, Validation Loss: 2.8764 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 3.1077, Validation Loss: 2.6519 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 2.9314, Validation Loss: 2.4978 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 2.7960, Validation Loss: 2.3297 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 2.6227, Validation Loss: 2.1623 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 2.5126, Validation Loss: 2.0351 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 2.4262, Validation Loss: 1.9448 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 2.3045, Validation Loss: 1.8551 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 2.2161, Validation Loss: 1.7679 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 2.1475, Validation Loss: 1.6957 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 2.1021, Validation Loss: 1.6389 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 2.0245, Validation Loss: 1.5871 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 1.9752, Validation Loss: 1.5353 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 1.9137, Validation Loss: 1.4831 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 1.8857, Validation Loss: 1.4408 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 1.8185, Validation Loss: 1.4072 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 1.7951, Validation Loss: 1.3782 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 1.7448, Validation Loss: 1.3557 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 1.7142, Validation Loss: 1.3337 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 1.7224, Validation Loss: 1.3116 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 1.6650, Validation Loss: 1.2902 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 1.6335, Validation Loss: 1.2706 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 1.6365, Validation Loss: 1.2536 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 1.6010, Validation Loss: 1.2396 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 1.5842, Validation Loss: 1.2285 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 1.5596, Validation Loss: 1.2197 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 1.5454, Validation Loss: 1.2104 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 1.5361, Validation Loss: 1.1983 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 1.5127, Validation Loss: 1.1853 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 1.5140, Validation Loss: 1.1740 \n",
            "Attention Heads: 1, Epoch: 34, Loss: 1.4816, Validation Loss: 1.1643 \n",
            "Attention Heads: 1, Epoch: 35, Loss: 1.4713, Validation Loss: 1.1564 \n",
            "Attention Heads: 1, Epoch: 36, Loss: 1.4751, Validation Loss: 1.1508 \n",
            "Attention Heads: 1, Epoch: 37, Loss: 1.4479, Validation Loss: 1.1463 \n",
            "Attention Heads: 1, Epoch: 38, Loss: 1.4527, Validation Loss: 1.1426 \n",
            "Attention Heads: 1, Epoch: 39, Loss: 1.4404, Validation Loss: 1.1366 \n",
            "Attention Heads: 1, Epoch: 40, Loss: 1.4313, Validation Loss: 1.1328 \n",
            "Attention Heads: 1, Epoch: 41, Loss: 1.4111, Validation Loss: 1.1303 \n",
            "Attention Heads: 1, Epoch: 42, Loss: 1.4170, Validation Loss: 1.1286 \n",
            "Attention Heads: 1, Epoch: 43, Loss: 1.4057, Validation Loss: 1.1263 \n",
            "Attention Heads: 1, Epoch: 44, Loss: 1.4034, Validation Loss: 1.1233 \n",
            "Attention Heads: 1, Epoch: 45, Loss: 1.4039, Validation Loss: 1.1204 \n",
            "Attention Heads: 1, Epoch: 46, Loss: 1.3772, Validation Loss: 1.1175 \n",
            "Attention Heads: 1, Epoch: 47, Loss: 1.3989, Validation Loss: 1.1159 \n",
            "Attention Heads: 1, Epoch: 48, Loss: 1.3808, Validation Loss: 1.1121 \n",
            "Attention Heads: 1, Epoch: 49, Loss: 1.3765, Validation Loss: 1.1080 \n",
            "Attention Heads: 1, Epoch: 50, Loss: 1.3794, Validation Loss: 1.1044 \n",
            "Attention Heads: 1, Epoch: 51, Loss: 1.3659, Validation Loss: 1.1005 \n",
            "Attention Heads: 1, Epoch: 52, Loss: 1.3750, Validation Loss: 1.0984 \n",
            "Attention Heads: 1, Epoch: 53, Loss: 1.3680, Validation Loss: 1.0978 \n",
            "Attention Heads: 1, Epoch: 54, Loss: 1.3435, Validation Loss: 1.0972 \n",
            "Attention Heads: 1, Epoch: 55, Loss: 1.3563, Validation Loss: 1.0949 \n",
            "Attention Heads: 1, Epoch: 56, Loss: 1.3430, Validation Loss: 1.0936 \n",
            "Attention Heads: 1, Epoch: 57, Loss: 1.3449, Validation Loss: 1.0921 \n",
            "Attention Heads: 1, Epoch: 58, Loss: 1.3458, Validation Loss: 1.0901 \n",
            "Attention Heads: 1, Epoch: 59, Loss: 1.3369, Validation Loss: 1.0899 \n",
            "Attention Heads: 1, Epoch: 60, Loss: 1.3266, Validation Loss: 1.0902 \n",
            "Attention Heads: 1, Epoch: 61, Loss: 1.3234, Validation Loss: 1.0898 \n",
            "Attention Heads: 1, Epoch: 62, Loss: 1.3160, Validation Loss: 1.0884 \n",
            "Attention Heads: 1, Epoch: 63, Loss: 1.3285, Validation Loss: 1.0847 \n",
            "Attention Heads: 1, Epoch: 64, Loss: 1.3216, Validation Loss: 1.0797 \n",
            "Attention Heads: 1, Epoch: 65, Loss: 1.3148, Validation Loss: 1.0752 \n",
            "Attention Heads: 1, Epoch: 66, Loss: 1.3248, Validation Loss: 1.0727 \n",
            "Attention Heads: 1, Epoch: 67, Loss: 1.3136, Validation Loss: 1.0724 \n",
            "Attention Heads: 1, Epoch: 68, Loss: 1.2940, Validation Loss: 1.0741 \n",
            "Attention Heads: 1, Epoch: 69, Loss: 1.2982, Validation Loss: 1.0779 \n",
            "Attention Heads: 1, Epoch: 70, Loss: 1.2962, Validation Loss: 1.0800 \n",
            "Early stopping at epoch 71 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 1.0821 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 4.2552, Validation Loss: 3.8162 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 3.8546, Validation Loss: 3.3039 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 3.5205, Validation Loss: 3.0310 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 3.1745, Validation Loss: 2.7302 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 2.9246, Validation Loss: 2.4177 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 2.6590, Validation Loss: 2.1497 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 2.4592, Validation Loss: 1.9545 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 2.2630, Validation Loss: 1.8195 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 2.1324, Validation Loss: 1.7070 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 2.0229, Validation Loss: 1.6029 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 1.9323, Validation Loss: 1.5216 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 1.8311, Validation Loss: 1.4608 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 1.7437, Validation Loss: 1.4093 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 1.7012, Validation Loss: 1.3599 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 1.6328, Validation Loss: 1.3119 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 1.5963, Validation Loss: 1.2724 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 1.5244, Validation Loss: 1.2401 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 1.4970, Validation Loss: 1.2129 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 1.4761, Validation Loss: 1.1928 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 1.4333, Validation Loss: 1.1758 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 1.4168, Validation Loss: 1.1637 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 1.3978, Validation Loss: 1.1530 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 1.3625, Validation Loss: 1.1427 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 1.3359, Validation Loss: 1.1318 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 1.3384, Validation Loss: 1.1197 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 1.3095, Validation Loss: 1.1109 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 1.3185, Validation Loss: 1.1057 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 1.2798, Validation Loss: 1.1007 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 1.2673, Validation Loss: 1.0939 \n",
            "Attention Heads: 2, Epoch: 29, Loss: 1.2656, Validation Loss: 1.0887 \n",
            "Attention Heads: 2, Epoch: 30, Loss: 1.2659, Validation Loss: 1.0840 \n",
            "Attention Heads: 2, Epoch: 31, Loss: 1.2442, Validation Loss: 1.0794 \n",
            "Attention Heads: 2, Epoch: 32, Loss: 1.2344, Validation Loss: 1.0760 \n",
            "Attention Heads: 2, Epoch: 33, Loss: 1.2181, Validation Loss: 1.0722 \n",
            "Attention Heads: 2, Epoch: 34, Loss: 1.2181, Validation Loss: 1.0690 \n",
            "Attention Heads: 2, Epoch: 35, Loss: 1.1967, Validation Loss: 1.0661 \n",
            "Attention Heads: 2, Epoch: 36, Loss: 1.2020, Validation Loss: 1.0644 \n",
            "Attention Heads: 2, Epoch: 37, Loss: 1.2046, Validation Loss: 1.0649 \n",
            "Attention Heads: 2, Epoch: 38, Loss: 1.1919, Validation Loss: 1.0646 \n",
            "Attention Heads: 2, Epoch: 39, Loss: 1.1797, Validation Loss: 1.0651 \n",
            "Attention Heads: 2, Epoch: 40, Loss: 1.1820, Validation Loss: 1.0670 \n",
            "Early stopping at epoch 41 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 1.0641 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 4.2492, Validation Loss: 3.6439 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 3.6920, Validation Loss: 3.0711 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 3.1551, Validation Loss: 2.5331 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 2.6888, Validation Loss: 2.1228 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 2.3128, Validation Loss: 1.8139 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 2.0793, Validation Loss: 1.6348 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 1.8725, Validation Loss: 1.4979 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 1.7167, Validation Loss: 1.3945 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 1.6126, Validation Loss: 1.3140 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 1.5238, Validation Loss: 1.2526 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 1.4439, Validation Loss: 1.2096 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 1.3854, Validation Loss: 1.1753 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 1.3402, Validation Loss: 1.1513 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 1.2884, Validation Loss: 1.1305 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 1.2695, Validation Loss: 1.1065 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 1.2196, Validation Loss: 1.0887 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 1.1805, Validation Loss: 1.0782 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 1.1546, Validation Loss: 1.0685 \n",
            "Attention Heads: 4, Epoch: 18, Loss: 1.1440, Validation Loss: 1.0629 \n",
            "Attention Heads: 4, Epoch: 19, Loss: 1.1280, Validation Loss: 1.0593 \n",
            "Attention Heads: 4, Epoch: 20, Loss: 1.1131, Validation Loss: 1.0543 \n",
            "Attention Heads: 4, Epoch: 21, Loss: 1.0812, Validation Loss: 1.0509 \n",
            "Attention Heads: 4, Epoch: 22, Loss: 1.0878, Validation Loss: 1.0507 \n",
            "Attention Heads: 4, Epoch: 23, Loss: 1.0764, Validation Loss: 1.0500 \n",
            "Attention Heads: 4, Epoch: 24, Loss: 1.0601, Validation Loss: 1.0515 \n",
            "Attention Heads: 4, Epoch: 25, Loss: 1.0664, Validation Loss: 1.0492 \n",
            "Attention Heads: 4, Epoch: 26, Loss: 1.0427, Validation Loss: 1.0461 \n",
            "Attention Heads: 4, Epoch: 27, Loss: 1.0287, Validation Loss: 1.0404 \n",
            "Attention Heads: 4, Epoch: 28, Loss: 1.0357, Validation Loss: 1.0339 \n",
            "Attention Heads: 4, Epoch: 29, Loss: 1.0373, Validation Loss: 1.0308 \n",
            "Attention Heads: 4, Epoch: 30, Loss: 1.0232, Validation Loss: 1.0276 \n",
            "Attention Heads: 4, Epoch: 31, Loss: 1.0016, Validation Loss: 1.0249 \n",
            "Attention Heads: 4, Epoch: 32, Loss: 1.0021, Validation Loss: 1.0244 \n",
            "Attention Heads: 4, Epoch: 33, Loss: 1.0076, Validation Loss: 1.0263 \n",
            "Attention Heads: 4, Epoch: 34, Loss: 1.0065, Validation Loss: 1.0331 \n",
            "Attention Heads: 4, Epoch: 35, Loss: 0.9791, Validation Loss: 1.0341 \n",
            "Early stopping at epoch 36 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 1.0336 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 4.2513, Validation Loss: 3.4824 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 3.5872, Validation Loss: 3.0409 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 3.1023, Validation Loss: 2.4380 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 2.5097, Validation Loss: 1.9658 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 2.1476, Validation Loss: 1.6472 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 1.8333, Validation Loss: 1.4702 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 1.6594, Validation Loss: 1.3655 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 1.5278, Validation Loss: 1.2758 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 1.4328, Validation Loss: 1.2235 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 1.3580, Validation Loss: 1.1892 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 1.3043, Validation Loss: 1.1521 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 1.2299, Validation Loss: 1.1316 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 1.1777, Validation Loss: 1.1127 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 1.1455, Validation Loss: 1.0948 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 1.1152, Validation Loss: 1.0838 \n",
            "Attention Heads: 8, Epoch: 15, Loss: 1.0897, Validation Loss: 1.0606 \n",
            "Attention Heads: 8, Epoch: 16, Loss: 1.0592, Validation Loss: 1.0505 \n",
            "Attention Heads: 8, Epoch: 17, Loss: 1.0340, Validation Loss: 1.0496 \n",
            "Attention Heads: 8, Epoch: 18, Loss: 1.0287, Validation Loss: 1.0500 \n",
            "Attention Heads: 8, Epoch: 19, Loss: 1.0216, Validation Loss: 1.0542 \n",
            "Attention Heads: 8, Epoch: 20, Loss: 0.9924, Validation Loss: 1.0479 \n",
            "Attention Heads: 8, Epoch: 21, Loss: 0.9769, Validation Loss: 1.0439 \n",
            "Attention Heads: 8, Epoch: 22, Loss: 0.9759, Validation Loss: 1.0364 \n",
            "Attention Heads: 8, Epoch: 23, Loss: 0.9706, Validation Loss: 1.0315 \n",
            "Attention Heads: 8, Epoch: 24, Loss: 0.9607, Validation Loss: 1.0356 \n",
            "Attention Heads: 8, Epoch: 25, Loss: 0.9459, Validation Loss: 1.0372 \n",
            "Attention Heads: 8, Epoch: 26, Loss: 0.9567, Validation Loss: 1.0361 \n",
            "Attention Heads: 8, Epoch: 27, Loss: 0.9246, Validation Loss: 1.0378 \n",
            "Early stopping at epoch 28 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 1.0405 \n",
            "\n",
            "Best Number of Attention Heads: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = TwoLayerGAT(input_dim=CoraFull.num_node_features, hidden_dim=best_hidden_dim , output_dim=CoraFull_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, CoraFull)\n",
        "    val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, CoraFull)\n",
        "print(f'Test Accuracy(two layer) on CoraFull dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3xkIPnbUA28",
        "outputId": "b5f16189-2ec7-4c87-a0d1-97549c5408cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 42 \n",
            "\n",
            "Test Accuracy(two layer) on CoraFull dataset: 0.7135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Citeseer dataset(two layer)**"
      ],
      "metadata": {
        "id": "0Bt2J5BQ0aNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "hidden_dims = [16 , 32, 64, 128, 256]\n",
        "\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_num_heads = None\n",
        "\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    model = TwoLayerGAT(input_dim=citeseer.num_node_features, hidden_dim= hidden_dim , output_dim=citeseer_dataset.num_classes, num_heads = 4).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Hidden Dimension: {hidden_dim}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Hidden Dimension: {hidden_dim}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_hidden_dim = hidden_dim\n",
        "\n",
        "print('Best Hidden Dimension: {}'.format(best_hidden_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef28b696-f7d2-4b71-9252-1cc61f50874e",
        "id": "UvNF8_rqZM-e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 16, Epoch: 0, Loss: 1.7958, Validation Loss: 1.5136 \n",
            "Hidden Dimension: 16, Epoch: 1, Loss: 1.5056, Validation Loss: 1.2191 \n",
            "Hidden Dimension: 16, Epoch: 2, Loss: 1.2490, Validation Loss: 0.9789 \n",
            "Hidden Dimension: 16, Epoch: 3, Loss: 1.0566, Validation Loss: 0.8304 \n",
            "Hidden Dimension: 16, Epoch: 4, Loss: 0.9464, Validation Loss: 0.7518 \n",
            "Hidden Dimension: 16, Epoch: 5, Loss: 0.8797, Validation Loss: 0.7183 \n",
            "Hidden Dimension: 16, Epoch: 6, Loss: 0.8223, Validation Loss: 0.6991 \n",
            "Hidden Dimension: 16, Epoch: 7, Loss: 0.8397, Validation Loss: 0.6819 \n",
            "Hidden Dimension: 16, Epoch: 8, Loss: 0.7556, Validation Loss: 0.6690 \n",
            "Hidden Dimension: 16, Epoch: 9, Loss: 0.7404, Validation Loss: 0.6636 \n",
            "Hidden Dimension: 16, Epoch: 10, Loss: 0.7110, Validation Loss: 0.6641 \n",
            "Hidden Dimension: 16, Epoch: 11, Loss: 0.7004, Validation Loss: 0.6689 \n",
            "Hidden Dimension: 16, Epoch: 12, Loss: 0.6380, Validation Loss: 0.6772 \n",
            "Hidden Dimension: 16, Epoch: 13, Loss: 0.6418, Validation Loss: 0.6878 \n",
            "Early stopping at epoch 14 \n",
            "\n",
            " Hidden Dimension: 16, Validation Loss: 0.6984 \n",
            "\n",
            "Hidden Dimension: 32, Epoch: 0, Loss: 1.7893, Validation Loss: 1.3769 \n",
            "Hidden Dimension: 32, Epoch: 1, Loss: 1.3872, Validation Loss: 1.0127 \n",
            "Hidden Dimension: 32, Epoch: 2, Loss: 1.0891, Validation Loss: 0.7962 \n",
            "Hidden Dimension: 32, Epoch: 3, Loss: 0.9097, Validation Loss: 0.6848 \n",
            "Hidden Dimension: 32, Epoch: 4, Loss: 0.8039, Validation Loss: 0.6459 \n",
            "Hidden Dimension: 32, Epoch: 5, Loss: 0.7729, Validation Loss: 0.6448 \n",
            "Hidden Dimension: 32, Epoch: 6, Loss: 0.7336, Validation Loss: 0.6467 \n",
            "Hidden Dimension: 32, Epoch: 7, Loss: 0.6869, Validation Loss: 0.6436 \n",
            "Hidden Dimension: 32, Epoch: 8, Loss: 0.6556, Validation Loss: 0.6512 \n",
            "Hidden Dimension: 32, Epoch: 9, Loss: 0.6321, Validation Loss: 0.6620 \n",
            "Hidden Dimension: 32, Epoch: 10, Loss: 0.6003, Validation Loss: 0.6760 \n",
            "Hidden Dimension: 32, Epoch: 11, Loss: 0.5928, Validation Loss: 0.6917 \n",
            "Early stopping at epoch 12 \n",
            "\n",
            " Hidden Dimension: 32, Validation Loss: 0.7064 \n",
            "\n",
            "Hidden Dimension: 64, Epoch: 0, Loss: 1.7963, Validation Loss: 1.3056 \n",
            "Hidden Dimension: 64, Epoch: 1, Loss: 1.3418, Validation Loss: 0.9027 \n",
            "Hidden Dimension: 64, Epoch: 2, Loss: 0.9886, Validation Loss: 0.7066 \n",
            "Hidden Dimension: 64, Epoch: 3, Loss: 0.8451, Validation Loss: 0.6320 \n",
            "Hidden Dimension: 64, Epoch: 4, Loss: 0.7688, Validation Loss: 0.6452 \n",
            "Hidden Dimension: 64, Epoch: 5, Loss: 0.7391, Validation Loss: 0.6390 \n",
            "Hidden Dimension: 64, Epoch: 6, Loss: 0.7005, Validation Loss: 0.6423 \n",
            "Hidden Dimension: 64, Epoch: 7, Loss: 0.6593, Validation Loss: 0.6541 \n",
            "Early stopping at epoch 8 \n",
            "\n",
            " Hidden Dimension: 64, Validation Loss: 0.6767 \n",
            "\n",
            "Hidden Dimension: 128, Epoch: 0, Loss: 1.8077, Validation Loss: 1.2399 \n",
            "Hidden Dimension: 128, Epoch: 1, Loss: 1.2926, Validation Loss: 0.8658 \n",
            "Hidden Dimension: 128, Epoch: 2, Loss: 0.9604, Validation Loss: 0.6754 \n",
            "Hidden Dimension: 128, Epoch: 3, Loss: 0.8072, Validation Loss: 0.6388 \n",
            "Hidden Dimension: 128, Epoch: 4, Loss: 0.7463, Validation Loss: 0.6838 \n",
            "Hidden Dimension: 128, Epoch: 5, Loss: 0.7198, Validation Loss: 0.6635 \n",
            "Hidden Dimension: 128, Epoch: 6, Loss: 0.6548, Validation Loss: 0.6794 \n",
            "Hidden Dimension: 128, Epoch: 7, Loss: 0.5854, Validation Loss: 0.6996 \n",
            "Early stopping at epoch 8 \n",
            "\n",
            " Hidden Dimension: 128, Validation Loss: 0.7175 \n",
            "\n",
            "Hidden Dimension: 256, Epoch: 0, Loss: 1.8047, Validation Loss: 1.1031 \n",
            "Hidden Dimension: 256, Epoch: 1, Loss: 1.1705, Validation Loss: 0.8590 \n",
            "Hidden Dimension: 256, Epoch: 2, Loss: 0.9666, Validation Loss: 0.6640 \n",
            "Hidden Dimension: 256, Epoch: 3, Loss: 0.7669, Validation Loss: 0.7182 \n",
            "Hidden Dimension: 256, Epoch: 4, Loss: 0.7479, Validation Loss: 0.7366 \n",
            "Hidden Dimension: 256, Epoch: 5, Loss: 0.6697, Validation Loss: 0.7209 \n",
            "Hidden Dimension: 256, Epoch: 6, Loss: 0.6162, Validation Loss: 0.7076 \n",
            "Early stopping at epoch 7 \n",
            "\n",
            " Hidden Dimension: 256, Validation Loss: 0.7100 \n",
            "\n",
            "Best Hidden Dimension: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for citeseer dataset (two layar )**"
      ],
      "metadata": {
        "id": "g5uT09PdSJZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = TwoLayerGAT(input_dim=citeseer.num_node_features,hidden_dim= best_hidden_dim ,output_dim= citeseer_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1b8baf-82f5-4fe6-c479-5cfb185a271c",
        "id": "b5UU_FcsSJZ3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 1.7975, Validation Loss: 1.5140 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 1.5095, Validation Loss: 1.2177 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 1.2745, Validation Loss: 0.9838 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 1.0791, Validation Loss: 0.8397 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 0.9827, Validation Loss: 0.7696 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 0.9399, Validation Loss: 0.7370 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 0.8748, Validation Loss: 0.7079 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 0.8925, Validation Loss: 0.6891 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 0.8142, Validation Loss: 0.6740 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 0.7901, Validation Loss: 0.6657 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 0.7812, Validation Loss: 0.6645 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 0.7518, Validation Loss: 0.6712 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 0.7374, Validation Loss: 0.6794 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 0.7298, Validation Loss: 0.6913 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 0.6980, Validation Loss: 0.7029 \n",
            "Early stopping at epoch 15 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 0.7115 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 1.7915, Validation Loss: 1.3850 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 1.3886, Validation Loss: 1.0176 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 1.0855, Validation Loss: 0.7941 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 0.9406, Validation Loss: 0.6987 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 0.8504, Validation Loss: 0.6604 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 0.8055, Validation Loss: 0.6523 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 0.7628, Validation Loss: 0.6673 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 0.7204, Validation Loss: 0.6590 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 0.6908, Validation Loss: 0.6555 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 0.6331, Validation Loss: 0.6681 \n",
            "Early stopping at epoch 10 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 0.6809 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 1.7949, Validation Loss: 1.3281 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 1.3621, Validation Loss: 0.9215 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 1.0154, Validation Loss: 0.7227 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 0.8522, Validation Loss: 0.6360 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 0.7911, Validation Loss: 0.6370 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 0.7397, Validation Loss: 0.6522 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 0.6972, Validation Loss: 0.6435 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 0.6859, Validation Loss: 0.6530 \n",
            "Early stopping at epoch 8 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 0.6741 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 1.8077, Validation Loss: 1.2190 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 1.2653, Validation Loss: 0.8514 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 0.9546, Validation Loss: 0.6697 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 0.7846, Validation Loss: 0.6728 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 0.7185, Validation Loss: 0.7076 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 0.7091, Validation Loss: 0.6777 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 0.6085, Validation Loss: 0.6923 \n",
            "Early stopping at epoch 7 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 0.7077 \n",
            "\n",
            "Best Number of Attention Heads: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = TwoLayerGAT(input_dim=citeseer.num_node_features,hidden_dim=best_hidden_dim , output_dim=citeseer_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, citeseer)\n",
        "    val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, citeseer)\n",
        "print(f'Test Accuracy(two layer) on citeseer dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8555919-d1b1-4c6e-8e2d-e47ec655bde4",
        "id": "wB-qOzWeSJZ4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 9 \n",
            "\n",
            "Test Accuracy(two layer) on citeseer dataset: 0.7308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One layer GAT**"
      ],
      "metadata": {
        "id": "zWeNyFRczo97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class OneLayerGAT(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_heads, dropout=0.6, negative_slope=0.2):\n",
        "        super(OneLayerGAT, self).__init__()\n",
        "        self.gat1 = GATConv(input_dim, output_dim, heads=num_heads, dropout=dropout, negative_slope=negative_slope)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.gat1(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "dfv1v2yvzxWh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Cora full dataset(One layer)**"
      ],
      "metadata": {
        "id": "sBx6MONE2J_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  cora full dataset(one layer)**"
      ],
      "metadata": {
        "id": "J_3s079bTcZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = OneLayerGAT(input_dim= CoraFull.num_node_features , output_dim= CoraFull_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76a43807-df19-494d-ff21-9df320af4345",
        "id": "ZvQjZGwtTcZG"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 4.2543, Validation Loss: 3.8281 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 3.8231, Validation Loss: 3.4111 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 3.5078, Validation Loss: 3.0978 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 3.2361, Validation Loss: 2.8806 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 3.0056, Validation Loss: 2.7098 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 2.8281, Validation Loss: 2.5213 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 2.6634, Validation Loss: 2.3686 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 2.5811, Validation Loss: 2.2856 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 2.4504, Validation Loss: 2.2229 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 2.3693, Validation Loss: 2.1461 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 2.2879, Validation Loss: 2.0680 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 2.2164, Validation Loss: 1.9896 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 2.1435, Validation Loss: 1.9212 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 2.0866, Validation Loss: 1.8679 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 2.0493, Validation Loss: 1.8209 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 1.9756, Validation Loss: 1.7737 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 1.9396, Validation Loss: 1.7289 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 1.8938, Validation Loss: 1.6899 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 1.8787, Validation Loss: 1.6575 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 1.8375, Validation Loss: 1.6285 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 1.8097, Validation Loss: 1.6007 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 1.8092, Validation Loss: 1.5764 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 1.7659, Validation Loss: 1.5573 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 1.7637, Validation Loss: 1.5423 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 1.7221, Validation Loss: 1.5254 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 1.7151, Validation Loss: 1.5098 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 1.7036, Validation Loss: 1.4959 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 1.6969, Validation Loss: 1.4869 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 1.6973, Validation Loss: 1.4803 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 1.6837, Validation Loss: 1.4729 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 1.6568, Validation Loss: 1.4659 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 1.6438, Validation Loss: 1.4580 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 1.6375, Validation Loss: 1.4514 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 1.6331, Validation Loss: 1.4482 \n",
            "Attention Heads: 1, Epoch: 34, Loss: 1.6141, Validation Loss: 1.4467 \n",
            "Attention Heads: 1, Epoch: 35, Loss: 1.6181, Validation Loss: 1.4443 \n",
            "Attention Heads: 1, Epoch: 36, Loss: 1.6224, Validation Loss: 1.4410 \n",
            "Attention Heads: 1, Epoch: 37, Loss: 1.6374, Validation Loss: 1.4372 \n",
            "Attention Heads: 1, Epoch: 38, Loss: 1.6139, Validation Loss: 1.4343 \n",
            "Attention Heads: 1, Epoch: 39, Loss: 1.6003, Validation Loss: 1.4323 \n",
            "Attention Heads: 1, Epoch: 40, Loss: 1.6083, Validation Loss: 1.4313 \n",
            "Attention Heads: 1, Epoch: 41, Loss: 1.6040, Validation Loss: 1.4309 \n",
            "Attention Heads: 1, Epoch: 42, Loss: 1.5933, Validation Loss: 1.4286 \n",
            "Attention Heads: 1, Epoch: 43, Loss: 1.6089, Validation Loss: 1.4263 \n",
            "Attention Heads: 1, Epoch: 44, Loss: 1.5938, Validation Loss: 1.4235 \n",
            "Attention Heads: 1, Epoch: 45, Loss: 1.6003, Validation Loss: 1.4206 \n",
            "Attention Heads: 1, Epoch: 46, Loss: 1.5970, Validation Loss: 1.4185 \n",
            "Attention Heads: 1, Epoch: 47, Loss: 1.5987, Validation Loss: 1.4178 \n",
            "Attention Heads: 1, Epoch: 48, Loss: 1.5970, Validation Loss: 1.4182 \n",
            "Attention Heads: 1, Epoch: 49, Loss: 1.5696, Validation Loss: 1.4177 \n",
            "Attention Heads: 1, Epoch: 50, Loss: 1.6004, Validation Loss: 1.4156 \n",
            "Attention Heads: 1, Epoch: 51, Loss: 1.5856, Validation Loss: 1.4132 \n",
            "Attention Heads: 1, Epoch: 52, Loss: 1.5922, Validation Loss: 1.4123 \n",
            "Attention Heads: 1, Epoch: 53, Loss: 1.5811, Validation Loss: 1.4109 \n",
            "Attention Heads: 1, Epoch: 54, Loss: 1.5700, Validation Loss: 1.4073 \n",
            "Attention Heads: 1, Epoch: 55, Loss: 1.5815, Validation Loss: 1.4036 \n",
            "Attention Heads: 1, Epoch: 56, Loss: 1.5726, Validation Loss: 1.3993 \n",
            "Attention Heads: 1, Epoch: 57, Loss: 1.5718, Validation Loss: 1.3976 \n",
            "Attention Heads: 1, Epoch: 58, Loss: 1.5742, Validation Loss: 1.3986 \n",
            "Attention Heads: 1, Epoch: 59, Loss: 1.5767, Validation Loss: 1.4007 \n",
            "Attention Heads: 1, Epoch: 60, Loss: 1.5689, Validation Loss: 1.4011 \n",
            "Attention Heads: 1, Epoch: 61, Loss: 1.5785, Validation Loss: 1.3995 \n",
            "Early stopping at epoch 62 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 1.3973 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 4.9395, Validation Loss: 4.3302 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 4.3172, Validation Loss: 3.8044 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 3.8372, Validation Loss: 3.3608 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 3.5100, Validation Loss: 3.0375 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 3.2347, Validation Loss: 2.8402 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 3.0364, Validation Loss: 2.7023 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 2.8781, Validation Loss: 2.5509 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 2.7298, Validation Loss: 2.3843 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 2.6005, Validation Loss: 2.2383 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 2.4925, Validation Loss: 2.1356 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 2.4108, Validation Loss: 2.0725 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 2.3240, Validation Loss: 2.0253 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 2.2741, Validation Loss: 1.9761 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 2.2060, Validation Loss: 1.9187 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 2.1457, Validation Loss: 1.8634 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 2.0797, Validation Loss: 1.8141 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 2.0627, Validation Loss: 1.7734 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 1.9891, Validation Loss: 1.7446 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 1.9714, Validation Loss: 1.7172 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 1.9457, Validation Loss: 1.6913 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 1.9394, Validation Loss: 1.6687 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 1.8985, Validation Loss: 1.6448 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 1.8753, Validation Loss: 1.6258 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 1.8585, Validation Loss: 1.6084 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 1.8382, Validation Loss: 1.5922 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 1.8194, Validation Loss: 1.5761 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 1.8103, Validation Loss: 1.5639 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 1.7800, Validation Loss: 1.5511 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 1.7743, Validation Loss: 1.5370 \n",
            "Attention Heads: 2, Epoch: 29, Loss: 1.7704, Validation Loss: 1.5231 \n",
            "Attention Heads: 2, Epoch: 30, Loss: 1.7622, Validation Loss: 1.5121 \n",
            "Attention Heads: 2, Epoch: 31, Loss: 1.7335, Validation Loss: 1.5050 \n",
            "Attention Heads: 2, Epoch: 32, Loss: 1.7247, Validation Loss: 1.5001 \n",
            "Attention Heads: 2, Epoch: 33, Loss: 1.7380, Validation Loss: 1.4949 \n",
            "Attention Heads: 2, Epoch: 34, Loss: 1.7208, Validation Loss: 1.4879 \n",
            "Attention Heads: 2, Epoch: 35, Loss: 1.7033, Validation Loss: 1.4786 \n",
            "Attention Heads: 2, Epoch: 36, Loss: 1.7284, Validation Loss: 1.4719 \n",
            "Attention Heads: 2, Epoch: 37, Loss: 1.6995, Validation Loss: 1.4677 \n",
            "Attention Heads: 2, Epoch: 38, Loss: 1.6865, Validation Loss: 1.4660 \n",
            "Attention Heads: 2, Epoch: 39, Loss: 1.6846, Validation Loss: 1.4628 \n",
            "Attention Heads: 2, Epoch: 40, Loss: 1.6867, Validation Loss: 1.4593 \n",
            "Attention Heads: 2, Epoch: 41, Loss: 1.6821, Validation Loss: 1.4561 \n",
            "Attention Heads: 2, Epoch: 42, Loss: 1.6713, Validation Loss: 1.4515 \n",
            "Attention Heads: 2, Epoch: 43, Loss: 1.6697, Validation Loss: 1.4471 \n",
            "Attention Heads: 2, Epoch: 44, Loss: 1.6631, Validation Loss: 1.4422 \n",
            "Attention Heads: 2, Epoch: 45, Loss: 1.6558, Validation Loss: 1.4402 \n",
            "Attention Heads: 2, Epoch: 46, Loss: 1.6837, Validation Loss: 1.4404 \n",
            "Attention Heads: 2, Epoch: 47, Loss: 1.6642, Validation Loss: 1.4408 \n",
            "Attention Heads: 2, Epoch: 48, Loss: 1.6643, Validation Loss: 1.4390 \n",
            "Attention Heads: 2, Epoch: 49, Loss: 1.6546, Validation Loss: 1.4341 \n",
            "Attention Heads: 2, Epoch: 50, Loss: 1.6604, Validation Loss: 1.4272 \n",
            "Attention Heads: 2, Epoch: 51, Loss: 1.6395, Validation Loss: 1.4242 \n",
            "Attention Heads: 2, Epoch: 52, Loss: 1.6667, Validation Loss: 1.4257 \n",
            "Attention Heads: 2, Epoch: 53, Loss: 1.6510, Validation Loss: 1.4270 \n",
            "Attention Heads: 2, Epoch: 54, Loss: 1.6043, Validation Loss: 1.4234 \n",
            "Attention Heads: 2, Epoch: 55, Loss: 1.6270, Validation Loss: 1.4182 \n",
            "Attention Heads: 2, Epoch: 56, Loss: 1.6498, Validation Loss: 1.4158 \n",
            "Attention Heads: 2, Epoch: 57, Loss: 1.6479, Validation Loss: 1.4160 \n",
            "Attention Heads: 2, Epoch: 58, Loss: 1.6508, Validation Loss: 1.4185 \n",
            "Attention Heads: 2, Epoch: 59, Loss: 1.6219, Validation Loss: 1.4172 \n",
            "Attention Heads: 2, Epoch: 60, Loss: 1.6198, Validation Loss: 1.4076 \n",
            "Attention Heads: 2, Epoch: 61, Loss: 1.6213, Validation Loss: 1.4013 \n",
            "Attention Heads: 2, Epoch: 62, Loss: 1.6321, Validation Loss: 1.4032 \n",
            "Attention Heads: 2, Epoch: 63, Loss: 1.6145, Validation Loss: 1.4074 \n",
            "Attention Heads: 2, Epoch: 64, Loss: 1.6127, Validation Loss: 1.4090 \n",
            "Attention Heads: 2, Epoch: 65, Loss: 1.6326, Validation Loss: 1.4043 \n",
            "Attention Heads: 2, Epoch: 66, Loss: 1.6173, Validation Loss: 1.3964 \n",
            "Attention Heads: 2, Epoch: 67, Loss: 1.6154, Validation Loss: 1.3932 \n",
            "Attention Heads: 2, Epoch: 68, Loss: 1.6070, Validation Loss: 1.3917 \n",
            "Attention Heads: 2, Epoch: 69, Loss: 1.5967, Validation Loss: 1.3927 \n",
            "Attention Heads: 2, Epoch: 70, Loss: 1.5731, Validation Loss: 1.3921 \n",
            "Attention Heads: 2, Epoch: 71, Loss: 1.5914, Validation Loss: 1.3893 \n",
            "Attention Heads: 2, Epoch: 72, Loss: 1.6096, Validation Loss: 1.3845 \n",
            "Attention Heads: 2, Epoch: 73, Loss: 1.6028, Validation Loss: 1.3821 \n",
            "Attention Heads: 2, Epoch: 74, Loss: 1.6073, Validation Loss: 1.3853 \n",
            "Attention Heads: 2, Epoch: 75, Loss: 1.5994, Validation Loss: 1.3883 \n",
            "Attention Heads: 2, Epoch: 76, Loss: 1.5843, Validation Loss: 1.3831 \n",
            "Attention Heads: 2, Epoch: 77, Loss: 1.5883, Validation Loss: 1.3796 \n",
            "Attention Heads: 2, Epoch: 78, Loss: 1.5871, Validation Loss: 1.3794 \n",
            "Attention Heads: 2, Epoch: 79, Loss: 1.5859, Validation Loss: 1.3777 \n",
            "Attention Heads: 2, Epoch: 80, Loss: 1.5932, Validation Loss: 1.3748 \n",
            "Attention Heads: 2, Epoch: 81, Loss: 1.5704, Validation Loss: 1.3715 \n",
            "Attention Heads: 2, Epoch: 82, Loss: 1.5842, Validation Loss: 1.3668 \n",
            "Attention Heads: 2, Epoch: 83, Loss: 1.5825, Validation Loss: 1.3643 \n",
            "Attention Heads: 2, Epoch: 84, Loss: 1.5715, Validation Loss: 1.3623 \n",
            "Attention Heads: 2, Epoch: 85, Loss: 1.5846, Validation Loss: 1.3609 \n",
            "Attention Heads: 2, Epoch: 86, Loss: 1.5631, Validation Loss: 1.3563 \n",
            "Attention Heads: 2, Epoch: 87, Loss: 1.5847, Validation Loss: 1.3561 \n",
            "Attention Heads: 2, Epoch: 88, Loss: 1.5892, Validation Loss: 1.3571 \n",
            "Attention Heads: 2, Epoch: 89, Loss: 1.5748, Validation Loss: 1.3554 \n",
            "Attention Heads: 2, Epoch: 90, Loss: 1.5726, Validation Loss: 1.3526 \n",
            "Attention Heads: 2, Epoch: 91, Loss: 1.5702, Validation Loss: 1.3520 \n",
            "Attention Heads: 2, Epoch: 92, Loss: 1.5922, Validation Loss: 1.3526 \n",
            "Attention Heads: 2, Epoch: 93, Loss: 1.5656, Validation Loss: 1.3530 \n",
            "Attention Heads: 2, Epoch: 94, Loss: 1.5734, Validation Loss: 1.3520 \n",
            "Attention Heads: 2, Epoch: 95, Loss: 1.5547, Validation Loss: 1.3471 \n",
            "Attention Heads: 2, Epoch: 96, Loss: 1.5701, Validation Loss: 1.3438 \n",
            "Attention Heads: 2, Epoch: 97, Loss: 1.5607, Validation Loss: 1.3449 \n",
            "Attention Heads: 2, Epoch: 98, Loss: 1.5815, Validation Loss: 1.3473 \n",
            "Attention Heads: 2, Epoch: 99, Loss: 1.5645, Validation Loss: 1.3463 \n",
            "Attention Heads: 2, Epoch: 100, Loss: 1.5878, Validation Loss: 1.3454 \n",
            "Early stopping at epoch 101 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 1.3496 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 5.6347, Validation Loss: 4.8208 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 4.8292, Validation Loss: 4.0909 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 4.2615, Validation Loss: 3.6080 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 3.8874, Validation Loss: 3.2515 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 3.5774, Validation Loss: 2.9960 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 3.3288, Validation Loss: 2.8258 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 3.1662, Validation Loss: 2.7091 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 3.0129, Validation Loss: 2.6100 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 2.8734, Validation Loss: 2.4561 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 2.7467, Validation Loss: 2.2986 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 2.6444, Validation Loss: 2.1884 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 2.5556, Validation Loss: 2.1299 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 2.4939, Validation Loss: 2.0968 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 2.4100, Validation Loss: 2.0499 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 2.3586, Validation Loss: 1.9841 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 2.2869, Validation Loss: 1.9286 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 2.2560, Validation Loss: 1.8909 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 2.2189, Validation Loss: 1.8647 \n",
            "Attention Heads: 4, Epoch: 18, Loss: 2.1962, Validation Loss: 1.8329 \n",
            "Attention Heads: 4, Epoch: 19, Loss: 2.1360, Validation Loss: 1.7865 \n",
            "Attention Heads: 4, Epoch: 20, Loss: 2.0958, Validation Loss: 1.7528 \n",
            "Attention Heads: 4, Epoch: 21, Loss: 2.0734, Validation Loss: 1.7363 \n",
            "Attention Heads: 4, Epoch: 22, Loss: 2.0561, Validation Loss: 1.7237 \n",
            "Attention Heads: 4, Epoch: 23, Loss: 2.0058, Validation Loss: 1.7028 \n",
            "Attention Heads: 4, Epoch: 24, Loss: 1.9873, Validation Loss: 1.6705 \n",
            "Attention Heads: 4, Epoch: 25, Loss: 1.9745, Validation Loss: 1.6412 \n",
            "Attention Heads: 4, Epoch: 26, Loss: 1.9428, Validation Loss: 1.6213 \n",
            "Attention Heads: 4, Epoch: 27, Loss: 1.9290, Validation Loss: 1.6139 \n",
            "Attention Heads: 4, Epoch: 28, Loss: 1.9199, Validation Loss: 1.6068 \n",
            "Attention Heads: 4, Epoch: 29, Loss: 1.8954, Validation Loss: 1.5909 \n",
            "Attention Heads: 4, Epoch: 30, Loss: 1.8907, Validation Loss: 1.5709 \n",
            "Attention Heads: 4, Epoch: 31, Loss: 1.8770, Validation Loss: 1.5580 \n",
            "Attention Heads: 4, Epoch: 32, Loss: 1.8613, Validation Loss: 1.5559 \n",
            "Attention Heads: 4, Epoch: 33, Loss: 1.8461, Validation Loss: 1.5441 \n",
            "Attention Heads: 4, Epoch: 34, Loss: 1.8554, Validation Loss: 1.5278 \n",
            "Attention Heads: 4, Epoch: 35, Loss: 1.8400, Validation Loss: 1.5177 \n",
            "Attention Heads: 4, Epoch: 36, Loss: 1.8161, Validation Loss: 1.5197 \n",
            "Attention Heads: 4, Epoch: 37, Loss: 1.8154, Validation Loss: 1.5176 \n",
            "Attention Heads: 4, Epoch: 38, Loss: 1.8023, Validation Loss: 1.5059 \n",
            "Attention Heads: 4, Epoch: 39, Loss: 1.8159, Validation Loss: 1.4953 \n",
            "Attention Heads: 4, Epoch: 40, Loss: 1.7857, Validation Loss: 1.4874 \n",
            "Attention Heads: 4, Epoch: 41, Loss: 1.7939, Validation Loss: 1.4912 \n",
            "Attention Heads: 4, Epoch: 42, Loss: 1.8017, Validation Loss: 1.4913 \n",
            "Attention Heads: 4, Epoch: 43, Loss: 1.7805, Validation Loss: 1.4819 \n",
            "Attention Heads: 4, Epoch: 44, Loss: 1.7838, Validation Loss: 1.4668 \n",
            "Attention Heads: 4, Epoch: 45, Loss: 1.7543, Validation Loss: 1.4633 \n",
            "Attention Heads: 4, Epoch: 46, Loss: 1.7656, Validation Loss: 1.4724 \n",
            "Attention Heads: 4, Epoch: 47, Loss: 1.7538, Validation Loss: 1.4684 \n",
            "Attention Heads: 4, Epoch: 48, Loss: 1.7609, Validation Loss: 1.4559 \n",
            "Attention Heads: 4, Epoch: 49, Loss: 1.7467, Validation Loss: 1.4454 \n",
            "Attention Heads: 4, Epoch: 50, Loss: 1.7596, Validation Loss: 1.4520 \n",
            "Attention Heads: 4, Epoch: 51, Loss: 1.7362, Validation Loss: 1.4578 \n",
            "Attention Heads: 4, Epoch: 52, Loss: 1.7292, Validation Loss: 1.4472 \n",
            "Attention Heads: 4, Epoch: 53, Loss: 1.7605, Validation Loss: 1.4399 \n",
            "Attention Heads: 4, Epoch: 54, Loss: 1.7325, Validation Loss: 1.4388 \n",
            "Attention Heads: 4, Epoch: 55, Loss: 1.7261, Validation Loss: 1.4418 \n",
            "Attention Heads: 4, Epoch: 56, Loss: 1.7489, Validation Loss: 1.4401 \n",
            "Attention Heads: 4, Epoch: 57, Loss: 1.7081, Validation Loss: 1.4308 \n",
            "Attention Heads: 4, Epoch: 58, Loss: 1.7332, Validation Loss: 1.4254 \n",
            "Attention Heads: 4, Epoch: 59, Loss: 1.6897, Validation Loss: 1.4331 \n",
            "Attention Heads: 4, Epoch: 60, Loss: 1.7146, Validation Loss: 1.4350 \n",
            "Attention Heads: 4, Epoch: 61, Loss: 1.6983, Validation Loss: 1.4208 \n",
            "Attention Heads: 4, Epoch: 62, Loss: 1.6932, Validation Loss: 1.4174 \n",
            "Attention Heads: 4, Epoch: 63, Loss: 1.6946, Validation Loss: 1.4273 \n",
            "Attention Heads: 4, Epoch: 64, Loss: 1.7022, Validation Loss: 1.4250 \n",
            "Attention Heads: 4, Epoch: 65, Loss: 1.7102, Validation Loss: 1.4156 \n",
            "Attention Heads: 4, Epoch: 66, Loss: 1.7027, Validation Loss: 1.4098 \n",
            "Attention Heads: 4, Epoch: 67, Loss: 1.6766, Validation Loss: 1.4114 \n",
            "Attention Heads: 4, Epoch: 68, Loss: 1.6780, Validation Loss: 1.4112 \n",
            "Attention Heads: 4, Epoch: 69, Loss: 1.6617, Validation Loss: 1.4040 \n",
            "Attention Heads: 4, Epoch: 70, Loss: 1.6921, Validation Loss: 1.4025 \n",
            "Attention Heads: 4, Epoch: 71, Loss: 1.6738, Validation Loss: 1.4077 \n",
            "Attention Heads: 4, Epoch: 72, Loss: 1.6893, Validation Loss: 1.3995 \n",
            "Attention Heads: 4, Epoch: 73, Loss: 1.6885, Validation Loss: 1.3941 \n",
            "Attention Heads: 4, Epoch: 74, Loss: 1.6751, Validation Loss: 1.3913 \n",
            "Attention Heads: 4, Epoch: 75, Loss: 1.6639, Validation Loss: 1.3966 \n",
            "Attention Heads: 4, Epoch: 76, Loss: 1.6721, Validation Loss: 1.3953 \n",
            "Attention Heads: 4, Epoch: 77, Loss: 1.6582, Validation Loss: 1.3887 \n",
            "Attention Heads: 4, Epoch: 78, Loss: 1.6766, Validation Loss: 1.3812 \n",
            "Attention Heads: 4, Epoch: 79, Loss: 1.6823, Validation Loss: 1.3886 \n",
            "Attention Heads: 4, Epoch: 80, Loss: 1.6496, Validation Loss: 1.3932 \n",
            "Attention Heads: 4, Epoch: 81, Loss: 1.6441, Validation Loss: 1.3839 \n",
            "Attention Heads: 4, Epoch: 82, Loss: 1.6595, Validation Loss: 1.3752 \n",
            "Attention Heads: 4, Epoch: 83, Loss: 1.6617, Validation Loss: 1.3770 \n",
            "Attention Heads: 4, Epoch: 84, Loss: 1.6548, Validation Loss: 1.3763 \n",
            "Attention Heads: 4, Epoch: 85, Loss: 1.6480, Validation Loss: 1.3785 \n",
            "Attention Heads: 4, Epoch: 86, Loss: 1.6550, Validation Loss: 1.3766 \n",
            "Attention Heads: 4, Epoch: 87, Loss: 1.6528, Validation Loss: 1.3729 \n",
            "Attention Heads: 4, Epoch: 88, Loss: 1.6536, Validation Loss: 1.3694 \n",
            "Attention Heads: 4, Epoch: 89, Loss: 1.6234, Validation Loss: 1.3674 \n",
            "Attention Heads: 4, Epoch: 90, Loss: 1.6402, Validation Loss: 1.3680 \n",
            "Attention Heads: 4, Epoch: 91, Loss: 1.6285, Validation Loss: 1.3687 \n",
            "Attention Heads: 4, Epoch: 92, Loss: 1.6543, Validation Loss: 1.3663 \n",
            "Attention Heads: 4, Epoch: 93, Loss: 1.6413, Validation Loss: 1.3631 \n",
            "Attention Heads: 4, Epoch: 94, Loss: 1.6430, Validation Loss: 1.3641 \n",
            "Attention Heads: 4, Epoch: 95, Loss: 1.6545, Validation Loss: 1.3626 \n",
            "Attention Heads: 4, Epoch: 96, Loss: 1.6364, Validation Loss: 1.3572 \n",
            "Attention Heads: 4, Epoch: 97, Loss: 1.6288, Validation Loss: 1.3601 \n",
            "Attention Heads: 4, Epoch: 98, Loss: 1.6253, Validation Loss: 1.3611 \n",
            "Attention Heads: 4, Epoch: 99, Loss: 1.6286, Validation Loss: 1.3539 \n",
            "Attention Heads: 4, Epoch: 100, Loss: 1.6395, Validation Loss: 1.3553 \n",
            "Attention Heads: 4, Epoch: 101, Loss: 1.6083, Validation Loss: 1.3558 \n",
            "Attention Heads: 4, Epoch: 102, Loss: 1.6242, Validation Loss: 1.3579 \n",
            "Attention Heads: 4, Epoch: 103, Loss: 1.6333, Validation Loss: 1.3567 \n",
            "Early stopping at epoch 104 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 1.3536 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 6.3382, Validation Loss: 5.4314 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 5.4487, Validation Loss: 4.5112 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 4.7329, Validation Loss: 3.9408 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 4.3051, Validation Loss: 3.5242 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 3.9784, Validation Loss: 3.2247 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 3.7108, Validation Loss: 3.0369 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 3.4597, Validation Loss: 2.9258 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 3.3048, Validation Loss: 2.8311 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 3.1580, Validation Loss: 2.6552 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 3.0308, Validation Loss: 2.4726 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 2.9069, Validation Loss: 2.3600 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 2.8071, Validation Loss: 2.2955 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 2.7275, Validation Loss: 2.2441 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 2.6630, Validation Loss: 2.1802 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 2.5920, Validation Loss: 2.1002 \n",
            "Attention Heads: 8, Epoch: 15, Loss: 2.5201, Validation Loss: 2.0379 \n",
            "Attention Heads: 8, Epoch: 16, Loss: 2.4832, Validation Loss: 2.0207 \n",
            "Attention Heads: 8, Epoch: 17, Loss: 2.4503, Validation Loss: 1.9917 \n",
            "Attention Heads: 8, Epoch: 18, Loss: 2.3725, Validation Loss: 1.9211 \n",
            "Attention Heads: 8, Epoch: 19, Loss: 2.3259, Validation Loss: 1.8693 \n",
            "Attention Heads: 8, Epoch: 20, Loss: 2.3167, Validation Loss: 1.8656 \n",
            "Attention Heads: 8, Epoch: 21, Loss: 2.2556, Validation Loss: 1.8389 \n",
            "Attention Heads: 8, Epoch: 22, Loss: 2.2213, Validation Loss: 1.7924 \n",
            "Attention Heads: 8, Epoch: 23, Loss: 2.2033, Validation Loss: 1.7671 \n",
            "Attention Heads: 8, Epoch: 24, Loss: 2.1665, Validation Loss: 1.7610 \n",
            "Attention Heads: 8, Epoch: 25, Loss: 2.1491, Validation Loss: 1.7434 \n",
            "Attention Heads: 8, Epoch: 26, Loss: 2.1491, Validation Loss: 1.7087 \n",
            "Attention Heads: 8, Epoch: 27, Loss: 2.0965, Validation Loss: 1.6726 \n",
            "Attention Heads: 8, Epoch: 28, Loss: 2.0775, Validation Loss: 1.6733 \n",
            "Attention Heads: 8, Epoch: 29, Loss: 2.0535, Validation Loss: 1.6655 \n",
            "Attention Heads: 8, Epoch: 30, Loss: 2.0467, Validation Loss: 1.6387 \n",
            "Attention Heads: 8, Epoch: 31, Loss: 2.0440, Validation Loss: 1.6131 \n",
            "Attention Heads: 8, Epoch: 32, Loss: 2.0134, Validation Loss: 1.6217 \n",
            "Attention Heads: 8, Epoch: 33, Loss: 1.9611, Validation Loss: 1.6136 \n",
            "Attention Heads: 8, Epoch: 34, Loss: 1.9769, Validation Loss: 1.5739 \n",
            "Attention Heads: 8, Epoch: 35, Loss: 1.9616, Validation Loss: 1.5593 \n",
            "Attention Heads: 8, Epoch: 36, Loss: 1.9388, Validation Loss: 1.5784 \n",
            "Attention Heads: 8, Epoch: 37, Loss: 1.9588, Validation Loss: 1.5664 \n",
            "Attention Heads: 8, Epoch: 38, Loss: 1.9292, Validation Loss: 1.5288 \n",
            "Attention Heads: 8, Epoch: 39, Loss: 1.9233, Validation Loss: 1.5189 \n",
            "Attention Heads: 8, Epoch: 40, Loss: 1.9271, Validation Loss: 1.5443 \n",
            "Attention Heads: 8, Epoch: 41, Loss: 1.9300, Validation Loss: 1.5431 \n",
            "Attention Heads: 8, Epoch: 42, Loss: 1.9131, Validation Loss: 1.5072 \n",
            "Attention Heads: 8, Epoch: 43, Loss: 1.8801, Validation Loss: 1.4963 \n",
            "Attention Heads: 8, Epoch: 44, Loss: 1.9011, Validation Loss: 1.5226 \n",
            "Attention Heads: 8, Epoch: 45, Loss: 1.8736, Validation Loss: 1.5301 \n",
            "Attention Heads: 8, Epoch: 46, Loss: 1.8700, Validation Loss: 1.4972 \n",
            "Attention Heads: 8, Epoch: 47, Loss: 1.8579, Validation Loss: 1.4832 \n",
            "Attention Heads: 8, Epoch: 48, Loss: 1.8726, Validation Loss: 1.4933 \n",
            "Attention Heads: 8, Epoch: 49, Loss: 1.8598, Validation Loss: 1.4998 \n",
            "Attention Heads: 8, Epoch: 50, Loss: 1.8673, Validation Loss: 1.4899 \n",
            "Attention Heads: 8, Epoch: 51, Loss: 1.8409, Validation Loss: 1.4700 \n",
            "Attention Heads: 8, Epoch: 52, Loss: 1.8555, Validation Loss: 1.4685 \n",
            "Attention Heads: 8, Epoch: 53, Loss: 1.8411, Validation Loss: 1.4832 \n",
            "Attention Heads: 8, Epoch: 54, Loss: 1.8540, Validation Loss: 1.4879 \n",
            "Attention Heads: 8, Epoch: 55, Loss: 1.8446, Validation Loss: 1.4711 \n",
            "Attention Heads: 8, Epoch: 56, Loss: 1.8395, Validation Loss: 1.4567 \n",
            "Attention Heads: 8, Epoch: 57, Loss: 1.8087, Validation Loss: 1.4658 \n",
            "Attention Heads: 8, Epoch: 58, Loss: 1.8238, Validation Loss: 1.4725 \n",
            "Attention Heads: 8, Epoch: 59, Loss: 1.8325, Validation Loss: 1.4623 \n",
            "Attention Heads: 8, Epoch: 60, Loss: 1.8096, Validation Loss: 1.4519 \n",
            "Attention Heads: 8, Epoch: 61, Loss: 1.8317, Validation Loss: 1.4546 \n",
            "Attention Heads: 8, Epoch: 62, Loss: 1.8018, Validation Loss: 1.4592 \n",
            "Attention Heads: 8, Epoch: 63, Loss: 1.7997, Validation Loss: 1.4468 \n",
            "Attention Heads: 8, Epoch: 64, Loss: 1.8028, Validation Loss: 1.4379 \n",
            "Attention Heads: 8, Epoch: 65, Loss: 1.8162, Validation Loss: 1.4477 \n",
            "Attention Heads: 8, Epoch: 66, Loss: 1.7972, Validation Loss: 1.4519 \n",
            "Attention Heads: 8, Epoch: 67, Loss: 1.8105, Validation Loss: 1.4349 \n",
            "Attention Heads: 8, Epoch: 68, Loss: 1.7981, Validation Loss: 1.4295 \n",
            "Attention Heads: 8, Epoch: 69, Loss: 1.8046, Validation Loss: 1.4397 \n",
            "Attention Heads: 8, Epoch: 70, Loss: 1.7855, Validation Loss: 1.4467 \n",
            "Attention Heads: 8, Epoch: 71, Loss: 1.7838, Validation Loss: 1.4302 \n",
            "Attention Heads: 8, Epoch: 72, Loss: 1.7656, Validation Loss: 1.4175 \n",
            "Attention Heads: 8, Epoch: 73, Loss: 1.7899, Validation Loss: 1.4199 \n",
            "Attention Heads: 8, Epoch: 74, Loss: 1.7823, Validation Loss: 1.4287 \n",
            "Attention Heads: 8, Epoch: 75, Loss: 1.8077, Validation Loss: 1.4192 \n",
            "Attention Heads: 8, Epoch: 76, Loss: 1.7882, Validation Loss: 1.4065 \n",
            "Attention Heads: 8, Epoch: 77, Loss: 1.7748, Validation Loss: 1.4096 \n",
            "Attention Heads: 8, Epoch: 78, Loss: 1.7743, Validation Loss: 1.4195 \n",
            "Attention Heads: 8, Epoch: 79, Loss: 1.7672, Validation Loss: 1.4147 \n",
            "Attention Heads: 8, Epoch: 80, Loss: 1.7521, Validation Loss: 1.4008 \n",
            "Attention Heads: 8, Epoch: 81, Loss: 1.7168, Validation Loss: 1.3954 \n",
            "Attention Heads: 8, Epoch: 82, Loss: 1.7524, Validation Loss: 1.4044 \n",
            "Attention Heads: 8, Epoch: 83, Loss: 1.7506, Validation Loss: 1.4026 \n",
            "Attention Heads: 8, Epoch: 84, Loss: 1.7517, Validation Loss: 1.3894 \n",
            "Attention Heads: 8, Epoch: 85, Loss: 1.7564, Validation Loss: 1.3927 \n",
            "Attention Heads: 8, Epoch: 86, Loss: 1.7511, Validation Loss: 1.4042 \n",
            "Attention Heads: 8, Epoch: 87, Loss: 1.7234, Validation Loss: 1.4007 \n",
            "Attention Heads: 8, Epoch: 88, Loss: 1.7335, Validation Loss: 1.3868 \n",
            "Attention Heads: 8, Epoch: 89, Loss: 1.7409, Validation Loss: 1.3837 \n",
            "Attention Heads: 8, Epoch: 90, Loss: 1.7398, Validation Loss: 1.3895 \n",
            "Attention Heads: 8, Epoch: 91, Loss: 1.7794, Validation Loss: 1.3925 \n",
            "Attention Heads: 8, Epoch: 92, Loss: 1.7314, Validation Loss: 1.3830 \n",
            "Attention Heads: 8, Epoch: 93, Loss: 1.7063, Validation Loss: 1.3776 \n",
            "Attention Heads: 8, Epoch: 94, Loss: 1.7277, Validation Loss: 1.3867 \n",
            "Attention Heads: 8, Epoch: 95, Loss: 1.7168, Validation Loss: 1.3909 \n",
            "Attention Heads: 8, Epoch: 96, Loss: 1.7182, Validation Loss: 1.3760 \n",
            "Attention Heads: 8, Epoch: 97, Loss: 1.7390, Validation Loss: 1.3682 \n",
            "Attention Heads: 8, Epoch: 98, Loss: 1.7158, Validation Loss: 1.3683 \n",
            "Attention Heads: 8, Epoch: 99, Loss: 1.7246, Validation Loss: 1.3834 \n",
            "Attention Heads: 8, Epoch: 100, Loss: 1.7177, Validation Loss: 1.3821 \n",
            "Attention Heads: 8, Epoch: 101, Loss: 1.7009, Validation Loss: 1.3629 \n",
            "Attention Heads: 8, Epoch: 102, Loss: 1.7158, Validation Loss: 1.3651 \n",
            "Attention Heads: 8, Epoch: 103, Loss: 1.7037, Validation Loss: 1.3809 \n",
            "Attention Heads: 8, Epoch: 104, Loss: 1.7051, Validation Loss: 1.3807 \n",
            "Attention Heads: 8, Epoch: 105, Loss: 1.7392, Validation Loss: 1.3665 \n",
            "Attention Heads: 8, Epoch: 106, Loss: 1.6962, Validation Loss: 1.3603 \n",
            "Attention Heads: 8, Epoch: 107, Loss: 1.7092, Validation Loss: 1.3664 \n",
            "Attention Heads: 8, Epoch: 108, Loss: 1.6873, Validation Loss: 1.3756 \n",
            "Attention Heads: 8, Epoch: 109, Loss: 1.6917, Validation Loss: 1.3671 \n",
            "Attention Heads: 8, Epoch: 110, Loss: 1.6824, Validation Loss: 1.3582 \n",
            "Attention Heads: 8, Epoch: 111, Loss: 1.6931, Validation Loss: 1.3599 \n",
            "Attention Heads: 8, Epoch: 112, Loss: 1.7006, Validation Loss: 1.3683 \n",
            "Attention Heads: 8, Epoch: 113, Loss: 1.7003, Validation Loss: 1.3601 \n",
            "Attention Heads: 8, Epoch: 114, Loss: 1.6884, Validation Loss: 1.3483 \n",
            "Attention Heads: 8, Epoch: 115, Loss: 1.6935, Validation Loss: 1.3503 \n",
            "Attention Heads: 8, Epoch: 116, Loss: 1.6840, Validation Loss: 1.3606 \n",
            "Attention Heads: 8, Epoch: 117, Loss: 1.6909, Validation Loss: 1.3576 \n",
            "Attention Heads: 8, Epoch: 118, Loss: 1.6813, Validation Loss: 1.3455 \n",
            "Attention Heads: 8, Epoch: 119, Loss: 1.6670, Validation Loss: 1.3448 \n",
            "Attention Heads: 8, Epoch: 120, Loss: 1.6939, Validation Loss: 1.3568 \n",
            "Attention Heads: 8, Epoch: 121, Loss: 1.6807, Validation Loss: 1.3622 \n",
            "Attention Heads: 8, Epoch: 122, Loss: 1.6697, Validation Loss: 1.3485 \n",
            "Attention Heads: 8, Epoch: 123, Loss: 1.7035, Validation Loss: 1.3401 \n",
            "Attention Heads: 8, Epoch: 124, Loss: 1.6851, Validation Loss: 1.3409 \n",
            "Attention Heads: 8, Epoch: 125, Loss: 1.6483, Validation Loss: 1.3506 \n",
            "Attention Heads: 8, Epoch: 126, Loss: 1.6941, Validation Loss: 1.3488 \n",
            "Attention Heads: 8, Epoch: 127, Loss: 1.6900, Validation Loss: 1.3389 \n",
            "Attention Heads: 8, Epoch: 128, Loss: 1.6937, Validation Loss: 1.3365 \n",
            "Attention Heads: 8, Epoch: 129, Loss: 1.6682, Validation Loss: 1.3442 \n",
            "Attention Heads: 8, Epoch: 130, Loss: 1.6849, Validation Loss: 1.3464 \n",
            "Attention Heads: 8, Epoch: 131, Loss: 1.6928, Validation Loss: 1.3367 \n",
            "Attention Heads: 8, Epoch: 132, Loss: 1.6704, Validation Loss: 1.3337 \n",
            "Attention Heads: 8, Epoch: 133, Loss: 1.6697, Validation Loss: 1.3437 \n",
            "Attention Heads: 8, Epoch: 134, Loss: 1.6621, Validation Loss: 1.3511 \n",
            "Attention Heads: 8, Epoch: 135, Loss: 1.6729, Validation Loss: 1.3380 \n",
            "Attention Heads: 8, Epoch: 136, Loss: 1.6720, Validation Loss: 1.3337 \n",
            "Early stopping at epoch 137 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 1.3435 \n",
            "\n",
            "Best Number of Attention Heads: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = OneLayerGAT(input_dim=CoraFull.num_node_features, output_dim=CoraFull_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, CoraFull)\n",
        "    val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, CoraFull)\n",
        "print(f'Test Accuracy(one layer) on CoraFull dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHNyaoJ8URN_",
        "outputId": "b7f8e2f4-7fbc-4415-fec1-0dfea12df205"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 72 \n",
            "\n",
            "Test Accuracy(one layer) on CoraFull dataset: 0.7016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Citeseer dataset(One layer)**"
      ],
      "metadata": {
        "id": "I5xR9XLk1SF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  citeseer dataset(one layer)**"
      ],
      "metadata": {
        "id": "7ybUEYxPUjxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = OneLayerGAT(input_dim= citeseer.num_node_features , output_dim= citeseer_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f07245-a422-4e1e-b361-28234c52d505",
        "id": "nFLYTHEPUjxz"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 1.8000, Validation Loss: 1.6394 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 1.6309, Validation Loss: 1.5077 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 1.4966, Validation Loss: 1.3924 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 1.4007, Validation Loss: 1.2932 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 1.3165, Validation Loss: 1.2080 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 1.2217, Validation Loss: 1.1350 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 1.1657, Validation Loss: 1.0727 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 1.0983, Validation Loss: 1.0191 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 1.0814, Validation Loss: 0.9750 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 1.0268, Validation Loss: 0.9376 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 0.9895, Validation Loss: 0.9056 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 0.9721, Validation Loss: 0.8789 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 0.9386, Validation Loss: 0.8562 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 0.9355, Validation Loss: 0.8364 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 0.9166, Validation Loss: 0.8193 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 0.8946, Validation Loss: 0.8041 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 0.9068, Validation Loss: 0.7904 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 0.8811, Validation Loss: 0.7784 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 0.8700, Validation Loss: 0.7677 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 0.8405, Validation Loss: 0.7582 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 0.8215, Validation Loss: 0.7504 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 0.8452, Validation Loss: 0.7438 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 0.8155, Validation Loss: 0.7388 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 0.8380, Validation Loss: 0.7349 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 0.8074, Validation Loss: 0.7314 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 0.7841, Validation Loss: 0.7284 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 0.7817, Validation Loss: 0.7260 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 0.7689, Validation Loss: 0.7243 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 0.7964, Validation Loss: 0.7233 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 0.7601, Validation Loss: 0.7228 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 0.7669, Validation Loss: 0.7230 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 0.7728, Validation Loss: 0.7235 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 0.7632, Validation Loss: 0.7243 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 0.7697, Validation Loss: 0.7254 \n",
            "Early stopping at epoch 34 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 0.7263 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 2.4740, Validation Loss: 2.1533 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 2.1541, Validation Loss: 1.9008 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 1.8989, Validation Loss: 1.7003 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 1.7182, Validation Loss: 1.5406 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 1.5627, Validation Loss: 1.4109 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 1.4641, Validation Loss: 1.3038 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 1.3465, Validation Loss: 1.2150 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 1.2759, Validation Loss: 1.1411 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 1.2135, Validation Loss: 1.0789 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 1.1918, Validation Loss: 1.0277 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 1.1292, Validation Loss: 0.9844 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 1.0979, Validation Loss: 0.9483 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 1.0496, Validation Loss: 0.9178 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 1.0323, Validation Loss: 0.8923 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 1.0241, Validation Loss: 0.8705 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 0.9926, Validation Loss: 0.8517 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 0.9858, Validation Loss: 0.8353 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 0.9740, Validation Loss: 0.8208 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 0.9379, Validation Loss: 0.8082 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 0.9322, Validation Loss: 0.7967 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 0.9370, Validation Loss: 0.7866 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 0.8991, Validation Loss: 0.7786 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 0.8970, Validation Loss: 0.7720 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 0.8657, Validation Loss: 0.7666 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 0.8863, Validation Loss: 0.7624 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 0.8710, Validation Loss: 0.7593 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 0.8647, Validation Loss: 0.7567 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 0.8506, Validation Loss: 0.7548 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 0.8727, Validation Loss: 0.7535 \n",
            "Attention Heads: 2, Epoch: 29, Loss: 0.8260, Validation Loss: 0.7525 \n",
            "Attention Heads: 2, Epoch: 30, Loss: 0.8035, Validation Loss: 0.7516 \n",
            "Attention Heads: 2, Epoch: 31, Loss: 0.8262, Validation Loss: 0.7507 \n",
            "Attention Heads: 2, Epoch: 32, Loss: 0.8221, Validation Loss: 0.7501 \n",
            "Attention Heads: 2, Epoch: 33, Loss: 0.8042, Validation Loss: 0.7495 \n",
            "Attention Heads: 2, Epoch: 34, Loss: 0.8147, Validation Loss: 0.7487 \n",
            "Attention Heads: 2, Epoch: 35, Loss: 0.8265, Validation Loss: 0.7478 \n",
            "Attention Heads: 2, Epoch: 36, Loss: 0.8138, Validation Loss: 0.7470 \n",
            "Attention Heads: 2, Epoch: 37, Loss: 0.7896, Validation Loss: 0.7463 \n",
            "Attention Heads: 2, Epoch: 38, Loss: 0.7736, Validation Loss: 0.7455 \n",
            "Attention Heads: 2, Epoch: 39, Loss: 0.7870, Validation Loss: 0.7448 \n",
            "Attention Heads: 2, Epoch: 40, Loss: 0.7866, Validation Loss: 0.7440 \n",
            "Attention Heads: 2, Epoch: 41, Loss: 0.7598, Validation Loss: 0.7430 \n",
            "Attention Heads: 2, Epoch: 42, Loss: 0.8034, Validation Loss: 0.7417 \n",
            "Attention Heads: 2, Epoch: 43, Loss: 0.7789, Validation Loss: 0.7405 \n",
            "Attention Heads: 2, Epoch: 44, Loss: 0.7940, Validation Loss: 0.7393 \n",
            "Attention Heads: 2, Epoch: 45, Loss: 0.7626, Validation Loss: 0.7381 \n",
            "Attention Heads: 2, Epoch: 46, Loss: 0.7629, Validation Loss: 0.7372 \n",
            "Attention Heads: 2, Epoch: 47, Loss: 0.7777, Validation Loss: 0.7363 \n",
            "Attention Heads: 2, Epoch: 48, Loss: 0.7769, Validation Loss: 0.7357 \n",
            "Attention Heads: 2, Epoch: 49, Loss: 0.8031, Validation Loss: 0.7351 \n",
            "Attention Heads: 2, Epoch: 50, Loss: 0.7663, Validation Loss: 0.7341 \n",
            "Attention Heads: 2, Epoch: 51, Loss: 0.7963, Validation Loss: 0.7334 \n",
            "Attention Heads: 2, Epoch: 52, Loss: 0.7497, Validation Loss: 0.7326 \n",
            "Attention Heads: 2, Epoch: 53, Loss: 0.7734, Validation Loss: 0.7314 \n",
            "Attention Heads: 2, Epoch: 54, Loss: 0.7756, Validation Loss: 0.7305 \n",
            "Attention Heads: 2, Epoch: 55, Loss: 0.7768, Validation Loss: 0.7294 \n",
            "Attention Heads: 2, Epoch: 56, Loss: 0.7638, Validation Loss: 0.7282 \n",
            "Attention Heads: 2, Epoch: 57, Loss: 0.7614, Validation Loss: 0.7271 \n",
            "Attention Heads: 2, Epoch: 58, Loss: 0.7453, Validation Loss: 0.7260 \n",
            "Attention Heads: 2, Epoch: 59, Loss: 0.7837, Validation Loss: 0.7250 \n",
            "Attention Heads: 2, Epoch: 60, Loss: 0.7833, Validation Loss: 0.7243 \n",
            "Attention Heads: 2, Epoch: 61, Loss: 0.7597, Validation Loss: 0.7235 \n",
            "Attention Heads: 2, Epoch: 62, Loss: 0.7231, Validation Loss: 0.7227 \n",
            "Attention Heads: 2, Epoch: 63, Loss: 0.7592, Validation Loss: 0.7219 \n",
            "Attention Heads: 2, Epoch: 64, Loss: 0.7432, Validation Loss: 0.7211 \n",
            "Attention Heads: 2, Epoch: 65, Loss: 0.7346, Validation Loss: 0.7204 \n",
            "Attention Heads: 2, Epoch: 66, Loss: 0.7408, Validation Loss: 0.7198 \n",
            "Attention Heads: 2, Epoch: 67, Loss: 0.7504, Validation Loss: 0.7196 \n",
            "Attention Heads: 2, Epoch: 68, Loss: 0.7448, Validation Loss: 0.7195 \n",
            "Attention Heads: 2, Epoch: 69, Loss: 0.7580, Validation Loss: 0.7197 \n",
            "Early stopping at epoch 70 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 0.7201 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 3.2009, Validation Loss: 2.7489 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 2.7497, Validation Loss: 2.3750 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 2.4034, Validation Loss: 2.0765 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 2.1306, Validation Loss: 1.8431 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 1.9223, Validation Loss: 1.6593 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 1.7729, Validation Loss: 1.5120 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 1.6235, Validation Loss: 1.3920 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 1.5282, Validation Loss: 1.2924 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 1.4160, Validation Loss: 1.2091 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 1.3802, Validation Loss: 1.1376 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 1.3127, Validation Loss: 1.0762 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 1.2461, Validation Loss: 1.0231 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 1.2413, Validation Loss: 0.9776 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 1.1835, Validation Loss: 0.9386 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 1.1305, Validation Loss: 0.9058 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 1.0907, Validation Loss: 0.8777 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 1.0560, Validation Loss: 0.8536 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 1.0454, Validation Loss: 0.8331 \n",
            "Attention Heads: 4, Epoch: 18, Loss: 1.0488, Validation Loss: 0.8159 \n",
            "Attention Heads: 4, Epoch: 19, Loss: 1.0228, Validation Loss: 0.8009 \n",
            "Attention Heads: 4, Epoch: 20, Loss: 0.9929, Validation Loss: 0.7876 \n",
            "Attention Heads: 4, Epoch: 21, Loss: 0.9918, Validation Loss: 0.7762 \n",
            "Attention Heads: 4, Epoch: 22, Loss: 0.9806, Validation Loss: 0.7662 \n",
            "Attention Heads: 4, Epoch: 23, Loss: 0.9841, Validation Loss: 0.7573 \n",
            "Attention Heads: 4, Epoch: 24, Loss: 0.9506, Validation Loss: 0.7495 \n",
            "Attention Heads: 4, Epoch: 25, Loss: 0.9513, Validation Loss: 0.7431 \n",
            "Attention Heads: 4, Epoch: 26, Loss: 0.9551, Validation Loss: 0.7378 \n",
            "Attention Heads: 4, Epoch: 27, Loss: 0.9291, Validation Loss: 0.7333 \n",
            "Attention Heads: 4, Epoch: 28, Loss: 0.9376, Validation Loss: 0.7296 \n",
            "Attention Heads: 4, Epoch: 29, Loss: 0.8896, Validation Loss: 0.7265 \n",
            "Attention Heads: 4, Epoch: 30, Loss: 0.9137, Validation Loss: 0.7244 \n",
            "Attention Heads: 4, Epoch: 31, Loss: 0.9193, Validation Loss: 0.7229 \n",
            "Attention Heads: 4, Epoch: 32, Loss: 0.9258, Validation Loss: 0.7216 \n",
            "Attention Heads: 4, Epoch: 33, Loss: 0.9081, Validation Loss: 0.7208 \n",
            "Attention Heads: 4, Epoch: 34, Loss: 0.9184, Validation Loss: 0.7202 \n",
            "Attention Heads: 4, Epoch: 35, Loss: 0.9223, Validation Loss: 0.7199 \n",
            "Attention Heads: 4, Epoch: 36, Loss: 0.8684, Validation Loss: 0.7196 \n",
            "Attention Heads: 4, Epoch: 37, Loss: 0.9022, Validation Loss: 0.7196 \n",
            "Attention Heads: 4, Epoch: 38, Loss: 0.8698, Validation Loss: 0.7198 \n",
            "Early stopping at epoch 39 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 0.7200 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 3.8473, Validation Loss: 3.3112 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 3.3141, Validation Loss: 2.8413 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 2.8841, Validation Loss: 2.4493 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 2.5599, Validation Loss: 2.1370 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 2.3177, Validation Loss: 1.8937 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 2.1286, Validation Loss: 1.7047 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 1.9379, Validation Loss: 1.5548 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 1.8137, Validation Loss: 1.4338 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 1.7268, Validation Loss: 1.3333 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 1.6006, Validation Loss: 1.2486 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 1.5618, Validation Loss: 1.1766 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 1.4938, Validation Loss: 1.1153 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 1.4152, Validation Loss: 1.0625 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 1.3633, Validation Loss: 1.0167 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 1.3400, Validation Loss: 0.9773 \n",
            "Attention Heads: 8, Epoch: 15, Loss: 1.2861, Validation Loss: 0.9434 \n",
            "Attention Heads: 8, Epoch: 16, Loss: 1.2520, Validation Loss: 0.9138 \n",
            "Attention Heads: 8, Epoch: 17, Loss: 1.2589, Validation Loss: 0.8881 \n",
            "Attention Heads: 8, Epoch: 18, Loss: 1.2132, Validation Loss: 0.8658 \n",
            "Attention Heads: 8, Epoch: 19, Loss: 1.1776, Validation Loss: 0.8464 \n",
            "Attention Heads: 8, Epoch: 20, Loss: 1.1520, Validation Loss: 0.8296 \n",
            "Attention Heads: 8, Epoch: 21, Loss: 1.1594, Validation Loss: 0.8152 \n",
            "Attention Heads: 8, Epoch: 22, Loss: 1.1129, Validation Loss: 0.8025 \n",
            "Attention Heads: 8, Epoch: 23, Loss: 1.1358, Validation Loss: 0.7917 \n",
            "Attention Heads: 8, Epoch: 24, Loss: 1.0912, Validation Loss: 0.7824 \n",
            "Attention Heads: 8, Epoch: 25, Loss: 1.1264, Validation Loss: 0.7744 \n",
            "Attention Heads: 8, Epoch: 26, Loss: 1.0546, Validation Loss: 0.7675 \n",
            "Attention Heads: 8, Epoch: 27, Loss: 1.0228, Validation Loss: 0.7616 \n",
            "Attention Heads: 8, Epoch: 28, Loss: 1.0508, Validation Loss: 0.7566 \n",
            "Attention Heads: 8, Epoch: 29, Loss: 1.0228, Validation Loss: 0.7525 \n",
            "Attention Heads: 8, Epoch: 30, Loss: 1.0445, Validation Loss: 0.7488 \n",
            "Attention Heads: 8, Epoch: 31, Loss: 1.1076, Validation Loss: 0.7457 \n",
            "Attention Heads: 8, Epoch: 32, Loss: 1.0388, Validation Loss: 0.7432 \n",
            "Attention Heads: 8, Epoch: 33, Loss: 1.0129, Validation Loss: 0.7412 \n",
            "Attention Heads: 8, Epoch: 34, Loss: 1.0237, Validation Loss: 0.7396 \n",
            "Attention Heads: 8, Epoch: 35, Loss: 1.0101, Validation Loss: 0.7383 \n",
            "Attention Heads: 8, Epoch: 36, Loss: 1.0175, Validation Loss: 0.7374 \n",
            "Attention Heads: 8, Epoch: 37, Loss: 1.0223, Validation Loss: 0.7368 \n",
            "Attention Heads: 8, Epoch: 38, Loss: 0.9946, Validation Loss: 0.7363 \n",
            "Attention Heads: 8, Epoch: 39, Loss: 1.0156, Validation Loss: 0.7360 \n",
            "Attention Heads: 8, Epoch: 40, Loss: 0.9676, Validation Loss: 0.7358 \n",
            "Attention Heads: 8, Epoch: 41, Loss: 0.9739, Validation Loss: 0.7358 \n",
            "Early stopping at epoch 42 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 0.7360 \n",
            "\n",
            "Best Number of Attention Heads: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = OneLayerGAT(input_dim=citeseer.num_node_features, output_dim=citeseer_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, citeseer)\n",
        "    val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, citeseer)\n",
        "print(f'Test Accuracy(one layer) on citeseer dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtffybfXUjx0",
        "outputId": "424e94dc-c4b9-40db-aec0-00451cf45d21"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 38 \n",
            "\n",
            "Test Accuracy(one layer) on citeseer dataset: 0.7368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Three layer GAT**"
      ],
      "metadata": {
        "id": "tS1CLFR3Ngd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ThreeLayerGAT(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, dropout=0.6, negative_slope=0.2):\n",
        "        super(ThreeLayerGAT, self).__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=num_heads, dropout=dropout, concat=True, negative_slope=negative_slope)\n",
        "        self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout, concat=True, negative_slope=negative_slope)\n",
        "        self.gat3 = GATConv(hidden_dim * num_heads, output_dim, heads=1, dropout=dropout, negative_slope=negative_slope)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.gat2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.gat3(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "J7ogpjPoNgq4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Corafull dataset(three layer)**"
      ],
      "metadata": {
        "id": "zT_uXsLQ3CCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "hidden_dims = [16 , 32, 64, 128, 256]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    model = ThreeLayerGAT(input_dim=CoraFull.num_node_features, hidden_dim= hidden_dim , output_dim=CoraFull_dataset.num_classes , num_heads = 4).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "        print(f'Hidden Dimension: {hidden_dim}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Hidden Dimension: {hidden_dim}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_hidden_dim = hidden_dim\n",
        "\n",
        "print('Best Hidden Dimension: {}'.format(best_hidden_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500f029e-6d15-4bdb-b3fd-7c04d9f73591",
        "id": "B_5HJG6m3CCZ"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 16, Epoch: 0, Loss: 4.2492, Validation Loss: 4.1232 \n",
            "Hidden Dimension: 16, Epoch: 1, Loss: 4.1367, Validation Loss: 3.8983 \n",
            "Hidden Dimension: 16, Epoch: 2, Loss: 3.9832, Validation Loss: 3.7161 \n",
            "Hidden Dimension: 16, Epoch: 3, Loss: 3.8327, Validation Loss: 3.5031 \n",
            "Hidden Dimension: 16, Epoch: 4, Loss: 3.6808, Validation Loss: 3.3070 \n",
            "Hidden Dimension: 16, Epoch: 5, Loss: 3.5263, Validation Loss: 3.1088 \n",
            "Hidden Dimension: 16, Epoch: 6, Loss: 3.3883, Validation Loss: 2.9292 \n",
            "Hidden Dimension: 16, Epoch: 7, Loss: 3.2353, Validation Loss: 2.7312 \n",
            "Hidden Dimension: 16, Epoch: 8, Loss: 3.1209, Validation Loss: 2.5419 \n",
            "Hidden Dimension: 16, Epoch: 9, Loss: 2.9720, Validation Loss: 2.3824 \n",
            "Hidden Dimension: 16, Epoch: 10, Loss: 2.8504, Validation Loss: 2.2484 \n",
            "Hidden Dimension: 16, Epoch: 11, Loss: 2.7538, Validation Loss: 2.1331 \n",
            "Hidden Dimension: 16, Epoch: 12, Loss: 2.6495, Validation Loss: 2.0222 \n",
            "Hidden Dimension: 16, Epoch: 13, Loss: 2.5806, Validation Loss: 1.9203 \n",
            "Hidden Dimension: 16, Epoch: 14, Loss: 2.4752, Validation Loss: 1.8278 \n",
            "Hidden Dimension: 16, Epoch: 15, Loss: 2.4128, Validation Loss: 1.7426 \n",
            "Hidden Dimension: 16, Epoch: 16, Loss: 2.3564, Validation Loss: 1.6710 \n",
            "Hidden Dimension: 16, Epoch: 17, Loss: 2.2872, Validation Loss: 1.6076 \n",
            "Hidden Dimension: 16, Epoch: 18, Loss: 2.2319, Validation Loss: 1.5538 \n",
            "Hidden Dimension: 16, Epoch: 19, Loss: 2.1891, Validation Loss: 1.5133 \n",
            "Hidden Dimension: 16, Epoch: 20, Loss: 2.1436, Validation Loss: 1.4874 \n",
            "Hidden Dimension: 16, Epoch: 21, Loss: 2.0807, Validation Loss: 1.4571 \n",
            "Hidden Dimension: 16, Epoch: 22, Loss: 2.0829, Validation Loss: 1.4349 \n",
            "Hidden Dimension: 16, Epoch: 23, Loss: 2.0356, Validation Loss: 1.4064 \n",
            "Hidden Dimension: 16, Epoch: 24, Loss: 2.0011, Validation Loss: 1.3895 \n",
            "Hidden Dimension: 16, Epoch: 25, Loss: 1.9794, Validation Loss: 1.3725 \n",
            "Hidden Dimension: 16, Epoch: 26, Loss: 1.9642, Validation Loss: 1.3581 \n",
            "Hidden Dimension: 16, Epoch: 27, Loss: 1.8981, Validation Loss: 1.3359 \n",
            "Hidden Dimension: 16, Epoch: 28, Loss: 1.9126, Validation Loss: 1.3122 \n",
            "Hidden Dimension: 16, Epoch: 29, Loss: 1.8973, Validation Loss: 1.2943 \n",
            "Hidden Dimension: 16, Epoch: 30, Loss: 1.8748, Validation Loss: 1.2831 \n",
            "Hidden Dimension: 16, Epoch: 31, Loss: 1.8408, Validation Loss: 1.2776 \n",
            "Hidden Dimension: 16, Epoch: 32, Loss: 1.8374, Validation Loss: 1.2604 \n",
            "Hidden Dimension: 16, Epoch: 33, Loss: 1.8295, Validation Loss: 1.2524 \n",
            "Hidden Dimension: 16, Epoch: 34, Loss: 1.8142, Validation Loss: 1.2459 \n",
            "Hidden Dimension: 16, Epoch: 35, Loss: 1.7908, Validation Loss: 1.2393 \n",
            "Hidden Dimension: 16, Epoch: 36, Loss: 1.7859, Validation Loss: 1.2335 \n",
            "Hidden Dimension: 16, Epoch: 37, Loss: 1.7683, Validation Loss: 1.2287 \n",
            "Hidden Dimension: 16, Epoch: 38, Loss: 1.7813, Validation Loss: 1.2237 \n",
            "Hidden Dimension: 16, Epoch: 39, Loss: 1.7606, Validation Loss: 1.2150 \n",
            "Hidden Dimension: 16, Epoch: 40, Loss: 1.7303, Validation Loss: 1.2069 \n",
            "Hidden Dimension: 16, Epoch: 41, Loss: 1.7441, Validation Loss: 1.2013 \n",
            "Hidden Dimension: 16, Epoch: 42, Loss: 1.7265, Validation Loss: 1.1979 \n",
            "Hidden Dimension: 16, Epoch: 43, Loss: 1.7079, Validation Loss: 1.1911 \n",
            "Hidden Dimension: 16, Epoch: 44, Loss: 1.7392, Validation Loss: 1.1933 \n",
            "Hidden Dimension: 16, Epoch: 45, Loss: 1.7207, Validation Loss: 1.1873 \n",
            "Hidden Dimension: 16, Epoch: 46, Loss: 1.6865, Validation Loss: 1.1928 \n",
            "Hidden Dimension: 16, Epoch: 47, Loss: 1.6772, Validation Loss: 1.1834 \n",
            "Hidden Dimension: 16, Epoch: 48, Loss: 1.6875, Validation Loss: 1.1844 \n",
            "Hidden Dimension: 16, Epoch: 49, Loss: 1.6796, Validation Loss: 1.1854 \n",
            "Hidden Dimension: 16, Epoch: 50, Loss: 1.6759, Validation Loss: 1.1809 \n",
            "Hidden Dimension: 16, Epoch: 51, Loss: 1.6772, Validation Loss: 1.1783 \n",
            "Hidden Dimension: 16, Epoch: 52, Loss: 1.6558, Validation Loss: 1.1832 \n",
            "Hidden Dimension: 16, Epoch: 53, Loss: 1.6617, Validation Loss: 1.1844 \n",
            "Hidden Dimension: 16, Epoch: 54, Loss: 1.6685, Validation Loss: 1.1741 \n",
            "Hidden Dimension: 16, Epoch: 55, Loss: 1.6579, Validation Loss: 1.1785 \n",
            "Hidden Dimension: 16, Epoch: 56, Loss: 1.6397, Validation Loss: 1.1788 \n",
            "Hidden Dimension: 16, Epoch: 57, Loss: 1.6092, Validation Loss: 1.1785 \n",
            "Hidden Dimension: 16, Epoch: 58, Loss: 1.6431, Validation Loss: 1.1699 \n",
            "Hidden Dimension: 16, Epoch: 59, Loss: 1.6189, Validation Loss: 1.1636 \n",
            "Hidden Dimension: 16, Epoch: 60, Loss: 1.6252, Validation Loss: 1.1680 \n",
            "Hidden Dimension: 16, Epoch: 61, Loss: 1.6351, Validation Loss: 1.1665 \n",
            "Hidden Dimension: 16, Epoch: 62, Loss: 1.6201, Validation Loss: 1.1630 \n",
            "Hidden Dimension: 16, Epoch: 63, Loss: 1.6097, Validation Loss: 1.1608 \n",
            "Hidden Dimension: 16, Epoch: 64, Loss: 1.5854, Validation Loss: 1.1555 \n",
            "Hidden Dimension: 16, Epoch: 65, Loss: 1.6155, Validation Loss: 1.1577 \n",
            "Hidden Dimension: 16, Epoch: 66, Loss: 1.5949, Validation Loss: 1.1551 \n",
            "Hidden Dimension: 16, Epoch: 67, Loss: 1.6065, Validation Loss: 1.1506 \n",
            "Hidden Dimension: 16, Epoch: 68, Loss: 1.5998, Validation Loss: 1.1534 \n",
            "Hidden Dimension: 16, Epoch: 69, Loss: 1.6005, Validation Loss: 1.1566 \n",
            "Hidden Dimension: 16, Epoch: 70, Loss: 1.5803, Validation Loss: 1.1541 \n",
            "Hidden Dimension: 16, Epoch: 71, Loss: 1.5848, Validation Loss: 1.1595 \n",
            "Early stopping at epoch 72 \n",
            "\n",
            " Hidden Dimension: 16, Validation Loss: 1.1536 \n",
            "\n",
            "Hidden Dimension: 32, Epoch: 0, Loss: 4.2437, Validation Loss: 3.9190 \n",
            "Hidden Dimension: 32, Epoch: 1, Loss: 4.0155, Validation Loss: 3.6923 \n",
            "Hidden Dimension: 32, Epoch: 2, Loss: 3.7910, Validation Loss: 3.3031 \n",
            "Hidden Dimension: 32, Epoch: 3, Loss: 3.6100, Validation Loss: 3.1163 \n",
            "Hidden Dimension: 32, Epoch: 4, Loss: 3.3978, Validation Loss: 2.8678 \n",
            "Hidden Dimension: 32, Epoch: 5, Loss: 3.1843, Validation Loss: 2.6171 \n",
            "Hidden Dimension: 32, Epoch: 6, Loss: 2.9981, Validation Loss: 2.3777 \n",
            "Hidden Dimension: 32, Epoch: 7, Loss: 2.8313, Validation Loss: 2.1635 \n",
            "Hidden Dimension: 32, Epoch: 8, Loss: 2.6682, Validation Loss: 1.9784 \n",
            "Hidden Dimension: 32, Epoch: 9, Loss: 2.5373, Validation Loss: 1.8548 \n",
            "Hidden Dimension: 32, Epoch: 10, Loss: 2.4334, Validation Loss: 1.7564 \n",
            "Hidden Dimension: 32, Epoch: 11, Loss: 2.3366, Validation Loss: 1.6687 \n",
            "Hidden Dimension: 32, Epoch: 12, Loss: 2.2326, Validation Loss: 1.5898 \n",
            "Hidden Dimension: 32, Epoch: 13, Loss: 2.1578, Validation Loss: 1.5236 \n",
            "Hidden Dimension: 32, Epoch: 14, Loss: 2.0982, Validation Loss: 1.4800 \n",
            "Hidden Dimension: 32, Epoch: 15, Loss: 2.0412, Validation Loss: 1.4361 \n",
            "Hidden Dimension: 32, Epoch: 16, Loss: 1.9861, Validation Loss: 1.4012 \n",
            "Hidden Dimension: 32, Epoch: 17, Loss: 1.9549, Validation Loss: 1.3667 \n",
            "Hidden Dimension: 32, Epoch: 18, Loss: 1.8806, Validation Loss: 1.3433 \n",
            "Hidden Dimension: 32, Epoch: 19, Loss: 1.8717, Validation Loss: 1.3166 \n",
            "Hidden Dimension: 32, Epoch: 20, Loss: 1.8267, Validation Loss: 1.2977 \n",
            "Hidden Dimension: 32, Epoch: 21, Loss: 1.8086, Validation Loss: 1.2890 \n",
            "Hidden Dimension: 32, Epoch: 22, Loss: 1.7815, Validation Loss: 1.2712 \n",
            "Hidden Dimension: 32, Epoch: 23, Loss: 1.7908, Validation Loss: 1.2476 \n",
            "Hidden Dimension: 32, Epoch: 24, Loss: 1.7483, Validation Loss: 1.2351 \n",
            "Hidden Dimension: 32, Epoch: 25, Loss: 1.7195, Validation Loss: 1.2331 \n",
            "Hidden Dimension: 32, Epoch: 26, Loss: 1.7142, Validation Loss: 1.2277 \n",
            "Hidden Dimension: 32, Epoch: 27, Loss: 1.6823, Validation Loss: 1.2251 \n",
            "Hidden Dimension: 32, Epoch: 28, Loss: 1.6787, Validation Loss: 1.2112 \n",
            "Hidden Dimension: 32, Epoch: 29, Loss: 1.6464, Validation Loss: 1.2012 \n",
            "Hidden Dimension: 32, Epoch: 30, Loss: 1.6438, Validation Loss: 1.1990 \n",
            "Hidden Dimension: 32, Epoch: 31, Loss: 1.6509, Validation Loss: 1.1992 \n",
            "Hidden Dimension: 32, Epoch: 32, Loss: 1.6338, Validation Loss: 1.1919 \n",
            "Hidden Dimension: 32, Epoch: 33, Loss: 1.6274, Validation Loss: 1.1798 \n",
            "Hidden Dimension: 32, Epoch: 34, Loss: 1.6037, Validation Loss: 1.1866 \n",
            "Hidden Dimension: 32, Epoch: 35, Loss: 1.5860, Validation Loss: 1.1840 \n",
            "Hidden Dimension: 32, Epoch: 36, Loss: 1.5933, Validation Loss: 1.1740 \n",
            "Hidden Dimension: 32, Epoch: 37, Loss: 1.5857, Validation Loss: 1.1776 \n",
            "Hidden Dimension: 32, Epoch: 38, Loss: 1.5913, Validation Loss: 1.1760 \n",
            "Hidden Dimension: 32, Epoch: 39, Loss: 1.5880, Validation Loss: 1.1811 \n",
            "Hidden Dimension: 32, Epoch: 40, Loss: 1.5458, Validation Loss: 1.1745 \n",
            "Hidden Dimension: 32, Epoch: 41, Loss: 1.5444, Validation Loss: 1.1712 \n",
            "Hidden Dimension: 32, Epoch: 42, Loss: 1.5596, Validation Loss: 1.1839 \n",
            "Hidden Dimension: 32, Epoch: 43, Loss: 1.5281, Validation Loss: 1.1778 \n",
            "Hidden Dimension: 32, Epoch: 44, Loss: 1.5476, Validation Loss: 1.1652 \n",
            "Hidden Dimension: 32, Epoch: 45, Loss: 1.5306, Validation Loss: 1.1690 \n",
            "Hidden Dimension: 32, Epoch: 46, Loss: 1.5338, Validation Loss: 1.1687 \n",
            "Hidden Dimension: 32, Epoch: 47, Loss: 1.5420, Validation Loss: 1.1660 \n",
            "Hidden Dimension: 32, Epoch: 48, Loss: 1.5436, Validation Loss: 1.1617 \n",
            "Hidden Dimension: 32, Epoch: 49, Loss: 1.5145, Validation Loss: 1.1752 \n",
            "Hidden Dimension: 32, Epoch: 50, Loss: 1.5042, Validation Loss: 1.1656 \n",
            "Hidden Dimension: 32, Epoch: 51, Loss: 1.5395, Validation Loss: 1.1570 \n",
            "Hidden Dimension: 32, Epoch: 52, Loss: 1.5033, Validation Loss: 1.1742 \n",
            "Hidden Dimension: 32, Epoch: 53, Loss: 1.5265, Validation Loss: 1.1692 \n",
            "Hidden Dimension: 32, Epoch: 54, Loss: 1.5159, Validation Loss: 1.1681 \n",
            "Hidden Dimension: 32, Epoch: 55, Loss: 1.4907, Validation Loss: 1.1711 \n",
            "Early stopping at epoch 56 \n",
            "\n",
            " Hidden Dimension: 32, Validation Loss: 1.1622 \n",
            "\n",
            "Hidden Dimension: 64, Epoch: 0, Loss: 4.2536, Validation Loss: 3.9083 \n",
            "Hidden Dimension: 64, Epoch: 1, Loss: 4.0199, Validation Loss: 3.7635 \n",
            "Hidden Dimension: 64, Epoch: 2, Loss: 3.8211, Validation Loss: 3.2753 \n",
            "Hidden Dimension: 64, Epoch: 3, Loss: 3.5015, Validation Loss: 2.8858 \n",
            "Hidden Dimension: 64, Epoch: 4, Loss: 3.2180, Validation Loss: 2.6510 \n",
            "Hidden Dimension: 64, Epoch: 5, Loss: 3.0121, Validation Loss: 2.3397 \n",
            "Hidden Dimension: 64, Epoch: 6, Loss: 2.7437, Validation Loss: 2.0449 \n",
            "Hidden Dimension: 64, Epoch: 7, Loss: 2.5070, Validation Loss: 1.8744 \n",
            "Hidden Dimension: 64, Epoch: 8, Loss: 2.3929, Validation Loss: 1.7477 \n",
            "Hidden Dimension: 64, Epoch: 9, Loss: 2.2767, Validation Loss: 1.6294 \n",
            "Hidden Dimension: 64, Epoch: 10, Loss: 2.1452, Validation Loss: 1.5788 \n",
            "Hidden Dimension: 64, Epoch: 11, Loss: 2.1038, Validation Loss: 1.4930 \n",
            "Hidden Dimension: 64, Epoch: 12, Loss: 2.0066, Validation Loss: 1.4411 \n",
            "Hidden Dimension: 64, Epoch: 13, Loss: 1.9408, Validation Loss: 1.3928 \n",
            "Hidden Dimension: 64, Epoch: 14, Loss: 1.8853, Validation Loss: 1.3699 \n",
            "Hidden Dimension: 64, Epoch: 15, Loss: 1.8443, Validation Loss: 1.3562 \n",
            "Hidden Dimension: 64, Epoch: 16, Loss: 1.8143, Validation Loss: 1.3139 \n",
            "Hidden Dimension: 64, Epoch: 17, Loss: 1.7750, Validation Loss: 1.2901 \n",
            "Hidden Dimension: 64, Epoch: 18, Loss: 1.7507, Validation Loss: 1.2732 \n",
            "Hidden Dimension: 64, Epoch: 19, Loss: 1.7270, Validation Loss: 1.2628 \n",
            "Hidden Dimension: 64, Epoch: 20, Loss: 1.6908, Validation Loss: 1.2437 \n",
            "Hidden Dimension: 64, Epoch: 21, Loss: 1.6519, Validation Loss: 1.2280 \n",
            "Hidden Dimension: 64, Epoch: 22, Loss: 1.6485, Validation Loss: 1.2130 \n",
            "Hidden Dimension: 64, Epoch: 23, Loss: 1.6216, Validation Loss: 1.2118 \n",
            "Hidden Dimension: 64, Epoch: 24, Loss: 1.6431, Validation Loss: 1.2011 \n",
            "Hidden Dimension: 64, Epoch: 25, Loss: 1.6010, Validation Loss: 1.1865 \n",
            "Hidden Dimension: 64, Epoch: 26, Loss: 1.5817, Validation Loss: 1.1813 \n",
            "Hidden Dimension: 64, Epoch: 27, Loss: 1.5763, Validation Loss: 1.1840 \n",
            "Hidden Dimension: 64, Epoch: 28, Loss: 1.5468, Validation Loss: 1.1782 \n",
            "Hidden Dimension: 64, Epoch: 29, Loss: 1.5574, Validation Loss: 1.1694 \n",
            "Hidden Dimension: 64, Epoch: 30, Loss: 1.5317, Validation Loss: 1.1657 \n",
            "Hidden Dimension: 64, Epoch: 31, Loss: 1.5170, Validation Loss: 1.1712 \n",
            "Hidden Dimension: 64, Epoch: 32, Loss: 1.5143, Validation Loss: 1.1715 \n",
            "Hidden Dimension: 64, Epoch: 33, Loss: 1.4752, Validation Loss: 1.1626 \n",
            "Hidden Dimension: 64, Epoch: 34, Loss: 1.5146, Validation Loss: 1.1548 \n",
            "Hidden Dimension: 64, Epoch: 35, Loss: 1.4794, Validation Loss: 1.1717 \n",
            "Hidden Dimension: 64, Epoch: 36, Loss: 1.5143, Validation Loss: 1.1954 \n",
            "Hidden Dimension: 64, Epoch: 37, Loss: 1.5045, Validation Loss: 1.1878 \n",
            "Hidden Dimension: 64, Epoch: 38, Loss: 1.5102, Validation Loss: 1.1891 \n",
            "Early stopping at epoch 39 \n",
            "\n",
            " Hidden Dimension: 64, Validation Loss: 1.1591 \n",
            "\n",
            "Hidden Dimension: 128, Epoch: 0, Loss: 4.2506, Validation Loss: 4.6009 \n",
            "Hidden Dimension: 128, Epoch: 1, Loss: 5.3811, Validation Loss: 3.9736 \n",
            "Hidden Dimension: 128, Epoch: 2, Loss: 4.0043, Validation Loss: 3.8555 \n",
            "Hidden Dimension: 128, Epoch: 3, Loss: 3.9657, Validation Loss: 3.6282 \n",
            "Hidden Dimension: 128, Epoch: 4, Loss: 3.7407, Validation Loss: 3.3309 \n",
            "Hidden Dimension: 128, Epoch: 5, Loss: 3.5228, Validation Loss: 2.9682 \n",
            "Hidden Dimension: 128, Epoch: 6, Loss: 3.2496, Validation Loss: 2.6572 \n",
            "Hidden Dimension: 128, Epoch: 7, Loss: 3.0313, Validation Loss: 2.4160 \n",
            "Hidden Dimension: 128, Epoch: 8, Loss: 2.8624, Validation Loss: 2.2661 \n",
            "Hidden Dimension: 128, Epoch: 9, Loss: 2.7861, Validation Loss: 2.2683 \n",
            "Hidden Dimension: 128, Epoch: 10, Loss: 2.7954, Validation Loss: 2.1147 \n",
            "Hidden Dimension: 128, Epoch: 11, Loss: 2.6250, Validation Loss: 2.0222 \n",
            "Hidden Dimension: 128, Epoch: 12, Loss: 2.4644, Validation Loss: 2.0035 \n",
            "Hidden Dimension: 128, Epoch: 13, Loss: 2.4639, Validation Loss: 1.8649 \n",
            "Hidden Dimension: 128, Epoch: 14, Loss: 2.3070, Validation Loss: 1.7850 \n",
            "Hidden Dimension: 128, Epoch: 15, Loss: 2.2472, Validation Loss: 1.7239 \n",
            "Hidden Dimension: 128, Epoch: 16, Loss: 2.1957, Validation Loss: 1.6383 \n",
            "Hidden Dimension: 128, Epoch: 17, Loss: 2.1315, Validation Loss: 1.5589 \n",
            "Hidden Dimension: 128, Epoch: 18, Loss: 2.0386, Validation Loss: 1.5163 \n",
            "Hidden Dimension: 128, Epoch: 19, Loss: 2.0100, Validation Loss: 1.4706 \n",
            "Hidden Dimension: 128, Epoch: 20, Loss: 1.9431, Validation Loss: 1.4304 \n",
            "Hidden Dimension: 128, Epoch: 21, Loss: 1.9064, Validation Loss: 1.3927 \n",
            "Hidden Dimension: 128, Epoch: 22, Loss: 1.8696, Validation Loss: 1.3735 \n",
            "Hidden Dimension: 128, Epoch: 23, Loss: 1.8499, Validation Loss: 1.3578 \n",
            "Hidden Dimension: 128, Epoch: 24, Loss: 1.8088, Validation Loss: 1.3510 \n",
            "Hidden Dimension: 128, Epoch: 25, Loss: 1.7914, Validation Loss: 1.3204 \n",
            "Hidden Dimension: 128, Epoch: 26, Loss: 1.7551, Validation Loss: 1.2913 \n",
            "Hidden Dimension: 128, Epoch: 27, Loss: 1.7276, Validation Loss: 1.2726 \n",
            "Hidden Dimension: 128, Epoch: 28, Loss: 1.6965, Validation Loss: 1.2661 \n",
            "Hidden Dimension: 128, Epoch: 29, Loss: 1.6894, Validation Loss: 1.2561 \n",
            "Hidden Dimension: 128, Epoch: 30, Loss: 1.6828, Validation Loss: 1.2483 \n",
            "Hidden Dimension: 128, Epoch: 31, Loss: 1.6715, Validation Loss: 1.2350 \n",
            "Hidden Dimension: 128, Epoch: 32, Loss: 1.6086, Validation Loss: 1.2285 \n",
            "Hidden Dimension: 128, Epoch: 33, Loss: 1.6209, Validation Loss: 1.2240 \n",
            "Hidden Dimension: 128, Epoch: 34, Loss: 1.5903, Validation Loss: 1.2041 \n",
            "Hidden Dimension: 128, Epoch: 35, Loss: 1.5918, Validation Loss: 1.2101 \n",
            "Hidden Dimension: 128, Epoch: 36, Loss: 1.5936, Validation Loss: 1.1954 \n",
            "Hidden Dimension: 128, Epoch: 37, Loss: 1.5728, Validation Loss: 1.2122 \n",
            "Hidden Dimension: 128, Epoch: 38, Loss: 1.5625, Validation Loss: 1.1926 \n",
            "Hidden Dimension: 128, Epoch: 39, Loss: 1.5601, Validation Loss: 1.1972 \n",
            "Hidden Dimension: 128, Epoch: 40, Loss: 1.5441, Validation Loss: 1.2102 \n",
            "Hidden Dimension: 128, Epoch: 41, Loss: 1.5480, Validation Loss: 1.1892 \n",
            "Hidden Dimension: 128, Epoch: 42, Loss: 1.5354, Validation Loss: 1.1811 \n",
            "Hidden Dimension: 128, Epoch: 43, Loss: 1.5117, Validation Loss: 1.1870 \n",
            "Hidden Dimension: 128, Epoch: 44, Loss: 1.5020, Validation Loss: 1.1885 \n",
            "Hidden Dimension: 128, Epoch: 45, Loss: 1.5018, Validation Loss: 1.2038 \n",
            "Hidden Dimension: 128, Epoch: 46, Loss: 1.4997, Validation Loss: 1.2235 \n",
            "Early stopping at epoch 47 \n",
            "\n",
            " Hidden Dimension: 128, Validation Loss: 1.2357 \n",
            "\n",
            "Hidden Dimension: 256, Epoch: 0, Loss: 4.2500, Validation Loss: 8.9425 \n",
            "Hidden Dimension: 256, Epoch: 1, Loss: 9.1709, Validation Loss: 4.1685 \n",
            "Hidden Dimension: 256, Epoch: 2, Loss: 4.3052, Validation Loss: 4.0121 \n",
            "Hidden Dimension: 256, Epoch: 3, Loss: 4.0391, Validation Loss: 3.8921 \n",
            "Hidden Dimension: 256, Epoch: 4, Loss: 3.9779, Validation Loss: 3.6952 \n",
            "Hidden Dimension: 256, Epoch: 5, Loss: 3.8060, Validation Loss: 3.4264 \n",
            "Hidden Dimension: 256, Epoch: 6, Loss: 3.6178, Validation Loss: 3.1282 \n",
            "Hidden Dimension: 256, Epoch: 7, Loss: 3.4011, Validation Loss: 2.8458 \n",
            "Hidden Dimension: 256, Epoch: 8, Loss: 3.2159, Validation Loss: 2.6976 \n",
            "Hidden Dimension: 256, Epoch: 9, Loss: 3.0903, Validation Loss: 2.5885 \n",
            "Hidden Dimension: 256, Epoch: 10, Loss: 3.0648, Validation Loss: 2.4262 \n",
            "Hidden Dimension: 256, Epoch: 11, Loss: 2.8444, Validation Loss: 2.3150 \n",
            "Hidden Dimension: 256, Epoch: 12, Loss: 2.7236, Validation Loss: 2.1342 \n",
            "Hidden Dimension: 256, Epoch: 13, Loss: 2.6060, Validation Loss: 2.0073 \n",
            "Hidden Dimension: 256, Epoch: 14, Loss: 2.4936, Validation Loss: 1.9241 \n",
            "Hidden Dimension: 256, Epoch: 15, Loss: 2.4314, Validation Loss: 1.8328 \n",
            "Hidden Dimension: 256, Epoch: 16, Loss: 2.3746, Validation Loss: 1.7737 \n",
            "Hidden Dimension: 256, Epoch: 17, Loss: 2.2653, Validation Loss: 1.7288 \n",
            "Hidden Dimension: 256, Epoch: 18, Loss: 2.2033, Validation Loss: 1.6648 \n",
            "Hidden Dimension: 256, Epoch: 19, Loss: 2.1306, Validation Loss: 1.6256 \n",
            "Hidden Dimension: 256, Epoch: 20, Loss: 2.1180, Validation Loss: 1.5732 \n",
            "Hidden Dimension: 256, Epoch: 21, Loss: 2.0371, Validation Loss: 1.5232 \n",
            "Hidden Dimension: 256, Epoch: 22, Loss: 1.9934, Validation Loss: 1.4856 \n",
            "Hidden Dimension: 256, Epoch: 23, Loss: 1.9634, Validation Loss: 1.4621 \n",
            "Hidden Dimension: 256, Epoch: 24, Loss: 1.9148, Validation Loss: 1.4258 \n",
            "Hidden Dimension: 256, Epoch: 25, Loss: 1.8672, Validation Loss: 1.3803 \n",
            "Hidden Dimension: 256, Epoch: 26, Loss: 1.8193, Validation Loss: 1.3517 \n",
            "Hidden Dimension: 256, Epoch: 27, Loss: 1.7815, Validation Loss: 1.3344 \n",
            "Hidden Dimension: 256, Epoch: 28, Loss: 1.7548, Validation Loss: 1.3257 \n",
            "Hidden Dimension: 256, Epoch: 29, Loss: 1.7374, Validation Loss: 1.3173 \n",
            "Hidden Dimension: 256, Epoch: 30, Loss: 1.7334, Validation Loss: 1.2915 \n",
            "Hidden Dimension: 256, Epoch: 31, Loss: 1.6925, Validation Loss: 1.2808 \n",
            "Hidden Dimension: 256, Epoch: 32, Loss: 1.6713, Validation Loss: 1.2687 \n",
            "Hidden Dimension: 256, Epoch: 33, Loss: 1.6697, Validation Loss: 1.2589 \n",
            "Hidden Dimension: 256, Epoch: 34, Loss: 1.6479, Validation Loss: 1.2407 \n",
            "Hidden Dimension: 256, Epoch: 35, Loss: 1.6169, Validation Loss: 1.2421 \n",
            "Hidden Dimension: 256, Epoch: 36, Loss: 1.6290, Validation Loss: 1.2235 \n",
            "Hidden Dimension: 256, Epoch: 37, Loss: 1.5968, Validation Loss: 1.2385 \n",
            "Hidden Dimension: 256, Epoch: 38, Loss: 1.5887, Validation Loss: 1.2290 \n",
            "Hidden Dimension: 256, Epoch: 39, Loss: 1.5727, Validation Loss: 1.2201 \n",
            "Hidden Dimension: 256, Epoch: 40, Loss: 1.5998, Validation Loss: 1.2095 \n",
            "Hidden Dimension: 256, Epoch: 41, Loss: 1.5490, Validation Loss: 1.2331 \n",
            "Hidden Dimension: 256, Epoch: 42, Loss: 1.5633, Validation Loss: 1.2210 \n",
            "Hidden Dimension: 256, Epoch: 43, Loss: 1.5458, Validation Loss: 1.2169 \n",
            "Hidden Dimension: 256, Epoch: 44, Loss: 1.5470, Validation Loss: 1.2191 \n",
            "Early stopping at epoch 45 \n",
            "\n",
            " Hidden Dimension: 256, Validation Loss: 1.2149 \n",
            "\n",
            "Best Hidden Dimension: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  cora full dataset(three layer)**"
      ],
      "metadata": {
        "id": "fz8ZHRwbWLz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = ThreeLayerGAT(input_dim= CoraFull.num_node_features ,hidden_dim=best_hidden_dim , output_dim= CoraFull_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1e0a9f-eff9-48eb-9977-83001c48d470",
        "id": "v-FS6TEgWLz7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 4.2486, Validation Loss: 4.2194 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 4.2197, Validation Loss: 4.1653 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 4.4370, Validation Loss: 4.1783 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 4.1675, Validation Loss: 4.1733 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 4.1700, Validation Loss: 4.1534 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 4.1492, Validation Loss: 4.1245 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 4.1242, Validation Loss: 4.0885 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 4.0988, Validation Loss: 4.0500 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 4.0838, Validation Loss: 4.0142 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 4.0452, Validation Loss: 3.9817 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 3.9956, Validation Loss: 3.9483 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 3.9907, Validation Loss: 3.9172 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 3.9648, Validation Loss: 3.8854 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 3.9318, Validation Loss: 3.8514 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 3.9069, Validation Loss: 3.8120 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 3.8740, Validation Loss: 3.7685 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 3.8497, Validation Loss: 3.7233 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 3.8400, Validation Loss: 3.6823 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 3.7950, Validation Loss: 3.6459 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 3.7789, Validation Loss: 3.6124 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 3.7401, Validation Loss: 3.5808 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 3.7219, Validation Loss: 3.5536 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 3.6814, Validation Loss: 3.5203 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 3.6645, Validation Loss: 3.4861 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 3.6150, Validation Loss: 3.4460 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 3.5952, Validation Loss: 3.4005 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 3.5760, Validation Loss: 3.3525 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 3.6255, Validation Loss: 3.3108 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 3.5335, Validation Loss: 3.2695 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 3.5307, Validation Loss: 3.2279 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 3.4832, Validation Loss: 3.1872 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 3.4856, Validation Loss: 3.1487 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 3.4368, Validation Loss: 3.1073 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 3.3965, Validation Loss: 3.0621 \n",
            "Attention Heads: 1, Epoch: 34, Loss: 3.3717, Validation Loss: 3.0140 \n",
            "Attention Heads: 1, Epoch: 35, Loss: 3.3366, Validation Loss: 2.9659 \n",
            "Attention Heads: 1, Epoch: 36, Loss: 3.3065, Validation Loss: 2.9231 \n",
            "Attention Heads: 1, Epoch: 37, Loss: 3.2739, Validation Loss: 2.8874 \n",
            "Attention Heads: 1, Epoch: 38, Loss: 3.2397, Validation Loss: 2.8606 \n",
            "Attention Heads: 1, Epoch: 39, Loss: 3.2140, Validation Loss: 2.8286 \n",
            "Attention Heads: 1, Epoch: 40, Loss: 3.1732, Validation Loss: 2.7882 \n",
            "Attention Heads: 1, Epoch: 41, Loss: 3.1613, Validation Loss: 2.7438 \n",
            "Attention Heads: 1, Epoch: 42, Loss: 3.1229, Validation Loss: 2.6993 \n",
            "Attention Heads: 1, Epoch: 43, Loss: 3.1051, Validation Loss: 2.6513 \n",
            "Attention Heads: 1, Epoch: 44, Loss: 3.0761, Validation Loss: 2.5996 \n",
            "Attention Heads: 1, Epoch: 45, Loss: 3.0614, Validation Loss: 2.5527 \n",
            "Attention Heads: 1, Epoch: 46, Loss: 3.0166, Validation Loss: 2.5109 \n",
            "Attention Heads: 1, Epoch: 47, Loss: 3.0117, Validation Loss: 2.4720 \n",
            "Attention Heads: 1, Epoch: 48, Loss: 2.9714, Validation Loss: 2.4329 \n",
            "Attention Heads: 1, Epoch: 49, Loss: 2.9319, Validation Loss: 2.3945 \n",
            "Attention Heads: 1, Epoch: 50, Loss: 2.9126, Validation Loss: 2.3562 \n",
            "Attention Heads: 1, Epoch: 51, Loss: 2.9064, Validation Loss: 2.3189 \n",
            "Attention Heads: 1, Epoch: 52, Loss: 2.8825, Validation Loss: 2.2863 \n",
            "Attention Heads: 1, Epoch: 53, Loss: 2.8719, Validation Loss: 2.2575 \n",
            "Attention Heads: 1, Epoch: 54, Loss: 2.8492, Validation Loss: 2.2296 \n",
            "Attention Heads: 1, Epoch: 55, Loss: 2.8159, Validation Loss: 2.2059 \n",
            "Attention Heads: 1, Epoch: 56, Loss: 2.7784, Validation Loss: 2.1844 \n",
            "Attention Heads: 1, Epoch: 57, Loss: 2.7721, Validation Loss: 2.1591 \n",
            "Attention Heads: 1, Epoch: 58, Loss: 2.7635, Validation Loss: 2.1313 \n",
            "Attention Heads: 1, Epoch: 59, Loss: 2.7474, Validation Loss: 2.1043 \n",
            "Attention Heads: 1, Epoch: 60, Loss: 2.7316, Validation Loss: 2.0794 \n",
            "Attention Heads: 1, Epoch: 61, Loss: 2.7115, Validation Loss: 2.0552 \n",
            "Attention Heads: 1, Epoch: 62, Loss: 2.6887, Validation Loss: 2.0325 \n",
            "Attention Heads: 1, Epoch: 63, Loss: 2.6550, Validation Loss: 2.0096 \n",
            "Attention Heads: 1, Epoch: 64, Loss: 2.6789, Validation Loss: 1.9872 \n",
            "Attention Heads: 1, Epoch: 65, Loss: 2.6270, Validation Loss: 1.9665 \n",
            "Attention Heads: 1, Epoch: 66, Loss: 2.6464, Validation Loss: 1.9456 \n",
            "Attention Heads: 1, Epoch: 67, Loss: 2.6266, Validation Loss: 1.9251 \n",
            "Attention Heads: 1, Epoch: 68, Loss: 2.6292, Validation Loss: 1.9070 \n",
            "Attention Heads: 1, Epoch: 69, Loss: 2.6053, Validation Loss: 1.8912 \n",
            "Attention Heads: 1, Epoch: 70, Loss: 2.5829, Validation Loss: 1.8779 \n",
            "Attention Heads: 1, Epoch: 71, Loss: 2.5825, Validation Loss: 1.8658 \n",
            "Attention Heads: 1, Epoch: 72, Loss: 2.5657, Validation Loss: 1.8537 \n",
            "Attention Heads: 1, Epoch: 73, Loss: 2.5608, Validation Loss: 1.8406 \n",
            "Attention Heads: 1, Epoch: 74, Loss: 2.5265, Validation Loss: 1.8274 \n",
            "Attention Heads: 1, Epoch: 75, Loss: 2.5448, Validation Loss: 1.8118 \n",
            "Attention Heads: 1, Epoch: 76, Loss: 2.5365, Validation Loss: 1.7980 \n",
            "Attention Heads: 1, Epoch: 77, Loss: 2.5004, Validation Loss: 1.7862 \n",
            "Attention Heads: 1, Epoch: 78, Loss: 2.5285, Validation Loss: 1.7756 \n",
            "Attention Heads: 1, Epoch: 79, Loss: 2.5144, Validation Loss: 1.7655 \n",
            "Attention Heads: 1, Epoch: 80, Loss: 2.5110, Validation Loss: 1.7569 \n",
            "Attention Heads: 1, Epoch: 81, Loss: 2.4685, Validation Loss: 1.7455 \n",
            "Attention Heads: 1, Epoch: 82, Loss: 2.4679, Validation Loss: 1.7325 \n",
            "Attention Heads: 1, Epoch: 83, Loss: 2.4675, Validation Loss: 1.7234 \n",
            "Attention Heads: 1, Epoch: 84, Loss: 2.4618, Validation Loss: 1.7142 \n",
            "Attention Heads: 1, Epoch: 85, Loss: 2.4492, Validation Loss: 1.7053 \n",
            "Attention Heads: 1, Epoch: 86, Loss: 2.4409, Validation Loss: 1.6978 \n",
            "Attention Heads: 1, Epoch: 87, Loss: 2.4558, Validation Loss: 1.6912 \n",
            "Attention Heads: 1, Epoch: 88, Loss: 2.4286, Validation Loss: 1.6839 \n",
            "Attention Heads: 1, Epoch: 89, Loss: 2.4734, Validation Loss: 1.6785 \n",
            "Attention Heads: 1, Epoch: 90, Loss: 2.4385, Validation Loss: 1.6713 \n",
            "Attention Heads: 1, Epoch: 91, Loss: 2.4198, Validation Loss: 1.6620 \n",
            "Attention Heads: 1, Epoch: 92, Loss: 2.4100, Validation Loss: 1.6531 \n",
            "Attention Heads: 1, Epoch: 93, Loss: 2.4175, Validation Loss: 1.6467 \n",
            "Attention Heads: 1, Epoch: 94, Loss: 2.4064, Validation Loss: 1.6390 \n",
            "Attention Heads: 1, Epoch: 95, Loss: 2.3812, Validation Loss: 1.6334 \n",
            "Attention Heads: 1, Epoch: 96, Loss: 2.4167, Validation Loss: 1.6300 \n",
            "Attention Heads: 1, Epoch: 97, Loss: 2.3937, Validation Loss: 1.6229 \n",
            "Attention Heads: 1, Epoch: 98, Loss: 2.3879, Validation Loss: 1.6166 \n",
            "Attention Heads: 1, Epoch: 99, Loss: 2.3710, Validation Loss: 1.6119 \n",
            "Attention Heads: 1, Epoch: 100, Loss: 2.3764, Validation Loss: 1.6077 \n",
            "Attention Heads: 1, Epoch: 101, Loss: 2.4063, Validation Loss: 1.6042 \n",
            "Attention Heads: 1, Epoch: 102, Loss: 2.3821, Validation Loss: 1.6008 \n",
            "Attention Heads: 1, Epoch: 103, Loss: 2.3639, Validation Loss: 1.5958 \n",
            "Attention Heads: 1, Epoch: 104, Loss: 2.3596, Validation Loss: 1.5886 \n",
            "Attention Heads: 1, Epoch: 105, Loss: 2.3488, Validation Loss: 1.5816 \n",
            "Attention Heads: 1, Epoch: 106, Loss: 2.3556, Validation Loss: 1.5733 \n",
            "Attention Heads: 1, Epoch: 107, Loss: 2.3444, Validation Loss: 1.5654 \n",
            "Attention Heads: 1, Epoch: 108, Loss: 2.3353, Validation Loss: 1.5574 \n",
            "Attention Heads: 1, Epoch: 109, Loss: 2.3320, Validation Loss: 1.5517 \n",
            "Attention Heads: 1, Epoch: 110, Loss: 2.3092, Validation Loss: 1.5465 \n",
            "Attention Heads: 1, Epoch: 111, Loss: 2.3218, Validation Loss: 1.5456 \n",
            "Attention Heads: 1, Epoch: 112, Loss: 2.3168, Validation Loss: 1.5444 \n",
            "Attention Heads: 1, Epoch: 113, Loss: 2.3419, Validation Loss: 1.5416 \n",
            "Attention Heads: 1, Epoch: 114, Loss: 2.3122, Validation Loss: 1.5376 \n",
            "Attention Heads: 1, Epoch: 115, Loss: 2.3121, Validation Loss: 1.5325 \n",
            "Attention Heads: 1, Epoch: 116, Loss: 2.3042, Validation Loss: 1.5281 \n",
            "Attention Heads: 1, Epoch: 117, Loss: 2.3137, Validation Loss: 1.5252 \n",
            "Attention Heads: 1, Epoch: 118, Loss: 2.3252, Validation Loss: 1.5264 \n",
            "Attention Heads: 1, Epoch: 119, Loss: 2.3019, Validation Loss: 1.5258 \n",
            "Attention Heads: 1, Epoch: 120, Loss: 2.2981, Validation Loss: 1.5203 \n",
            "Attention Heads: 1, Epoch: 121, Loss: 2.3041, Validation Loss: 1.5129 \n",
            "Attention Heads: 1, Epoch: 122, Loss: 2.2978, Validation Loss: 1.5051 \n",
            "Attention Heads: 1, Epoch: 123, Loss: 2.2970, Validation Loss: 1.4961 \n",
            "Attention Heads: 1, Epoch: 124, Loss: 2.2981, Validation Loss: 1.4928 \n",
            "Attention Heads: 1, Epoch: 125, Loss: 2.2945, Validation Loss: 1.4939 \n",
            "Attention Heads: 1, Epoch: 126, Loss: 2.2854, Validation Loss: 1.4903 \n",
            "Attention Heads: 1, Epoch: 127, Loss: 2.2915, Validation Loss: 1.4905 \n",
            "Attention Heads: 1, Epoch: 128, Loss: 2.2841, Validation Loss: 1.4921 \n",
            "Attention Heads: 1, Epoch: 129, Loss: 2.2786, Validation Loss: 1.4877 \n",
            "Attention Heads: 1, Epoch: 130, Loss: 2.2620, Validation Loss: 1.4858 \n",
            "Attention Heads: 1, Epoch: 131, Loss: 2.2636, Validation Loss: 1.4858 \n",
            "Attention Heads: 1, Epoch: 132, Loss: 2.2513, Validation Loss: 1.4809 \n",
            "Attention Heads: 1, Epoch: 133, Loss: 2.2902, Validation Loss: 1.4779 \n",
            "Attention Heads: 1, Epoch: 134, Loss: 2.2676, Validation Loss: 1.4745 \n",
            "Attention Heads: 1, Epoch: 135, Loss: 2.2619, Validation Loss: 1.4668 \n",
            "Attention Heads: 1, Epoch: 136, Loss: 2.2391, Validation Loss: 1.4613 \n",
            "Attention Heads: 1, Epoch: 137, Loss: 2.2581, Validation Loss: 1.4577 \n",
            "Attention Heads: 1, Epoch: 138, Loss: 2.2446, Validation Loss: 1.4543 \n",
            "Attention Heads: 1, Epoch: 139, Loss: 2.2552, Validation Loss: 1.4525 \n",
            "Attention Heads: 1, Epoch: 140, Loss: 2.2448, Validation Loss: 1.4492 \n",
            "Attention Heads: 1, Epoch: 141, Loss: 2.2387, Validation Loss: 1.4459 \n",
            "Attention Heads: 1, Epoch: 142, Loss: 2.2380, Validation Loss: 1.4422 \n",
            "Attention Heads: 1, Epoch: 143, Loss: 2.2272, Validation Loss: 1.4407 \n",
            "Attention Heads: 1, Epoch: 144, Loss: 2.2181, Validation Loss: 1.4388 \n",
            "Attention Heads: 1, Epoch: 145, Loss: 2.2475, Validation Loss: 1.4385 \n",
            "Attention Heads: 1, Epoch: 146, Loss: 2.2449, Validation Loss: 1.4386 \n",
            "Attention Heads: 1, Epoch: 147, Loss: 2.2592, Validation Loss: 1.4386 \n",
            "Attention Heads: 1, Epoch: 148, Loss: 2.2151, Validation Loss: 1.4379 \n",
            "Attention Heads: 1, Epoch: 149, Loss: 2.2115, Validation Loss: 1.4357 \n",
            "Attention Heads: 1, Epoch: 150, Loss: 2.2237, Validation Loss: 1.4313 \n",
            "Attention Heads: 1, Epoch: 151, Loss: 2.2615, Validation Loss: 1.4326 \n",
            "Attention Heads: 1, Epoch: 152, Loss: 2.2284, Validation Loss: 1.4312 \n",
            "Attention Heads: 1, Epoch: 153, Loss: 2.2238, Validation Loss: 1.4293 \n",
            "Attention Heads: 1, Epoch: 154, Loss: 2.2136, Validation Loss: 1.4254 \n",
            "Attention Heads: 1, Epoch: 155, Loss: 2.2378, Validation Loss: 1.4249 \n",
            "Attention Heads: 1, Epoch: 156, Loss: 2.2275, Validation Loss: 1.4262 \n",
            "Attention Heads: 1, Epoch: 157, Loss: 2.2299, Validation Loss: 1.4267 \n",
            "Attention Heads: 1, Epoch: 158, Loss: 2.2163, Validation Loss: 1.4281 \n",
            "Early stopping at epoch 159 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 1.4267 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 4.2479, Validation Loss: 4.1530 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 4.1557, Validation Loss: 3.9868 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 4.2810, Validation Loss: 4.0487 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 4.0547, Validation Loss: 4.0583 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 4.0657, Validation Loss: 4.0136 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 4.0362, Validation Loss: 3.9423 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 3.9839, Validation Loss: 3.8576 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 3.9278, Validation Loss: 3.7659 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 3.8581, Validation Loss: 3.6661 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 3.7894, Validation Loss: 3.5644 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 3.7119, Validation Loss: 3.4658 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 3.6509, Validation Loss: 3.3719 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 3.5915, Validation Loss: 3.2817 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 3.5324, Validation Loss: 3.1859 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 3.4557, Validation Loss: 3.0881 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 3.3960, Validation Loss: 3.0027 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 3.3177, Validation Loss: 2.9250 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 3.2753, Validation Loss: 2.8539 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 3.2345, Validation Loss: 2.7847 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 3.1538, Validation Loss: 2.7172 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 3.1102, Validation Loss: 2.6447 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 3.0522, Validation Loss: 2.5756 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 2.9778, Validation Loss: 2.5116 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 2.9282, Validation Loss: 2.4432 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 2.8924, Validation Loss: 2.3697 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 2.8433, Validation Loss: 2.2948 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 2.8013, Validation Loss: 2.2267 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 2.7449, Validation Loss: 2.1662 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 2.6913, Validation Loss: 2.1138 \n",
            "Attention Heads: 2, Epoch: 29, Loss: 2.6372, Validation Loss: 2.0632 \n",
            "Attention Heads: 2, Epoch: 30, Loss: 2.6164, Validation Loss: 2.0137 \n",
            "Attention Heads: 2, Epoch: 31, Loss: 2.5707, Validation Loss: 1.9640 \n",
            "Attention Heads: 2, Epoch: 32, Loss: 2.5435, Validation Loss: 1.9201 \n",
            "Attention Heads: 2, Epoch: 33, Loss: 2.5215, Validation Loss: 1.8786 \n",
            "Attention Heads: 2, Epoch: 34, Loss: 2.4565, Validation Loss: 1.8368 \n",
            "Attention Heads: 2, Epoch: 35, Loss: 2.4351, Validation Loss: 1.7949 \n",
            "Attention Heads: 2, Epoch: 36, Loss: 2.4080, Validation Loss: 1.7576 \n",
            "Attention Heads: 2, Epoch: 37, Loss: 2.3961, Validation Loss: 1.7223 \n",
            "Attention Heads: 2, Epoch: 38, Loss: 2.3637, Validation Loss: 1.6895 \n",
            "Attention Heads: 2, Epoch: 39, Loss: 2.3390, Validation Loss: 1.6609 \n",
            "Attention Heads: 2, Epoch: 40, Loss: 2.3151, Validation Loss: 1.6365 \n",
            "Attention Heads: 2, Epoch: 41, Loss: 2.2769, Validation Loss: 1.6133 \n",
            "Attention Heads: 2, Epoch: 42, Loss: 2.2666, Validation Loss: 1.5918 \n",
            "Attention Heads: 2, Epoch: 43, Loss: 2.2329, Validation Loss: 1.5748 \n",
            "Attention Heads: 2, Epoch: 44, Loss: 2.2244, Validation Loss: 1.5575 \n",
            "Attention Heads: 2, Epoch: 45, Loss: 2.2446, Validation Loss: 1.5401 \n",
            "Attention Heads: 2, Epoch: 46, Loss: 2.1914, Validation Loss: 1.5240 \n",
            "Attention Heads: 2, Epoch: 47, Loss: 2.1747, Validation Loss: 1.5104 \n",
            "Attention Heads: 2, Epoch: 48, Loss: 2.1754, Validation Loss: 1.4992 \n",
            "Attention Heads: 2, Epoch: 49, Loss: 2.1499, Validation Loss: 1.4863 \n",
            "Attention Heads: 2, Epoch: 50, Loss: 2.1087, Validation Loss: 1.4716 \n",
            "Attention Heads: 2, Epoch: 51, Loss: 2.1432, Validation Loss: 1.4577 \n",
            "Attention Heads: 2, Epoch: 52, Loss: 2.1069, Validation Loss: 1.4470 \n",
            "Attention Heads: 2, Epoch: 53, Loss: 2.0928, Validation Loss: 1.4341 \n",
            "Attention Heads: 2, Epoch: 54, Loss: 2.0834, Validation Loss: 1.4228 \n",
            "Attention Heads: 2, Epoch: 55, Loss: 2.0893, Validation Loss: 1.4131 \n",
            "Attention Heads: 2, Epoch: 56, Loss: 2.0776, Validation Loss: 1.4071 \n",
            "Attention Heads: 2, Epoch: 57, Loss: 2.0451, Validation Loss: 1.4022 \n",
            "Attention Heads: 2, Epoch: 58, Loss: 2.0464, Validation Loss: 1.3910 \n",
            "Attention Heads: 2, Epoch: 59, Loss: 2.0282, Validation Loss: 1.3786 \n",
            "Attention Heads: 2, Epoch: 60, Loss: 2.0165, Validation Loss: 1.3685 \n",
            "Attention Heads: 2, Epoch: 61, Loss: 2.0233, Validation Loss: 1.3609 \n",
            "Attention Heads: 2, Epoch: 62, Loss: 2.0225, Validation Loss: 1.3592 \n",
            "Attention Heads: 2, Epoch: 63, Loss: 1.9952, Validation Loss: 1.3579 \n",
            "Attention Heads: 2, Epoch: 64, Loss: 2.0051, Validation Loss: 1.3517 \n",
            "Attention Heads: 2, Epoch: 65, Loss: 1.9791, Validation Loss: 1.3465 \n",
            "Attention Heads: 2, Epoch: 66, Loss: 1.9876, Validation Loss: 1.3439 \n",
            "Attention Heads: 2, Epoch: 67, Loss: 1.9999, Validation Loss: 1.3404 \n",
            "Attention Heads: 2, Epoch: 68, Loss: 1.9825, Validation Loss: 1.3308 \n",
            "Attention Heads: 2, Epoch: 69, Loss: 1.9656, Validation Loss: 1.3267 \n",
            "Attention Heads: 2, Epoch: 70, Loss: 1.9792, Validation Loss: 1.3225 \n",
            "Attention Heads: 2, Epoch: 71, Loss: 1.9589, Validation Loss: 1.3232 \n",
            "Attention Heads: 2, Epoch: 72, Loss: 1.9454, Validation Loss: 1.3197 \n",
            "Attention Heads: 2, Epoch: 73, Loss: 1.9565, Validation Loss: 1.3066 \n",
            "Attention Heads: 2, Epoch: 74, Loss: 1.9398, Validation Loss: 1.2976 \n",
            "Attention Heads: 2, Epoch: 75, Loss: 1.9213, Validation Loss: 1.2970 \n",
            "Attention Heads: 2, Epoch: 76, Loss: 1.9503, Validation Loss: 1.2957 \n",
            "Attention Heads: 2, Epoch: 77, Loss: 1.9123, Validation Loss: 1.2992 \n",
            "Attention Heads: 2, Epoch: 78, Loss: 1.9148, Validation Loss: 1.2909 \n",
            "Attention Heads: 2, Epoch: 79, Loss: 1.9462, Validation Loss: 1.2868 \n",
            "Attention Heads: 2, Epoch: 80, Loss: 1.9118, Validation Loss: 1.2834 \n",
            "Attention Heads: 2, Epoch: 81, Loss: 1.9105, Validation Loss: 1.2819 \n",
            "Attention Heads: 2, Epoch: 82, Loss: 1.8976, Validation Loss: 1.2834 \n",
            "Attention Heads: 2, Epoch: 83, Loss: 1.9050, Validation Loss: 1.2750 \n",
            "Attention Heads: 2, Epoch: 84, Loss: 1.8967, Validation Loss: 1.2713 \n",
            "Attention Heads: 2, Epoch: 85, Loss: 1.9048, Validation Loss: 1.2667 \n",
            "Attention Heads: 2, Epoch: 86, Loss: 1.8752, Validation Loss: 1.2646 \n",
            "Attention Heads: 2, Epoch: 87, Loss: 1.8825, Validation Loss: 1.2582 \n",
            "Attention Heads: 2, Epoch: 88, Loss: 1.8826, Validation Loss: 1.2515 \n",
            "Attention Heads: 2, Epoch: 89, Loss: 1.8816, Validation Loss: 1.2503 \n",
            "Attention Heads: 2, Epoch: 90, Loss: 1.8952, Validation Loss: 1.2516 \n",
            "Attention Heads: 2, Epoch: 91, Loss: 1.8609, Validation Loss: 1.2488 \n",
            "Attention Heads: 2, Epoch: 92, Loss: 1.8718, Validation Loss: 1.2459 \n",
            "Attention Heads: 2, Epoch: 93, Loss: 1.8598, Validation Loss: 1.2457 \n",
            "Attention Heads: 2, Epoch: 94, Loss: 1.8640, Validation Loss: 1.2453 \n",
            "Attention Heads: 2, Epoch: 95, Loss: 1.8811, Validation Loss: 1.2465 \n",
            "Attention Heads: 2, Epoch: 96, Loss: 1.8734, Validation Loss: 1.2460 \n",
            "Attention Heads: 2, Epoch: 97, Loss: 1.8533, Validation Loss: 1.2431 \n",
            "Attention Heads: 2, Epoch: 98, Loss: 1.8402, Validation Loss: 1.2380 \n",
            "Attention Heads: 2, Epoch: 99, Loss: 1.8472, Validation Loss: 1.2375 \n",
            "Attention Heads: 2, Epoch: 100, Loss: 1.8369, Validation Loss: 1.2361 \n",
            "Attention Heads: 2, Epoch: 101, Loss: 1.8569, Validation Loss: 1.2337 \n",
            "Attention Heads: 2, Epoch: 102, Loss: 1.8506, Validation Loss: 1.2337 \n",
            "Attention Heads: 2, Epoch: 103, Loss: 1.8451, Validation Loss: 1.2307 \n",
            "Attention Heads: 2, Epoch: 104, Loss: 1.8302, Validation Loss: 1.2292 \n",
            "Attention Heads: 2, Epoch: 105, Loss: 1.8354, Validation Loss: 1.2274 \n",
            "Attention Heads: 2, Epoch: 106, Loss: 1.8342, Validation Loss: 1.2281 \n",
            "Attention Heads: 2, Epoch: 107, Loss: 1.8038, Validation Loss: 1.2335 \n",
            "Attention Heads: 2, Epoch: 108, Loss: 1.8346, Validation Loss: 1.2286 \n",
            "Attention Heads: 2, Epoch: 109, Loss: 1.8297, Validation Loss: 1.2316 \n",
            "Early stopping at epoch 110 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 1.2281 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 4.2494, Validation Loss: 4.1200 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 4.1360, Validation Loss: 3.9244 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 3.9872, Validation Loss: 3.7644 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 3.8202, Validation Loss: 3.5147 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 3.6677, Validation Loss: 3.2859 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 3.5161, Validation Loss: 3.1064 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 3.3467, Validation Loss: 2.9175 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 3.2103, Validation Loss: 2.7481 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 3.1052, Validation Loss: 2.5907 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 2.9662, Validation Loss: 2.4248 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 2.8526, Validation Loss: 2.2759 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 2.7466, Validation Loss: 2.1464 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 2.6382, Validation Loss: 2.0223 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 2.5380, Validation Loss: 1.9110 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 2.4861, Validation Loss: 1.8277 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 2.3870, Validation Loss: 1.7585 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 2.3397, Validation Loss: 1.6945 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 2.2889, Validation Loss: 1.6388 \n",
            "Attention Heads: 4, Epoch: 18, Loss: 2.2159, Validation Loss: 1.5837 \n",
            "Attention Heads: 4, Epoch: 19, Loss: 2.1853, Validation Loss: 1.5318 \n",
            "Attention Heads: 4, Epoch: 20, Loss: 2.1385, Validation Loss: 1.4874 \n",
            "Attention Heads: 4, Epoch: 21, Loss: 2.0737, Validation Loss: 1.4499 \n",
            "Attention Heads: 4, Epoch: 22, Loss: 2.0668, Validation Loss: 1.4189 \n",
            "Attention Heads: 4, Epoch: 23, Loss: 2.0293, Validation Loss: 1.3912 \n",
            "Attention Heads: 4, Epoch: 24, Loss: 1.9952, Validation Loss: 1.3578 \n",
            "Attention Heads: 4, Epoch: 25, Loss: 1.9698, Validation Loss: 1.3330 \n",
            "Attention Heads: 4, Epoch: 26, Loss: 1.9099, Validation Loss: 1.3222 \n",
            "Attention Heads: 4, Epoch: 27, Loss: 1.8973, Validation Loss: 1.3123 \n",
            "Attention Heads: 4, Epoch: 28, Loss: 1.9139, Validation Loss: 1.2970 \n",
            "Attention Heads: 4, Epoch: 29, Loss: 1.8664, Validation Loss: 1.2861 \n",
            "Attention Heads: 4, Epoch: 30, Loss: 1.8647, Validation Loss: 1.2775 \n",
            "Attention Heads: 4, Epoch: 31, Loss: 1.8427, Validation Loss: 1.2671 \n",
            "Attention Heads: 4, Epoch: 32, Loss: 1.8160, Validation Loss: 1.2593 \n",
            "Attention Heads: 4, Epoch: 33, Loss: 1.8006, Validation Loss: 1.2523 \n",
            "Attention Heads: 4, Epoch: 34, Loss: 1.7819, Validation Loss: 1.2397 \n",
            "Attention Heads: 4, Epoch: 35, Loss: 1.7868, Validation Loss: 1.2312 \n",
            "Attention Heads: 4, Epoch: 36, Loss: 1.7621, Validation Loss: 1.2343 \n",
            "Attention Heads: 4, Epoch: 37, Loss: 1.7704, Validation Loss: 1.2337 \n",
            "Attention Heads: 4, Epoch: 38, Loss: 1.7531, Validation Loss: 1.2231 \n",
            "Attention Heads: 4, Epoch: 39, Loss: 1.7393, Validation Loss: 1.2163 \n",
            "Attention Heads: 4, Epoch: 40, Loss: 1.7511, Validation Loss: 1.2232 \n",
            "Attention Heads: 4, Epoch: 41, Loss: 1.6965, Validation Loss: 1.2162 \n",
            "Attention Heads: 4, Epoch: 42, Loss: 1.7059, Validation Loss: 1.2083 \n",
            "Attention Heads: 4, Epoch: 43, Loss: 1.7292, Validation Loss: 1.1968 \n",
            "Attention Heads: 4, Epoch: 44, Loss: 1.6930, Validation Loss: 1.2097 \n",
            "Attention Heads: 4, Epoch: 45, Loss: 1.7223, Validation Loss: 1.1991 \n",
            "Attention Heads: 4, Epoch: 46, Loss: 1.6956, Validation Loss: 1.1958 \n",
            "Attention Heads: 4, Epoch: 47, Loss: 1.6648, Validation Loss: 1.1986 \n",
            "Attention Heads: 4, Epoch: 48, Loss: 1.6769, Validation Loss: 1.1868 \n",
            "Attention Heads: 4, Epoch: 49, Loss: 1.6803, Validation Loss: 1.1844 \n",
            "Attention Heads: 4, Epoch: 50, Loss: 1.6719, Validation Loss: 1.1774 \n",
            "Attention Heads: 4, Epoch: 51, Loss: 1.6651, Validation Loss: 1.1738 \n",
            "Attention Heads: 4, Epoch: 52, Loss: 1.6424, Validation Loss: 1.1755 \n",
            "Attention Heads: 4, Epoch: 53, Loss: 1.6484, Validation Loss: 1.1759 \n",
            "Attention Heads: 4, Epoch: 54, Loss: 1.6521, Validation Loss: 1.1716 \n",
            "Attention Heads: 4, Epoch: 55, Loss: 1.6597, Validation Loss: 1.1730 \n",
            "Attention Heads: 4, Epoch: 56, Loss: 1.6304, Validation Loss: 1.1733 \n",
            "Attention Heads: 4, Epoch: 57, Loss: 1.6268, Validation Loss: 1.1728 \n",
            "Attention Heads: 4, Epoch: 58, Loss: 1.6112, Validation Loss: 1.1699 \n",
            "Attention Heads: 4, Epoch: 59, Loss: 1.6054, Validation Loss: 1.1741 \n",
            "Attention Heads: 4, Epoch: 60, Loss: 1.6279, Validation Loss: 1.1718 \n",
            "Attention Heads: 4, Epoch: 61, Loss: 1.6125, Validation Loss: 1.1697 \n",
            "Attention Heads: 4, Epoch: 62, Loss: 1.6181, Validation Loss: 1.1667 \n",
            "Attention Heads: 4, Epoch: 63, Loss: 1.6070, Validation Loss: 1.1674 \n",
            "Attention Heads: 4, Epoch: 64, Loss: 1.6071, Validation Loss: 1.1675 \n",
            "Attention Heads: 4, Epoch: 65, Loss: 1.6022, Validation Loss: 1.1682 \n",
            "Attention Heads: 4, Epoch: 66, Loss: 1.6024, Validation Loss: 1.1683 \n",
            "Early stopping at epoch 67 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 1.1715 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 4.2509, Validation Loss: 4.0222 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 4.0512, Validation Loss: 3.6820 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 3.9071, Validation Loss: 3.5641 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 3.6800, Validation Loss: 3.3946 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 3.5073, Validation Loss: 3.0648 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 3.2724, Validation Loss: 2.7298 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 3.0623, Validation Loss: 2.4828 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 2.8929, Validation Loss: 2.2532 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 2.7080, Validation Loss: 2.0975 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 2.5682, Validation Loss: 1.9621 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 2.4618, Validation Loss: 1.8239 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 2.3319, Validation Loss: 1.7004 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 2.2283, Validation Loss: 1.6121 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 2.1654, Validation Loss: 1.5432 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 2.1004, Validation Loss: 1.4880 \n",
            "Attention Heads: 8, Epoch: 15, Loss: 2.0477, Validation Loss: 1.4483 \n",
            "Attention Heads: 8, Epoch: 16, Loss: 1.9740, Validation Loss: 1.4063 \n",
            "Attention Heads: 8, Epoch: 17, Loss: 1.9428, Validation Loss: 1.3652 \n",
            "Attention Heads: 8, Epoch: 18, Loss: 1.9007, Validation Loss: 1.3386 \n",
            "Attention Heads: 8, Epoch: 19, Loss: 1.8700, Validation Loss: 1.3205 \n",
            "Attention Heads: 8, Epoch: 20, Loss: 1.8062, Validation Loss: 1.3086 \n",
            "Attention Heads: 8, Epoch: 21, Loss: 1.8117, Validation Loss: 1.2992 \n",
            "Attention Heads: 8, Epoch: 22, Loss: 1.7861, Validation Loss: 1.2794 \n",
            "Attention Heads: 8, Epoch: 23, Loss: 1.7430, Validation Loss: 1.2660 \n",
            "Attention Heads: 8, Epoch: 24, Loss: 1.7243, Validation Loss: 1.2500 \n",
            "Attention Heads: 8, Epoch: 25, Loss: 1.7121, Validation Loss: 1.2425 \n",
            "Attention Heads: 8, Epoch: 26, Loss: 1.6983, Validation Loss: 1.2303 \n",
            "Attention Heads: 8, Epoch: 27, Loss: 1.6796, Validation Loss: 1.2140 \n",
            "Attention Heads: 8, Epoch: 28, Loss: 1.6737, Validation Loss: 1.1979 \n",
            "Attention Heads: 8, Epoch: 29, Loss: 1.6403, Validation Loss: 1.1888 \n",
            "Attention Heads: 8, Epoch: 30, Loss: 1.6009, Validation Loss: 1.1892 \n",
            "Attention Heads: 8, Epoch: 31, Loss: 1.6130, Validation Loss: 1.1872 \n",
            "Attention Heads: 8, Epoch: 32, Loss: 1.6283, Validation Loss: 1.1809 \n",
            "Attention Heads: 8, Epoch: 33, Loss: 1.5841, Validation Loss: 1.1789 \n",
            "Attention Heads: 8, Epoch: 34, Loss: 1.5793, Validation Loss: 1.1731 \n",
            "Attention Heads: 8, Epoch: 35, Loss: 1.5638, Validation Loss: 1.1761 \n",
            "Attention Heads: 8, Epoch: 36, Loss: 1.5444, Validation Loss: 1.1697 \n",
            "Attention Heads: 8, Epoch: 37, Loss: 1.5560, Validation Loss: 1.1682 \n",
            "Attention Heads: 8, Epoch: 38, Loss: 1.5492, Validation Loss: 1.1725 \n",
            "Attention Heads: 8, Epoch: 39, Loss: 1.5498, Validation Loss: 1.1710 \n",
            "Attention Heads: 8, Epoch: 40, Loss: 1.5327, Validation Loss: 1.1653 \n",
            "Attention Heads: 8, Epoch: 41, Loss: 1.4958, Validation Loss: 1.1714 \n",
            "Attention Heads: 8, Epoch: 42, Loss: 1.5189, Validation Loss: 1.1689 \n",
            "Attention Heads: 8, Epoch: 43, Loss: 1.5038, Validation Loss: 1.1705 \n",
            "Attention Heads: 8, Epoch: 44, Loss: 1.5045, Validation Loss: 1.1798 \n",
            "Early stopping at epoch 45 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 1.1673 \n",
            "\n",
            "Best Number of Attention Heads: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = ThreeLayerGAT(input_dim=CoraFull.num_node_features, hidden_dim=best_hidden_dim , output_dim=CoraFull_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, CoraFull)\n",
        "    val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, CoraFull)\n",
        "print(f'Test Accuracy(three layer) on CoraFull dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsYiHP6qWLz9",
        "outputId": "630e7af8-2977-429f-8e8e-a300021ed33c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 42 \n",
            "\n",
            "Test Accuracy(three layer) on CoraFull dataset: 0.6930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Citeseer dataset(three layer)**"
      ],
      "metadata": {
        "id": "_sajPny13CCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "hidden_dims = [16 , 32, 64, 128, 256]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    model = ThreeLayerGAT(input_dim=citeseer.num_node_features, hidden_dim= hidden_dim , output_dim= citeseer_dataset.num_classes , num_heads = 4).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "        # Print the loss\n",
        "        print(f'Hidden Dimension: {hidden_dim}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Hidden Dimension: {hidden_dim}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_hidden_dim = hidden_dim\n",
        "\n",
        "print('Best Hidden Dimension: {}'.format(best_hidden_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaea4b1e-a881-4519-d348-8a6e7d1d9b9d",
        "id": "M0bdwXQXs2Gy"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 16, Epoch: 0, Loss: 1.7945, Validation Loss: 1.6741 \n",
            "Hidden Dimension: 16, Epoch: 1, Loss: 1.6708, Validation Loss: 1.4569 \n",
            "Hidden Dimension: 16, Epoch: 2, Loss: 1.5316, Validation Loss: 1.2061 \n",
            "Hidden Dimension: 16, Epoch: 3, Loss: 1.3840, Validation Loss: 1.0382 \n",
            "Hidden Dimension: 16, Epoch: 4, Loss: 1.2975, Validation Loss: 0.9378 \n",
            "Hidden Dimension: 16, Epoch: 5, Loss: 1.2030, Validation Loss: 0.8562 \n",
            "Hidden Dimension: 16, Epoch: 6, Loss: 1.1838, Validation Loss: 0.8105 \n",
            "Hidden Dimension: 16, Epoch: 7, Loss: 1.1351, Validation Loss: 0.7824 \n",
            "Hidden Dimension: 16, Epoch: 8, Loss: 1.1033, Validation Loss: 0.7408 \n",
            "Hidden Dimension: 16, Epoch: 9, Loss: 1.0629, Validation Loss: 0.7045 \n",
            "Hidden Dimension: 16, Epoch: 10, Loss: 1.0370, Validation Loss: 0.6816 \n",
            "Hidden Dimension: 16, Epoch: 11, Loss: 1.0257, Validation Loss: 0.6693 \n",
            "Hidden Dimension: 16, Epoch: 12, Loss: 0.9782, Validation Loss: 0.6632 \n",
            "Hidden Dimension: 16, Epoch: 13, Loss: 0.9589, Validation Loss: 0.6582 \n",
            "Hidden Dimension: 16, Epoch: 14, Loss: 0.9522, Validation Loss: 0.6524 \n",
            "Hidden Dimension: 16, Epoch: 15, Loss: 0.9441, Validation Loss: 0.6486 \n",
            "Hidden Dimension: 16, Epoch: 16, Loss: 0.9329, Validation Loss: 0.6493 \n",
            "Hidden Dimension: 16, Epoch: 17, Loss: 0.9162, Validation Loss: 0.6504 \n",
            "Hidden Dimension: 16, Epoch: 18, Loss: 0.9238, Validation Loss: 0.6515 \n",
            "Hidden Dimension: 16, Epoch: 19, Loss: 0.8781, Validation Loss: 0.6552 \n",
            "Early stopping at epoch 20 \n",
            "\n",
            " Hidden Dimension: 16, Validation Loss: 0.6590 \n",
            "\n",
            "Hidden Dimension: 32, Epoch: 0, Loss: 1.8081, Validation Loss: 1.6061 \n",
            "Hidden Dimension: 32, Epoch: 1, Loss: 1.6455, Validation Loss: 1.2959 \n",
            "Hidden Dimension: 32, Epoch: 2, Loss: 1.4238, Validation Loss: 0.9926 \n",
            "Hidden Dimension: 32, Epoch: 3, Loss: 1.2706, Validation Loss: 0.8497 \n",
            "Hidden Dimension: 32, Epoch: 4, Loss: 1.2085, Validation Loss: 0.7618 \n",
            "Hidden Dimension: 32, Epoch: 5, Loss: 1.1407, Validation Loss: 0.7263 \n",
            "Hidden Dimension: 32, Epoch: 6, Loss: 1.0495, Validation Loss: 0.7516 \n",
            "Hidden Dimension: 32, Epoch: 7, Loss: 1.0606, Validation Loss: 0.7210 \n",
            "Hidden Dimension: 32, Epoch: 8, Loss: 1.0212, Validation Loss: 0.7142 \n",
            "Hidden Dimension: 32, Epoch: 9, Loss: 0.9896, Validation Loss: 0.7166 \n",
            "Hidden Dimension: 32, Epoch: 10, Loss: 0.9528, Validation Loss: 0.7158 \n",
            "Hidden Dimension: 32, Epoch: 11, Loss: 0.9172, Validation Loss: 0.7164 \n",
            "Hidden Dimension: 32, Epoch: 12, Loss: 0.9924, Validation Loss: 0.7174 \n",
            "Early stopping at epoch 13 \n",
            "\n",
            " Hidden Dimension: 32, Validation Loss: 0.7184 \n",
            "\n",
            "Hidden Dimension: 64, Epoch: 0, Loss: 1.7962, Validation Loss: 1.5222 \n",
            "Hidden Dimension: 64, Epoch: 1, Loss: 1.5873, Validation Loss: 1.2903 \n",
            "Hidden Dimension: 64, Epoch: 2, Loss: 1.4735, Validation Loss: 0.9690 \n",
            "Hidden Dimension: 64, Epoch: 3, Loss: 1.2128, Validation Loss: 0.8868 \n",
            "Hidden Dimension: 64, Epoch: 4, Loss: 1.1737, Validation Loss: 0.7946 \n",
            "Hidden Dimension: 64, Epoch: 5, Loss: 1.0493, Validation Loss: 0.7692 \n",
            "Hidden Dimension: 64, Epoch: 6, Loss: 1.0347, Validation Loss: 0.7764 \n",
            "Hidden Dimension: 64, Epoch: 7, Loss: 1.0234, Validation Loss: 0.7673 \n",
            "Hidden Dimension: 64, Epoch: 8, Loss: 0.9995, Validation Loss: 0.7441 \n",
            "Hidden Dimension: 64, Epoch: 9, Loss: 0.9820, Validation Loss: 0.7106 \n",
            "Hidden Dimension: 64, Epoch: 10, Loss: 0.9361, Validation Loss: 0.6898 \n",
            "Hidden Dimension: 64, Epoch: 11, Loss: 0.9386, Validation Loss: 0.6882 \n",
            "Hidden Dimension: 64, Epoch: 12, Loss: 0.9222, Validation Loss: 0.6846 \n",
            "Hidden Dimension: 64, Epoch: 13, Loss: 0.8659, Validation Loss: 0.6807 \n",
            "Hidden Dimension: 64, Epoch: 14, Loss: 0.8698, Validation Loss: 0.6931 \n",
            "Hidden Dimension: 64, Epoch: 15, Loss: 0.8319, Validation Loss: 0.7082 \n",
            "Hidden Dimension: 64, Epoch: 16, Loss: 0.8501, Validation Loss: 0.7093 \n",
            "Hidden Dimension: 64, Epoch: 17, Loss: 0.8311, Validation Loss: 0.6959 \n",
            "Early stopping at epoch 18 \n",
            "\n",
            " Hidden Dimension: 64, Validation Loss: 0.6941 \n",
            "\n",
            "Hidden Dimension: 128, Epoch: 0, Loss: 1.8026, Validation Loss: 1.6409 \n",
            "Hidden Dimension: 128, Epoch: 1, Loss: 1.8670, Validation Loss: 1.6652 \n",
            "Hidden Dimension: 128, Epoch: 2, Loss: 1.8503, Validation Loss: 1.2557 \n",
            "Hidden Dimension: 128, Epoch: 3, Loss: 1.3654, Validation Loss: 1.1110 \n",
            "Hidden Dimension: 128, Epoch: 4, Loss: 1.1971, Validation Loss: 0.9573 \n",
            "Hidden Dimension: 128, Epoch: 5, Loss: 1.1442, Validation Loss: 0.8406 \n",
            "Hidden Dimension: 128, Epoch: 6, Loss: 1.1182, Validation Loss: 0.7847 \n",
            "Hidden Dimension: 128, Epoch: 7, Loss: 1.0448, Validation Loss: 0.7699 \n",
            "Hidden Dimension: 128, Epoch: 8, Loss: 0.9889, Validation Loss: 0.7635 \n",
            "Hidden Dimension: 128, Epoch: 9, Loss: 0.9671, Validation Loss: 0.7304 \n",
            "Hidden Dimension: 128, Epoch: 10, Loss: 0.9530, Validation Loss: 0.7311 \n",
            "Hidden Dimension: 128, Epoch: 11, Loss: 0.9048, Validation Loss: 0.7240 \n",
            "Hidden Dimension: 128, Epoch: 12, Loss: 0.8868, Validation Loss: 0.7179 \n",
            "Hidden Dimension: 128, Epoch: 13, Loss: 0.8985, Validation Loss: 0.7227 \n",
            "Hidden Dimension: 128, Epoch: 14, Loss: 0.8859, Validation Loss: 0.7333 \n",
            "Hidden Dimension: 128, Epoch: 15, Loss: 0.8563, Validation Loss: 0.7329 \n",
            "Hidden Dimension: 128, Epoch: 16, Loss: 0.8607, Validation Loss: 0.7326 \n",
            "Early stopping at epoch 17 \n",
            "\n",
            " Hidden Dimension: 128, Validation Loss: 0.7378 \n",
            "\n",
            "Hidden Dimension: 256, Epoch: 0, Loss: 1.7977, Validation Loss: 2.8364 \n",
            "Hidden Dimension: 256, Epoch: 1, Loss: 4.1945, Validation Loss: 1.9897 \n",
            "Hidden Dimension: 256, Epoch: 2, Loss: 2.4155, Validation Loss: 1.4620 \n",
            "Hidden Dimension: 256, Epoch: 3, Loss: 1.7395, Validation Loss: 1.3167 \n",
            "Hidden Dimension: 256, Epoch: 4, Loss: 1.4585, Validation Loss: 1.2730 \n",
            "Hidden Dimension: 256, Epoch: 5, Loss: 1.3844, Validation Loss: 1.1964 \n",
            "Hidden Dimension: 256, Epoch: 6, Loss: 1.3176, Validation Loss: 1.0931 \n",
            "Hidden Dimension: 256, Epoch: 7, Loss: 1.2735, Validation Loss: 0.9959 \n",
            "Hidden Dimension: 256, Epoch: 8, Loss: 1.1690, Validation Loss: 0.9306 \n",
            "Hidden Dimension: 256, Epoch: 9, Loss: 1.1221, Validation Loss: 0.8943 \n",
            "Hidden Dimension: 256, Epoch: 10, Loss: 1.1037, Validation Loss: 0.8542 \n",
            "Hidden Dimension: 256, Epoch: 11, Loss: 1.0516, Validation Loss: 0.8222 \n",
            "Hidden Dimension: 256, Epoch: 12, Loss: 1.0822, Validation Loss: 0.7853 \n",
            "Hidden Dimension: 256, Epoch: 13, Loss: 1.0216, Validation Loss: 0.7833 \n",
            "Hidden Dimension: 256, Epoch: 14, Loss: 1.0075, Validation Loss: 0.7945 \n",
            "Hidden Dimension: 256, Epoch: 15, Loss: 1.0004, Validation Loss: 0.7879 \n",
            "Hidden Dimension: 256, Epoch: 16, Loss: 0.9990, Validation Loss: 0.7798 \n",
            "Hidden Dimension: 256, Epoch: 17, Loss: 0.9505, Validation Loss: 0.7834 \n",
            "Hidden Dimension: 256, Epoch: 18, Loss: 0.9320, Validation Loss: 0.7712 \n",
            "Hidden Dimension: 256, Epoch: 19, Loss: 0.9354, Validation Loss: 0.7484 \n",
            "Hidden Dimension: 256, Epoch: 20, Loss: 0.9292, Validation Loss: 0.7353 \n",
            "Hidden Dimension: 256, Epoch: 21, Loss: 0.9001, Validation Loss: 0.7357 \n",
            "Hidden Dimension: 256, Epoch: 22, Loss: 0.8887, Validation Loss: 0.7312 \n",
            "Hidden Dimension: 256, Epoch: 23, Loss: 0.8814, Validation Loss: 0.7288 \n",
            "Hidden Dimension: 256, Epoch: 24, Loss: 0.9051, Validation Loss: 0.7431 \n",
            "Hidden Dimension: 256, Epoch: 25, Loss: 0.8456, Validation Loss: 0.7526 \n",
            "Hidden Dimension: 256, Epoch: 26, Loss: 0.8840, Validation Loss: 0.7441 \n",
            "Hidden Dimension: 256, Epoch: 27, Loss: 0.8627, Validation Loss: 0.7360 \n",
            "Early stopping at epoch 28 \n",
            "\n",
            " Hidden Dimension: 256, Validation Loss: 0.7351 \n",
            "\n",
            "Best Hidden Dimension: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  citeseer dataset(three layer)**"
      ],
      "metadata": {
        "id": "H-hYd9-eWY9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = ThreeLayerGAT(input_dim= citeseer.num_node_features ,hidden_dim=best_hidden_dim , output_dim= citeseer_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c2f043-d16e-4d75-9e66-874b86b1c9eb",
        "id": "FulOmEvrWY9n"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 1.8028, Validation Loss: 1.7497 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 1.7451, Validation Loss: 1.6756 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 1.6858, Validation Loss: 1.5962 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 1.6271, Validation Loss: 1.5263 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 1.5838, Validation Loss: 1.4708 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 1.5500, Validation Loss: 1.4239 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 1.5342, Validation Loss: 1.3979 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 1.5499, Validation Loss: 1.3815 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 1.5139, Validation Loss: 1.3633 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 1.4819, Validation Loss: 1.3372 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 1.4422, Validation Loss: 1.3033 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 1.4320, Validation Loss: 1.2625 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 1.4567, Validation Loss: 1.2194 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 1.4000, Validation Loss: 1.1769 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 1.3997, Validation Loss: 1.1384 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 1.3663, Validation Loss: 1.1052 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 1.3570, Validation Loss: 1.0748 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 1.3822, Validation Loss: 1.0491 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 1.3661, Validation Loss: 1.0259 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 1.3055, Validation Loss: 1.0056 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 1.3068, Validation Loss: 0.9873 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 1.2822, Validation Loss: 0.9686 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 1.2923, Validation Loss: 0.9471 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 1.2739, Validation Loss: 0.9231 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 1.2583, Validation Loss: 0.9006 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 1.2577, Validation Loss: 0.8812 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 1.2439, Validation Loss: 0.8654 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 1.2385, Validation Loss: 0.8538 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 1.2409, Validation Loss: 0.8453 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 1.2399, Validation Loss: 0.8385 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 1.2330, Validation Loss: 0.8340 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 1.2238, Validation Loss: 0.8296 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 1.2061, Validation Loss: 0.8245 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 1.2101, Validation Loss: 0.8192 \n",
            "Attention Heads: 1, Epoch: 34, Loss: 1.2156, Validation Loss: 0.8140 \n",
            "Attention Heads: 1, Epoch: 35, Loss: 1.2475, Validation Loss: 0.8099 \n",
            "Attention Heads: 1, Epoch: 36, Loss: 1.2113, Validation Loss: 0.8051 \n",
            "Attention Heads: 1, Epoch: 37, Loss: 1.2045, Validation Loss: 0.8016 \n",
            "Attention Heads: 1, Epoch: 38, Loss: 1.1934, Validation Loss: 0.7984 \n",
            "Attention Heads: 1, Epoch: 39, Loss: 1.1862, Validation Loss: 0.7936 \n",
            "Attention Heads: 1, Epoch: 40, Loss: 1.1760, Validation Loss: 0.7883 \n",
            "Attention Heads: 1, Epoch: 41, Loss: 1.1855, Validation Loss: 0.7824 \n",
            "Attention Heads: 1, Epoch: 42, Loss: 1.1731, Validation Loss: 0.7767 \n",
            "Attention Heads: 1, Epoch: 43, Loss: 1.1911, Validation Loss: 0.7741 \n",
            "Attention Heads: 1, Epoch: 44, Loss: 1.1819, Validation Loss: 0.7718 \n",
            "Attention Heads: 1, Epoch: 45, Loss: 1.1878, Validation Loss: 0.7682 \n",
            "Attention Heads: 1, Epoch: 46, Loss: 1.1737, Validation Loss: 0.7660 \n",
            "Attention Heads: 1, Epoch: 47, Loss: 1.1977, Validation Loss: 0.7636 \n",
            "Attention Heads: 1, Epoch: 48, Loss: 1.1539, Validation Loss: 0.7593 \n",
            "Attention Heads: 1, Epoch: 49, Loss: 1.1587, Validation Loss: 0.7531 \n",
            "Attention Heads: 1, Epoch: 50, Loss: 1.1836, Validation Loss: 0.7480 \n",
            "Attention Heads: 1, Epoch: 51, Loss: 1.1671, Validation Loss: 0.7433 \n",
            "Attention Heads: 1, Epoch: 52, Loss: 1.1517, Validation Loss: 0.7376 \n",
            "Attention Heads: 1, Epoch: 53, Loss: 1.1494, Validation Loss: 0.7332 \n",
            "Attention Heads: 1, Epoch: 54, Loss: 1.1512, Validation Loss: 0.7309 \n",
            "Attention Heads: 1, Epoch: 55, Loss: 1.1425, Validation Loss: 0.7288 \n",
            "Attention Heads: 1, Epoch: 56, Loss: 1.1578, Validation Loss: 0.7273 \n",
            "Attention Heads: 1, Epoch: 57, Loss: 1.1159, Validation Loss: 0.7259 \n",
            "Attention Heads: 1, Epoch: 58, Loss: 1.1504, Validation Loss: 0.7241 \n",
            "Attention Heads: 1, Epoch: 59, Loss: 1.1555, Validation Loss: 0.7236 \n",
            "Attention Heads: 1, Epoch: 60, Loss: 1.1422, Validation Loss: 0.7231 \n",
            "Attention Heads: 1, Epoch: 61, Loss: 1.1141, Validation Loss: 0.7217 \n",
            "Attention Heads: 1, Epoch: 62, Loss: 1.1595, Validation Loss: 0.7215 \n",
            "Attention Heads: 1, Epoch: 63, Loss: 1.1475, Validation Loss: 0.7220 \n",
            "Attention Heads: 1, Epoch: 64, Loss: 1.1237, Validation Loss: 0.7214 \n",
            "Attention Heads: 1, Epoch: 65, Loss: 1.1221, Validation Loss: 0.7195 \n",
            "Attention Heads: 1, Epoch: 66, Loss: 1.1253, Validation Loss: 0.7148 \n",
            "Attention Heads: 1, Epoch: 67, Loss: 1.1478, Validation Loss: 0.7096 \n",
            "Attention Heads: 1, Epoch: 68, Loss: 1.1435, Validation Loss: 0.7055 \n",
            "Attention Heads: 1, Epoch: 69, Loss: 1.1094, Validation Loss: 0.7012 \n",
            "Attention Heads: 1, Epoch: 70, Loss: 1.1223, Validation Loss: 0.7003 \n",
            "Attention Heads: 1, Epoch: 71, Loss: 1.1031, Validation Loss: 0.7000 \n",
            "Attention Heads: 1, Epoch: 72, Loss: 1.1045, Validation Loss: 0.7010 \n",
            "Attention Heads: 1, Epoch: 73, Loss: 1.1125, Validation Loss: 0.7031 \n",
            "Attention Heads: 1, Epoch: 74, Loss: 1.1118, Validation Loss: 0.7053 \n",
            "Attention Heads: 1, Epoch: 75, Loss: 1.1201, Validation Loss: 0.7087 \n",
            "Early stopping at epoch 76 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 0.7117 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 1.8117, Validation Loss: 1.7501 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 1.7462, Validation Loss: 1.6752 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 1.6924, Validation Loss: 1.5566 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 1.5974, Validation Loss: 1.4111 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 1.5318, Validation Loss: 1.2639 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 1.4437, Validation Loss: 1.1317 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 1.3710, Validation Loss: 1.0317 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 1.3160, Validation Loss: 0.9526 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 1.2927, Validation Loss: 0.8888 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 1.2278, Validation Loss: 0.8348 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 1.2069, Validation Loss: 0.7928 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 1.2160, Validation Loss: 0.7652 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 1.2256, Validation Loss: 0.7516 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 1.1525, Validation Loss: 0.7422 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 1.1206, Validation Loss: 0.7351 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 1.1241, Validation Loss: 0.7265 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 1.1183, Validation Loss: 0.7210 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 1.0712, Validation Loss: 0.7120 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 1.0606, Validation Loss: 0.7011 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 1.0672, Validation Loss: 0.6919 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 1.0556, Validation Loss: 0.6843 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 1.0446, Validation Loss: 0.6780 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 1.0717, Validation Loss: 0.6705 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 1.0368, Validation Loss: 0.6637 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 1.0320, Validation Loss: 0.6593 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 1.0169, Validation Loss: 0.6595 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 0.9929, Validation Loss: 0.6617 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 0.9932, Validation Loss: 0.6634 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 0.9925, Validation Loss: 0.6636 \n",
            "Early stopping at epoch 29 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 0.6619 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 1.7974, Validation Loss: 1.6683 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 1.6671, Validation Loss: 1.4660 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 1.5332, Validation Loss: 1.2189 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 1.3767, Validation Loss: 1.0134 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 1.2851, Validation Loss: 0.8791 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 1.2219, Validation Loss: 0.8078 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 1.1543, Validation Loss: 0.7764 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 1.1389, Validation Loss: 0.7623 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 1.0702, Validation Loss: 0.7235 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 1.0280, Validation Loss: 0.6951 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 1.0344, Validation Loss: 0.6849 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 0.9787, Validation Loss: 0.6862 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 1.0041, Validation Loss: 0.6724 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 0.9752, Validation Loss: 0.6693 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 0.9604, Validation Loss: 0.6741 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 0.9507, Validation Loss: 0.6791 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 0.9326, Validation Loss: 0.6862 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 0.9242, Validation Loss: 0.6966 \n",
            "Early stopping at epoch 18 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 0.7039 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 1.7969, Validation Loss: 1.6019 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 1.6438, Validation Loss: 1.2931 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 1.4254, Validation Loss: 0.9779 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 1.1967, Validation Loss: 0.7582 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 1.1389, Validation Loss: 0.6875 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 1.0598, Validation Loss: 0.7014 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 1.0466, Validation Loss: 0.6651 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 1.0346, Validation Loss: 0.6269 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 0.9823, Validation Loss: 0.6184 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 0.9616, Validation Loss: 0.6253 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 0.9130, Validation Loss: 0.6377 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 0.8718, Validation Loss: 0.6589 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 0.8897, Validation Loss: 0.6684 \n",
            "Early stopping at epoch 13 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 0.6727 \n",
            "\n",
            "Best Number of Attention Heads: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = ThreeLayerGAT(input_dim=citeseer.num_node_features, hidden_dim=best_hidden_dim , output_dim=citeseer_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, citeseer)\n",
        "    val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, citeseer)\n",
        "print(f'Test Accuracy(three layer) on citeseer dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSI6zJYaWY9o",
        "outputId": "d287f6d6-c366-48a9-9578-2880de5c3c8f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 34 \n",
            "\n",
            "Test Accuracy(three layer) on citeseer dataset: 0.7308\n"
          ]
        }
      ]
    }
  ]
}