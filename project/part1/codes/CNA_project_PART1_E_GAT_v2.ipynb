{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogi5UJ2Jn10Q",
        "outputId": "f996f6f6-695c-4b33-f625-1ed1bcec102c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf9QNoPJr7NC",
        "outputId": "fd5bc34f-3a80-4449-d973-1c3af1265e02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819c1c13-f976-4a4b-afd2-7a0b833237dd",
        "id": "GuU5ZiX3SQ6v"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpiB408fj9if",
        "outputId": "800855ec-49bd-476e-ca09-3e814f7df3c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m0.9/1.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "4ynbTYiVoXaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load citeseer**"
      ],
      "metadata": {
        "id": "wEvJbzGIR3Nz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0JY7caPEjvAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65afcbe9-aaa2-46af-b46b-47f843b9b1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "\n",
        "\n",
        "citeseer_dataset = Planetoid(root='', name='CiteSeer')\n",
        "\n",
        "citeseer = citeseer_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load CoraFull**"
      ],
      "metadata": {
        "id": "jlV8WuThSBVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import CoraFull\n",
        "\n",
        "\n",
        "root = './CoraFull'\n",
        "\n",
        "\n",
        "CoraFull_dataset = CoraFull(root)\n",
        "\n",
        "CoraFull_dataset.download()\n",
        "CoraFull_dataset.process()\n",
        "\n",
        "CoraFull =  CoraFull_dataset[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppnuLvEY_CVp",
        "outputId": "b158f1cc-901b-4277-dc56-9d3452ac6220"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/abojchevski/graph2gauss/raw/master/data/cora.npz\n",
            "Processing...\n",
            "Done!\n",
            "Using existing file cora.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split dataset**"
      ],
      "metadata": {
        "id": "VGRM6eBU283L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **split cora full**"
      ],
      "metadata": {
        "id": "5spl5OsX_blo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import RandomNodeSplit\n",
        "\n",
        "transform = RandomNodeSplit(num_train_per_class=int(CoraFull.num_nodes * 0.7), num_val=int(CoraFull.num_nodes * 0.1), num_test=int(CoraFull.num_nodes * 0.2))\n",
        "CoraFull = transform(CoraFull)"
      ],
      "metadata": {
        "id": "D3iaM0-22xJD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **split citeseer**"
      ],
      "metadata": {
        "id": "CWzjNh6n_ZzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = RandomNodeSplit(num_train_per_class=int(citeseer.num_nodes * 0.7), num_val=int(citeseer.num_nodes * 0.1), num_test=int(citeseer.num_nodes * 0.2))\n",
        "citeseer = transform(citeseer)"
      ],
      "metadata": {
        "id": "eYYIj54J2MV_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **important functions**"
      ],
      "metadata": {
        "id": "qMtwDUTKC-WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    data = data.to(device)\n",
        "    out = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def validate(model, criterion, data):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
        "    return val_loss.item()\n",
        "\n",
        "def test(model, criterion, data):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        _, pred = torch.max(out, dim=1)\n",
        "        correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "        acc = correct / data.test_mask.sum().item()\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "TXzM1GOJC-WL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **E) implement GAT V2**"
      ],
      "metadata": {
        "id": "EsQShX44g-V4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Two layer GAT V2**"
      ],
      "metadata": {
        "id": "nGrbVegunuMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TwoLayerGAT_v2(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, dropout=0.4, negative_slope=0.2, share_weights = False):\n",
        "        super(TwoLayerGAT_v2, self).__init__()\n",
        "        self.gat1 = GATv2Conv(input_dim, hidden_dim, heads=num_heads, dropout=dropout, negative_slope=negative_slope,share_weights = share_weights)\n",
        "        self.gat2 = GATv2Conv(hidden_dim * num_heads, output_dim, heads=1, dropout=dropout, negative_slope=negative_slope,share_weights = share_weights)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.gat2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Msj1TMc6oKtQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Corafull dataset(two layer)**"
      ],
      "metadata": {
        "id": "JaW37ET8oL9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "hidden_dims = [16 , 32, 64, 128, 256]\n",
        "\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_num_heads = None\n",
        "\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    model = TwoLayerGAT_v2(input_dim=CoraFull.num_node_features, hidden_dim= hidden_dim , output_dim=CoraFull_dataset.num_classes, num_heads = 4).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Hidden Dimension: {hidden_dim}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Hidden Dimension: {hidden_dim}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_hidden_dim = hidden_dim\n",
        "\n",
        "print('Best Hidden Dimension: {}'.format(best_hidden_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWiBuz88ykr6",
        "outputId": "6c4b8b81-e603-48f0-a3b2-975dcd5e55e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 16, Epoch: 0, Loss: 4.2560, Validation Loss: 3.9831 \n",
            "Hidden Dimension: 16, Epoch: 1, Loss: 3.9932, Validation Loss: 3.6448 \n",
            "Hidden Dimension: 16, Epoch: 2, Loss: 3.7154, Validation Loss: 3.3450 \n",
            "Hidden Dimension: 16, Epoch: 3, Loss: 3.4657, Validation Loss: 3.1165 \n",
            "Hidden Dimension: 16, Epoch: 4, Loss: 3.2254, Validation Loss: 2.9030 \n",
            "Hidden Dimension: 16, Epoch: 5, Loss: 3.0311, Validation Loss: 2.6771 \n",
            "Hidden Dimension: 16, Epoch: 6, Loss: 2.8502, Validation Loss: 2.4674 \n",
            "Hidden Dimension: 16, Epoch: 7, Loss: 2.6966, Validation Loss: 2.2883 \n",
            "Hidden Dimension: 16, Epoch: 8, Loss: 2.5398, Validation Loss: 2.1233 \n",
            "Hidden Dimension: 16, Epoch: 9, Loss: 2.4004, Validation Loss: 1.9723 \n",
            "Hidden Dimension: 16, Epoch: 10, Loss: 2.2897, Validation Loss: 1.8488 \n",
            "Hidden Dimension: 16, Epoch: 11, Loss: 2.1962, Validation Loss: 1.7390 \n",
            "Hidden Dimension: 16, Epoch: 12, Loss: 2.1023, Validation Loss: 1.6407 \n",
            "Hidden Dimension: 16, Epoch: 13, Loss: 2.0151, Validation Loss: 1.5652 \n",
            "Hidden Dimension: 16, Epoch: 14, Loss: 1.9401, Validation Loss: 1.5062 \n",
            "Hidden Dimension: 16, Epoch: 15, Loss: 1.8649, Validation Loss: 1.4564 \n",
            "Hidden Dimension: 16, Epoch: 16, Loss: 1.8292, Validation Loss: 1.4116 \n",
            "Hidden Dimension: 16, Epoch: 17, Loss: 1.7704, Validation Loss: 1.3695 \n",
            "Hidden Dimension: 16, Epoch: 18, Loss: 1.7210, Validation Loss: 1.3325 \n",
            "Hidden Dimension: 16, Epoch: 19, Loss: 1.6806, Validation Loss: 1.2996 \n",
            "Hidden Dimension: 16, Epoch: 20, Loss: 1.6509, Validation Loss: 1.2730 \n",
            "Hidden Dimension: 16, Epoch: 21, Loss: 1.6231, Validation Loss: 1.2516 \n",
            "Hidden Dimension: 16, Epoch: 22, Loss: 1.5800, Validation Loss: 1.2332 \n",
            "Hidden Dimension: 16, Epoch: 23, Loss: 1.5538, Validation Loss: 1.2185 \n",
            "Hidden Dimension: 16, Epoch: 24, Loss: 1.5304, Validation Loss: 1.2035 \n",
            "Hidden Dimension: 16, Epoch: 25, Loss: 1.5005, Validation Loss: 1.1875 \n",
            "Hidden Dimension: 16, Epoch: 26, Loss: 1.4844, Validation Loss: 1.1740 \n",
            "Hidden Dimension: 16, Epoch: 27, Loss: 1.4542, Validation Loss: 1.1623 \n",
            "Hidden Dimension: 16, Epoch: 28, Loss: 1.4315, Validation Loss: 1.1519 \n",
            "Hidden Dimension: 16, Epoch: 29, Loss: 1.4072, Validation Loss: 1.1458 \n",
            "Hidden Dimension: 16, Epoch: 30, Loss: 1.3981, Validation Loss: 1.1410 \n",
            "Hidden Dimension: 16, Epoch: 31, Loss: 1.3843, Validation Loss: 1.1380 \n",
            "Hidden Dimension: 16, Epoch: 32, Loss: 1.3705, Validation Loss: 1.1311 \n",
            "Hidden Dimension: 16, Epoch: 33, Loss: 1.3756, Validation Loss: 1.1225 \n",
            "Hidden Dimension: 16, Epoch: 34, Loss: 1.3495, Validation Loss: 1.1131 \n",
            "Hidden Dimension: 16, Epoch: 35, Loss: 1.3393, Validation Loss: 1.1072 \n",
            "Hidden Dimension: 16, Epoch: 36, Loss: 1.3337, Validation Loss: 1.1042 \n",
            "Hidden Dimension: 16, Epoch: 37, Loss: 1.3118, Validation Loss: 1.1007 \n",
            "Hidden Dimension: 16, Epoch: 38, Loss: 1.3004, Validation Loss: 1.0966 \n",
            "Hidden Dimension: 16, Epoch: 39, Loss: 1.3125, Validation Loss: 1.0913 \n",
            "Hidden Dimension: 16, Epoch: 40, Loss: 1.2734, Validation Loss: 1.0864 \n",
            "Hidden Dimension: 16, Epoch: 41, Loss: 1.2758, Validation Loss: 1.0822 \n",
            "Hidden Dimension: 16, Epoch: 42, Loss: 1.2644, Validation Loss: 1.0798 \n",
            "Hidden Dimension: 16, Epoch: 43, Loss: 1.2666, Validation Loss: 1.0808 \n",
            "Hidden Dimension: 16, Epoch: 44, Loss: 1.2576, Validation Loss: 1.0788 \n",
            "Hidden Dimension: 16, Epoch: 45, Loss: 1.2497, Validation Loss: 1.0751 \n",
            "Hidden Dimension: 16, Epoch: 46, Loss: 1.2401, Validation Loss: 1.0702 \n",
            "Hidden Dimension: 16, Epoch: 47, Loss: 1.2398, Validation Loss: 1.0648 \n",
            "Hidden Dimension: 16, Epoch: 48, Loss: 1.2257, Validation Loss: 1.0624 \n",
            "Hidden Dimension: 16, Epoch: 49, Loss: 1.2195, Validation Loss: 1.0603 \n",
            "Hidden Dimension: 16, Epoch: 50, Loss: 1.2214, Validation Loss: 1.0601 \n",
            "Hidden Dimension: 16, Epoch: 51, Loss: 1.2023, Validation Loss: 1.0604 \n",
            "Hidden Dimension: 16, Epoch: 52, Loss: 1.1956, Validation Loss: 1.0610 \n",
            "Hidden Dimension: 16, Epoch: 53, Loss: 1.2086, Validation Loss: 1.0623 \n",
            "Early stopping at epoch 54 \n",
            "\n",
            " Hidden Dimension: 16, Validation Loss: 1.0610 \n",
            "\n",
            "Hidden Dimension: 32, Epoch: 0, Loss: 4.2453, Validation Loss: 3.8113 \n",
            "Hidden Dimension: 32, Epoch: 1, Loss: 3.8418, Validation Loss: 3.3405 \n",
            "Hidden Dimension: 32, Epoch: 2, Loss: 3.4562, Validation Loss: 2.9255 \n",
            "Hidden Dimension: 32, Epoch: 3, Loss: 3.0866, Validation Loss: 2.5895 \n",
            "Hidden Dimension: 32, Epoch: 4, Loss: 2.7586, Validation Loss: 2.3131 \n",
            "Hidden Dimension: 32, Epoch: 5, Loss: 2.5405, Validation Loss: 2.0716 \n",
            "Hidden Dimension: 32, Epoch: 6, Loss: 2.3468, Validation Loss: 1.8506 \n",
            "Hidden Dimension: 32, Epoch: 7, Loss: 2.1399, Validation Loss: 1.6757 \n",
            "Hidden Dimension: 32, Epoch: 8, Loss: 2.0215, Validation Loss: 1.5500 \n",
            "Hidden Dimension: 32, Epoch: 9, Loss: 1.9030, Validation Loss: 1.4564 \n",
            "Hidden Dimension: 32, Epoch: 10, Loss: 1.7852, Validation Loss: 1.3875 \n",
            "Hidden Dimension: 32, Epoch: 11, Loss: 1.7175, Validation Loss: 1.3298 \n",
            "Hidden Dimension: 32, Epoch: 12, Loss: 1.6386, Validation Loss: 1.2828 \n",
            "Hidden Dimension: 32, Epoch: 13, Loss: 1.5794, Validation Loss: 1.2451 \n",
            "Hidden Dimension: 32, Epoch: 14, Loss: 1.5168, Validation Loss: 1.2124 \n",
            "Hidden Dimension: 32, Epoch: 15, Loss: 1.4722, Validation Loss: 1.1854 \n",
            "Hidden Dimension: 32, Epoch: 16, Loss: 1.4353, Validation Loss: 1.1628 \n",
            "Hidden Dimension: 32, Epoch: 17, Loss: 1.4128, Validation Loss: 1.1436 \n",
            "Hidden Dimension: 32, Epoch: 18, Loss: 1.3638, Validation Loss: 1.1305 \n",
            "Hidden Dimension: 32, Epoch: 19, Loss: 1.3371, Validation Loss: 1.1212 \n",
            "Hidden Dimension: 32, Epoch: 20, Loss: 1.3077, Validation Loss: 1.1114 \n",
            "Hidden Dimension: 32, Epoch: 21, Loss: 1.2778, Validation Loss: 1.1010 \n",
            "Hidden Dimension: 32, Epoch: 22, Loss: 1.2635, Validation Loss: 1.0926 \n",
            "Hidden Dimension: 32, Epoch: 23, Loss: 1.2453, Validation Loss: 1.0839 \n",
            "Hidden Dimension: 32, Epoch: 24, Loss: 1.2229, Validation Loss: 1.0754 \n",
            "Hidden Dimension: 32, Epoch: 25, Loss: 1.2219, Validation Loss: 1.0694 \n",
            "Hidden Dimension: 32, Epoch: 26, Loss: 1.1994, Validation Loss: 1.0638 \n",
            "Hidden Dimension: 32, Epoch: 27, Loss: 1.1752, Validation Loss: 1.0595 \n",
            "Hidden Dimension: 32, Epoch: 28, Loss: 1.1851, Validation Loss: 1.0582 \n",
            "Hidden Dimension: 32, Epoch: 29, Loss: 1.1743, Validation Loss: 1.0565 \n",
            "Hidden Dimension: 32, Epoch: 30, Loss: 1.1413, Validation Loss: 1.0547 \n",
            "Hidden Dimension: 32, Epoch: 31, Loss: 1.1411, Validation Loss: 1.0508 \n",
            "Hidden Dimension: 32, Epoch: 32, Loss: 1.1094, Validation Loss: 1.0498 \n",
            "Hidden Dimension: 32, Epoch: 33, Loss: 1.1206, Validation Loss: 1.0505 \n",
            "Hidden Dimension: 32, Epoch: 34, Loss: 1.1231, Validation Loss: 1.0501 \n",
            "Hidden Dimension: 32, Epoch: 35, Loss: 1.1030, Validation Loss: 1.0489 \n",
            "Hidden Dimension: 32, Epoch: 36, Loss: 1.0889, Validation Loss: 1.0478 \n",
            "Hidden Dimension: 32, Epoch: 37, Loss: 1.1047, Validation Loss: 1.0487 \n",
            "Hidden Dimension: 32, Epoch: 38, Loss: 1.0683, Validation Loss: 1.0494 \n",
            "Hidden Dimension: 32, Epoch: 39, Loss: 1.0633, Validation Loss: 1.0454 \n",
            "Hidden Dimension: 32, Epoch: 40, Loss: 1.0601, Validation Loss: 1.0428 \n",
            "Hidden Dimension: 32, Epoch: 41, Loss: 1.0562, Validation Loss: 1.0428 \n",
            "Hidden Dimension: 32, Epoch: 42, Loss: 1.0617, Validation Loss: 1.0415 \n",
            "Hidden Dimension: 32, Epoch: 43, Loss: 1.0508, Validation Loss: 1.0403 \n",
            "Hidden Dimension: 32, Epoch: 44, Loss: 1.0325, Validation Loss: 1.0393 \n",
            "Hidden Dimension: 32, Epoch: 45, Loss: 1.0175, Validation Loss: 1.0371 \n",
            "Hidden Dimension: 32, Epoch: 46, Loss: 1.0398, Validation Loss: 1.0336 \n",
            "Hidden Dimension: 32, Epoch: 47, Loss: 1.0136, Validation Loss: 1.0305 \n",
            "Hidden Dimension: 32, Epoch: 48, Loss: 1.0089, Validation Loss: 1.0327 \n",
            "Hidden Dimension: 32, Epoch: 49, Loss: 1.0150, Validation Loss: 1.0383 \n",
            "Hidden Dimension: 32, Epoch: 50, Loss: 1.0020, Validation Loss: 1.0408 \n",
            "Hidden Dimension: 32, Epoch: 51, Loss: 1.0018, Validation Loss: 1.0445 \n",
            "Early stopping at epoch 52 \n",
            "\n",
            " Hidden Dimension: 32, Validation Loss: 1.0450 \n",
            "\n",
            "Hidden Dimension: 64, Epoch: 0, Loss: 4.2494, Validation Loss: 3.6095 \n",
            "Hidden Dimension: 64, Epoch: 1, Loss: 3.6472, Validation Loss: 2.9956 \n",
            "Hidden Dimension: 64, Epoch: 2, Loss: 3.1698, Validation Loss: 2.5682 \n",
            "Hidden Dimension: 64, Epoch: 3, Loss: 2.7087, Validation Loss: 2.1674 \n",
            "Hidden Dimension: 64, Epoch: 4, Loss: 2.3663, Validation Loss: 1.8418 \n",
            "Hidden Dimension: 64, Epoch: 5, Loss: 2.1104, Validation Loss: 1.6140 \n",
            "Hidden Dimension: 64, Epoch: 6, Loss: 1.9019, Validation Loss: 1.4530 \n",
            "Hidden Dimension: 64, Epoch: 7, Loss: 1.7348, Validation Loss: 1.3402 \n",
            "Hidden Dimension: 64, Epoch: 8, Loss: 1.6309, Validation Loss: 1.2633 \n",
            "Hidden Dimension: 64, Epoch: 9, Loss: 1.5326, Validation Loss: 1.2169 \n",
            "Hidden Dimension: 64, Epoch: 10, Loss: 1.4627, Validation Loss: 1.1762 \n",
            "Hidden Dimension: 64, Epoch: 11, Loss: 1.3891, Validation Loss: 1.1416 \n",
            "Hidden Dimension: 64, Epoch: 12, Loss: 1.3443, Validation Loss: 1.1157 \n",
            "Hidden Dimension: 64, Epoch: 13, Loss: 1.2974, Validation Loss: 1.0963 \n",
            "Hidden Dimension: 64, Epoch: 14, Loss: 1.2658, Validation Loss: 1.0860 \n",
            "Hidden Dimension: 64, Epoch: 15, Loss: 1.2328, Validation Loss: 1.0763 \n",
            "Hidden Dimension: 64, Epoch: 16, Loss: 1.2083, Validation Loss: 1.0655 \n",
            "Hidden Dimension: 64, Epoch: 17, Loss: 1.1709, Validation Loss: 1.0564 \n",
            "Hidden Dimension: 64, Epoch: 18, Loss: 1.1438, Validation Loss: 1.0508 \n",
            "Hidden Dimension: 64, Epoch: 19, Loss: 1.1169, Validation Loss: 1.0446 \n",
            "Hidden Dimension: 64, Epoch: 20, Loss: 1.1112, Validation Loss: 1.0407 \n",
            "Hidden Dimension: 64, Epoch: 21, Loss: 1.0871, Validation Loss: 1.0418 \n",
            "Hidden Dimension: 64, Epoch: 22, Loss: 1.0654, Validation Loss: 1.0401 \n",
            "Hidden Dimension: 64, Epoch: 23, Loss: 1.0515, Validation Loss: 1.0391 \n",
            "Hidden Dimension: 64, Epoch: 24, Loss: 1.0590, Validation Loss: 1.0358 \n",
            "Hidden Dimension: 64, Epoch: 25, Loss: 1.0429, Validation Loss: 1.0327 \n",
            "Hidden Dimension: 64, Epoch: 26, Loss: 1.0278, Validation Loss: 1.0301 \n",
            "Hidden Dimension: 64, Epoch: 27, Loss: 1.0063, Validation Loss: 1.0282 \n",
            "Hidden Dimension: 64, Epoch: 28, Loss: 1.0158, Validation Loss: 1.0254 \n",
            "Hidden Dimension: 64, Epoch: 29, Loss: 1.0020, Validation Loss: 1.0228 \n",
            "Hidden Dimension: 64, Epoch: 30, Loss: 1.0010, Validation Loss: 1.0198 \n",
            "Hidden Dimension: 64, Epoch: 31, Loss: 0.9940, Validation Loss: 1.0198 \n",
            "Hidden Dimension: 64, Epoch: 32, Loss: 0.9963, Validation Loss: 1.0216 \n",
            "Hidden Dimension: 64, Epoch: 33, Loss: 0.9810, Validation Loss: 1.0249 \n",
            "Hidden Dimension: 64, Epoch: 34, Loss: 0.9828, Validation Loss: 1.0247 \n",
            "Early stopping at epoch 35 \n",
            "\n",
            " Hidden Dimension: 64, Validation Loss: 1.0237 \n",
            "\n",
            "Hidden Dimension: 128, Epoch: 0, Loss: 4.2429, Validation Loss: 3.5232 \n",
            "Hidden Dimension: 128, Epoch: 1, Loss: 3.6441, Validation Loss: 3.1099 \n",
            "Hidden Dimension: 128, Epoch: 2, Loss: 3.1843, Validation Loss: 2.4684 \n",
            "Hidden Dimension: 128, Epoch: 3, Loss: 2.6416, Validation Loss: 1.9742 \n",
            "Hidden Dimension: 128, Epoch: 4, Loss: 2.2370, Validation Loss: 1.7254 \n",
            "Hidden Dimension: 128, Epoch: 5, Loss: 1.9844, Validation Loss: 1.4893 \n",
            "Hidden Dimension: 128, Epoch: 6, Loss: 1.7730, Validation Loss: 1.3577 \n",
            "Hidden Dimension: 128, Epoch: 7, Loss: 1.6277, Validation Loss: 1.2756 \n",
            "Hidden Dimension: 128, Epoch: 8, Loss: 1.5179, Validation Loss: 1.2022 \n",
            "Hidden Dimension: 128, Epoch: 9, Loss: 1.4212, Validation Loss: 1.1686 \n",
            "Hidden Dimension: 128, Epoch: 10, Loss: 1.3478, Validation Loss: 1.1590 \n",
            "Hidden Dimension: 128, Epoch: 11, Loss: 1.3043, Validation Loss: 1.1284 \n",
            "Hidden Dimension: 128, Epoch: 12, Loss: 1.2501, Validation Loss: 1.0927 \n",
            "Hidden Dimension: 128, Epoch: 13, Loss: 1.1976, Validation Loss: 1.0684 \n",
            "Hidden Dimension: 128, Epoch: 14, Loss: 1.1608, Validation Loss: 1.0602 \n",
            "Hidden Dimension: 128, Epoch: 15, Loss: 1.1442, Validation Loss: 1.0526 \n",
            "Hidden Dimension: 128, Epoch: 16, Loss: 1.1252, Validation Loss: 1.0410 \n",
            "Hidden Dimension: 128, Epoch: 17, Loss: 1.0926, Validation Loss: 1.0373 \n",
            "Hidden Dimension: 128, Epoch: 18, Loss: 1.0870, Validation Loss: 1.0266 \n",
            "Hidden Dimension: 128, Epoch: 19, Loss: 1.0579, Validation Loss: 1.0300 \n",
            "Hidden Dimension: 128, Epoch: 20, Loss: 1.0388, Validation Loss: 1.0367 \n",
            "Hidden Dimension: 128, Epoch: 21, Loss: 1.0247, Validation Loss: 1.0215 \n",
            "Hidden Dimension: 128, Epoch: 22, Loss: 1.0166, Validation Loss: 1.0173 \n",
            "Hidden Dimension: 128, Epoch: 23, Loss: 0.9971, Validation Loss: 1.0174 \n",
            "Hidden Dimension: 128, Epoch: 24, Loss: 0.9851, Validation Loss: 1.0170 \n",
            "Hidden Dimension: 128, Epoch: 25, Loss: 0.9794, Validation Loss: 1.0149 \n",
            "Hidden Dimension: 128, Epoch: 26, Loss: 0.9694, Validation Loss: 1.0147 \n",
            "Hidden Dimension: 128, Epoch: 27, Loss: 0.9581, Validation Loss: 1.0139 \n",
            "Hidden Dimension: 128, Epoch: 28, Loss: 0.9482, Validation Loss: 1.0078 \n",
            "Hidden Dimension: 128, Epoch: 29, Loss: 0.9307, Validation Loss: 1.0076 \n",
            "Hidden Dimension: 128, Epoch: 30, Loss: 0.9345, Validation Loss: 1.0138 \n",
            "Hidden Dimension: 128, Epoch: 31, Loss: 0.9322, Validation Loss: 1.0214 \n",
            "Hidden Dimension: 128, Epoch: 32, Loss: 0.9153, Validation Loss: 1.0179 \n",
            "Early stopping at epoch 33 \n",
            "\n",
            " Hidden Dimension: 128, Validation Loss: 1.0086 \n",
            "\n",
            "Hidden Dimension: 256, Epoch: 0, Loss: 4.2505, Validation Loss: 3.7864 \n",
            "Hidden Dimension: 256, Epoch: 1, Loss: 3.9016, Validation Loss: 3.0973 \n",
            "Hidden Dimension: 256, Epoch: 2, Loss: 3.1766, Validation Loss: 2.5064 \n",
            "Hidden Dimension: 256, Epoch: 3, Loss: 2.6603, Validation Loss: 1.9690 \n",
            "Hidden Dimension: 256, Epoch: 4, Loss: 2.1687, Validation Loss: 1.6626 \n",
            "Hidden Dimension: 256, Epoch: 5, Loss: 1.9166, Validation Loss: 1.4291 \n",
            "Hidden Dimension: 256, Epoch: 6, Loss: 1.7131, Validation Loss: 1.3667 \n",
            "Hidden Dimension: 256, Epoch: 7, Loss: 1.6526, Validation Loss: 1.2607 \n",
            "Hidden Dimension: 256, Epoch: 8, Loss: 1.4780, Validation Loss: 1.2133 \n",
            "Hidden Dimension: 256, Epoch: 9, Loss: 1.3960, Validation Loss: 1.1523 \n",
            "Hidden Dimension: 256, Epoch: 10, Loss: 1.3072, Validation Loss: 1.1562 \n",
            "Hidden Dimension: 256, Epoch: 11, Loss: 1.2804, Validation Loss: 1.1252 \n",
            "Hidden Dimension: 256, Epoch: 12, Loss: 1.2233, Validation Loss: 1.0802 \n",
            "Hidden Dimension: 256, Epoch: 13, Loss: 1.1661, Validation Loss: 1.0624 \n",
            "Hidden Dimension: 256, Epoch: 14, Loss: 1.1341, Validation Loss: 1.0573 \n",
            "Hidden Dimension: 256, Epoch: 15, Loss: 1.1177, Validation Loss: 1.0493 \n",
            "Hidden Dimension: 256, Epoch: 16, Loss: 1.0783, Validation Loss: 1.0387 \n",
            "Hidden Dimension: 256, Epoch: 17, Loss: 1.0733, Validation Loss: 1.0307 \n",
            "Hidden Dimension: 256, Epoch: 18, Loss: 1.0453, Validation Loss: 1.0292 \n",
            "Hidden Dimension: 256, Epoch: 19, Loss: 1.0082, Validation Loss: 1.0312 \n",
            "Hidden Dimension: 256, Epoch: 20, Loss: 1.0048, Validation Loss: 1.0258 \n",
            "Hidden Dimension: 256, Epoch: 21, Loss: 0.9996, Validation Loss: 1.0225 \n",
            "Hidden Dimension: 256, Epoch: 22, Loss: 0.9813, Validation Loss: 1.0207 \n",
            "Hidden Dimension: 256, Epoch: 23, Loss: 0.9633, Validation Loss: 1.0222 \n",
            "Hidden Dimension: 256, Epoch: 24, Loss: 0.9617, Validation Loss: 1.0288 \n",
            "Hidden Dimension: 256, Epoch: 25, Loss: 0.9467, Validation Loss: 1.0353 \n",
            "Hidden Dimension: 256, Epoch: 26, Loss: 0.9606, Validation Loss: 1.0179 \n",
            "Hidden Dimension: 256, Epoch: 27, Loss: 0.9124, Validation Loss: 1.0204 \n",
            "Hidden Dimension: 256, Epoch: 28, Loss: 0.9228, Validation Loss: 1.0308 \n",
            "Hidden Dimension: 256, Epoch: 29, Loss: 0.9168, Validation Loss: 1.0236 \n",
            "Hidden Dimension: 256, Epoch: 30, Loss: 0.8839, Validation Loss: 1.0317 \n",
            "Early stopping at epoch 31 \n",
            "\n",
            " Hidden Dimension: 256, Validation Loss: 1.0359 \n",
            "\n",
            "Best Hidden Dimension: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  cora full dataset(two layer)**"
      ],
      "metadata": {
        "id": "m8hBVxjgQEIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = TwoLayerGAT_v2(input_dim= CoraFull.num_node_features ,hidden_dim=best_hidden_dim , output_dim= CoraFull_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc40703f-f4c8-42f4-ba45-d13e17d603a2",
        "id": "6HpvE21QQEIk"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 4.2536, Validation Loss: 3.8501 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 3.8546, Validation Loss: 3.3241 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 3.5424, Validation Loss: 3.1489 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 3.2346, Validation Loss: 2.9019 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 3.0175, Validation Loss: 2.5663 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 2.7556, Validation Loss: 2.2926 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 2.5127, Validation Loss: 2.1193 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 2.3613, Validation Loss: 1.9525 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 2.2367, Validation Loss: 1.7938 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 2.0961, Validation Loss: 1.6670 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 2.0242, Validation Loss: 1.5792 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 1.9283, Validation Loss: 1.5054 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 1.8287, Validation Loss: 1.4307 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 1.7753, Validation Loss: 1.3643 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 1.7368, Validation Loss: 1.3133 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 1.6830, Validation Loss: 1.2769 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 1.6149, Validation Loss: 1.2466 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 1.5889, Validation Loss: 1.2193 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 1.5441, Validation Loss: 1.1972 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 1.5044, Validation Loss: 1.1807 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 1.4738, Validation Loss: 1.1674 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 1.4630, Validation Loss: 1.1560 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 1.4381, Validation Loss: 1.1438 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 1.4300, Validation Loss: 1.1307 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 1.3823, Validation Loss: 1.1224 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 1.3855, Validation Loss: 1.1167 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 1.3548, Validation Loss: 1.1083 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 1.3564, Validation Loss: 1.0983 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 1.3343, Validation Loss: 1.0886 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 1.3178, Validation Loss: 1.0808 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 1.3120, Validation Loss: 1.0756 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 1.3143, Validation Loss: 1.0695 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 1.2865, Validation Loss: 1.0628 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 1.2849, Validation Loss: 1.0589 \n",
            "Attention Heads: 1, Epoch: 34, Loss: 1.2799, Validation Loss: 1.0566 \n",
            "Attention Heads: 1, Epoch: 35, Loss: 1.2636, Validation Loss: 1.0528 \n",
            "Attention Heads: 1, Epoch: 36, Loss: 1.2501, Validation Loss: 1.0482 \n",
            "Attention Heads: 1, Epoch: 37, Loss: 1.2595, Validation Loss: 1.0451 \n",
            "Attention Heads: 1, Epoch: 38, Loss: 1.2543, Validation Loss: 1.0415 \n",
            "Attention Heads: 1, Epoch: 39, Loss: 1.2202, Validation Loss: 1.0379 \n",
            "Attention Heads: 1, Epoch: 40, Loss: 1.2146, Validation Loss: 1.0357 \n",
            "Attention Heads: 1, Epoch: 41, Loss: 1.2152, Validation Loss: 1.0356 \n",
            "Attention Heads: 1, Epoch: 42, Loss: 1.2102, Validation Loss: 1.0352 \n",
            "Attention Heads: 1, Epoch: 43, Loss: 1.1989, Validation Loss: 1.0358 \n",
            "Attention Heads: 1, Epoch: 44, Loss: 1.1928, Validation Loss: 1.0343 \n",
            "Attention Heads: 1, Epoch: 45, Loss: 1.1986, Validation Loss: 1.0301 \n",
            "Attention Heads: 1, Epoch: 46, Loss: 1.1832, Validation Loss: 1.0267 \n",
            "Attention Heads: 1, Epoch: 47, Loss: 1.1754, Validation Loss: 1.0253 \n",
            "Attention Heads: 1, Epoch: 48, Loss: 1.1824, Validation Loss: 1.0260 \n",
            "Attention Heads: 1, Epoch: 49, Loss: 1.1718, Validation Loss: 1.0253 \n",
            "Attention Heads: 1, Epoch: 50, Loss: 1.1787, Validation Loss: 1.0224 \n",
            "Attention Heads: 1, Epoch: 51, Loss: 1.1727, Validation Loss: 1.0227 \n",
            "Attention Heads: 1, Epoch: 52, Loss: 1.1487, Validation Loss: 1.0236 \n",
            "Attention Heads: 1, Epoch: 53, Loss: 1.1528, Validation Loss: 1.0208 \n",
            "Attention Heads: 1, Epoch: 54, Loss: 1.1523, Validation Loss: 1.0224 \n",
            "Attention Heads: 1, Epoch: 55, Loss: 1.1369, Validation Loss: 1.0273 \n",
            "Attention Heads: 1, Epoch: 56, Loss: 1.1440, Validation Loss: 1.0293 \n",
            "Attention Heads: 1, Epoch: 57, Loss: 1.1282, Validation Loss: 1.0275 \n",
            "Early stopping at epoch 58 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 1.0265 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 4.2556, Validation Loss: 3.6683 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 3.7043, Validation Loss: 3.0659 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 3.2665, Validation Loss: 2.8267 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 2.9223, Validation Loss: 2.3885 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 2.5858, Validation Loss: 2.0183 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 2.2690, Validation Loss: 1.7916 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 2.0632, Validation Loss: 1.5971 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 1.9105, Validation Loss: 1.4609 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 1.7891, Validation Loss: 1.3704 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 1.6491, Validation Loss: 1.3127 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 1.5767, Validation Loss: 1.2487 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 1.5204, Validation Loss: 1.2028 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 1.4516, Validation Loss: 1.1754 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 1.3867, Validation Loss: 1.1431 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 1.3595, Validation Loss: 1.1112 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 1.3065, Validation Loss: 1.0924 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 1.2838, Validation Loss: 1.0802 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 1.2774, Validation Loss: 1.0745 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 1.2250, Validation Loss: 1.0644 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 1.2021, Validation Loss: 1.0592 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 1.2097, Validation Loss: 1.0548 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 1.1624, Validation Loss: 1.0509 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 1.1681, Validation Loss: 1.0467 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 1.1417, Validation Loss: 1.0424 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 1.1274, Validation Loss: 1.0395 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 1.1307, Validation Loss: 1.0367 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 1.1083, Validation Loss: 1.0344 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 1.1099, Validation Loss: 1.0314 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 1.0795, Validation Loss: 1.0323 \n",
            "Attention Heads: 2, Epoch: 29, Loss: 1.0914, Validation Loss: 1.0287 \n",
            "Attention Heads: 2, Epoch: 30, Loss: 1.0657, Validation Loss: 1.0236 \n",
            "Attention Heads: 2, Epoch: 31, Loss: 1.0565, Validation Loss: 1.0249 \n",
            "Attention Heads: 2, Epoch: 32, Loss: 1.0504, Validation Loss: 1.0268 \n",
            "Attention Heads: 2, Epoch: 33, Loss: 1.0361, Validation Loss: 1.0265 \n",
            "Attention Heads: 2, Epoch: 34, Loss: 1.0455, Validation Loss: 1.0220 \n",
            "Attention Heads: 2, Epoch: 35, Loss: 1.0299, Validation Loss: 1.0201 \n",
            "Attention Heads: 2, Epoch: 36, Loss: 1.0292, Validation Loss: 1.0227 \n",
            "Attention Heads: 2, Epoch: 37, Loss: 1.0330, Validation Loss: 1.0225 \n",
            "Attention Heads: 2, Epoch: 38, Loss: 1.0010, Validation Loss: 1.0210 \n",
            "Attention Heads: 2, Epoch: 39, Loss: 1.0028, Validation Loss: 1.0221 \n",
            "Early stopping at epoch 40 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 1.0226 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 4.2580, Validation Loss: 3.5264 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 3.5973, Validation Loss: 2.8520 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 2.9375, Validation Loss: 2.2241 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 2.5399, Validation Loss: 2.0107 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 2.2046, Validation Loss: 1.6927 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 1.9303, Validation Loss: 1.4503 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 1.7368, Validation Loss: 1.3381 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 1.6106, Validation Loss: 1.2552 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 1.5006, Validation Loss: 1.1922 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 1.4164, Validation Loss: 1.1551 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 1.3395, Validation Loss: 1.1222 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 1.2832, Validation Loss: 1.0977 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 1.2412, Validation Loss: 1.0731 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 1.1933, Validation Loss: 1.0658 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 1.1761, Validation Loss: 1.0569 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 1.1356, Validation Loss: 1.0475 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 1.1092, Validation Loss: 1.0458 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 1.0825, Validation Loss: 1.0385 \n",
            "Attention Heads: 4, Epoch: 18, Loss: 1.0830, Validation Loss: 1.0271 \n",
            "Attention Heads: 4, Epoch: 19, Loss: 1.0578, Validation Loss: 1.0248 \n",
            "Attention Heads: 4, Epoch: 20, Loss: 1.0465, Validation Loss: 1.0306 \n",
            "Attention Heads: 4, Epoch: 21, Loss: 1.0365, Validation Loss: 1.0334 \n",
            "Attention Heads: 4, Epoch: 22, Loss: 1.0157, Validation Loss: 1.0312 \n",
            "Attention Heads: 4, Epoch: 23, Loss: 0.9974, Validation Loss: 1.0194 \n",
            "Attention Heads: 4, Epoch: 24, Loss: 0.9816, Validation Loss: 1.0172 \n",
            "Attention Heads: 4, Epoch: 25, Loss: 0.9812, Validation Loss: 1.0223 \n",
            "Attention Heads: 4, Epoch: 26, Loss: 0.9636, Validation Loss: 1.0253 \n",
            "Attention Heads: 4, Epoch: 27, Loss: 0.9645, Validation Loss: 1.0265 \n",
            "Attention Heads: 4, Epoch: 28, Loss: 0.9497, Validation Loss: 1.0235 \n",
            "Early stopping at epoch 29 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 1.0228 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 4.2487, Validation Loss: 3.6740 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 3.8422, Validation Loss: 3.1101 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 3.1918, Validation Loss: 2.5687 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 2.7127, Validation Loss: 2.0140 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 2.1970, Validation Loss: 1.7484 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 1.9782, Validation Loss: 1.4852 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 1.7158, Validation Loss: 1.3406 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 1.5670, Validation Loss: 1.2412 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 1.4498, Validation Loss: 1.1798 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 1.3760, Validation Loss: 1.1345 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 1.3000, Validation Loss: 1.1233 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 1.2616, Validation Loss: 1.1046 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 1.2069, Validation Loss: 1.0733 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 1.1686, Validation Loss: 1.0546 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 1.1340, Validation Loss: 1.0433 \n",
            "Attention Heads: 8, Epoch: 15, Loss: 1.1030, Validation Loss: 1.0390 \n",
            "Attention Heads: 8, Epoch: 16, Loss: 1.0746, Validation Loss: 1.0371 \n",
            "Attention Heads: 8, Epoch: 17, Loss: 1.0384, Validation Loss: 1.0269 \n",
            "Attention Heads: 8, Epoch: 18, Loss: 1.0390, Validation Loss: 1.0176 \n",
            "Attention Heads: 8, Epoch: 19, Loss: 0.9999, Validation Loss: 1.0218 \n",
            "Attention Heads: 8, Epoch: 20, Loss: 0.9972, Validation Loss: 1.0212 \n",
            "Attention Heads: 8, Epoch: 21, Loss: 0.9810, Validation Loss: 1.0153 \n",
            "Attention Heads: 8, Epoch: 22, Loss: 0.9514, Validation Loss: 1.0061 \n",
            "Attention Heads: 8, Epoch: 23, Loss: 0.9424, Validation Loss: 1.0057 \n",
            "Attention Heads: 8, Epoch: 24, Loss: 0.9470, Validation Loss: 1.0107 \n",
            "Attention Heads: 8, Epoch: 25, Loss: 0.9398, Validation Loss: 1.0187 \n",
            "Attention Heads: 8, Epoch: 26, Loss: 0.9151, Validation Loss: 1.0195 \n",
            "Early stopping at epoch 27 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 1.0124 \n",
            "\n",
            "Best Number of Attention Heads: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = TwoLayerGAT_v2(input_dim=CoraFull.num_node_features, hidden_dim=best_hidden_dim , output_dim=CoraFull_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, CoraFull)\n",
        "    val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, CoraFull)\n",
        "print(f'Test Accuracy(two layer) on CoraFull dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3xkIPnbUA28",
        "outputId": "2ce58a39-abee-47b3-9f43-c91b940bd306"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 28 \n",
            "\n",
            "Test Accuracy(two layer) on CoraFull dataset: 0.7079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Citeseer dataset(two layer)**"
      ],
      "metadata": {
        "id": "0Bt2J5BQ0aNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "hidden_dims = [16 , 32, 64, 128, 256]\n",
        "\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_num_heads = None\n",
        "\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    model = TwoLayerGAT_v2(input_dim=citeseer.num_node_features, hidden_dim= hidden_dim , output_dim=citeseer_dataset.num_classes, num_heads = 4).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Hidden Dimension: {hidden_dim}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Hidden Dimension: {hidden_dim}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_hidden_dim = hidden_dim\n",
        "\n",
        "print('Best Hidden Dimension: {}'.format(best_hidden_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86def660-93a0-455c-dfbc-8e4839997bb1",
        "id": "UvNF8_rqZM-e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 16, Epoch: 0, Loss: 1.8110, Validation Loss: 1.5092 \n",
            "Hidden Dimension: 16, Epoch: 1, Loss: 1.5265, Validation Loss: 1.2310 \n",
            "Hidden Dimension: 16, Epoch: 2, Loss: 1.2586, Validation Loss: 1.0299 \n",
            "Hidden Dimension: 16, Epoch: 3, Loss: 1.0816, Validation Loss: 0.9145 \n",
            "Hidden Dimension: 16, Epoch: 4, Loss: 0.9599, Validation Loss: 0.8585 \n",
            "Hidden Dimension: 16, Epoch: 5, Loss: 0.9299, Validation Loss: 0.8292 \n",
            "Hidden Dimension: 16, Epoch: 6, Loss: 0.8509, Validation Loss: 0.8144 \n",
            "Hidden Dimension: 16, Epoch: 7, Loss: 0.8310, Validation Loss: 0.8067 \n",
            "Hidden Dimension: 16, Epoch: 8, Loss: 0.8012, Validation Loss: 0.8058 \n",
            "Hidden Dimension: 16, Epoch: 9, Loss: 0.7678, Validation Loss: 0.8075 \n",
            "Hidden Dimension: 16, Epoch: 10, Loss: 0.7184, Validation Loss: 0.8122 \n",
            "Hidden Dimension: 16, Epoch: 11, Loss: 0.6863, Validation Loss: 0.8195 \n",
            "Early stopping at epoch 12 \n",
            "\n",
            " Hidden Dimension: 16, Validation Loss: 0.8310 \n",
            "\n",
            "Hidden Dimension: 32, Epoch: 0, Loss: 1.7975, Validation Loss: 1.4166 \n",
            "Hidden Dimension: 32, Epoch: 1, Loss: 1.4016, Validation Loss: 1.0738 \n",
            "Hidden Dimension: 32, Epoch: 2, Loss: 1.0787, Validation Loss: 0.8960 \n",
            "Hidden Dimension: 32, Epoch: 3, Loss: 0.9197, Validation Loss: 0.8319 \n",
            "Hidden Dimension: 32, Epoch: 4, Loss: 0.8121, Validation Loss: 0.8363 \n",
            "Hidden Dimension: 32, Epoch: 5, Loss: 0.7976, Validation Loss: 0.8496 \n",
            "Hidden Dimension: 32, Epoch: 6, Loss: 0.7414, Validation Loss: 0.8546 \n",
            "Hidden Dimension: 32, Epoch: 7, Loss: 0.6935, Validation Loss: 0.8687 \n",
            "Early stopping at epoch 8 \n",
            "\n",
            " Hidden Dimension: 32, Validation Loss: 0.8847 \n",
            "\n",
            "Hidden Dimension: 64, Epoch: 0, Loss: 1.7918, Validation Loss: 1.3215 \n",
            "Hidden Dimension: 64, Epoch: 1, Loss: 1.2942, Validation Loss: 1.0023 \n",
            "Hidden Dimension: 64, Epoch: 2, Loss: 0.9919, Validation Loss: 0.8692 \n",
            "Hidden Dimension: 64, Epoch: 3, Loss: 0.8625, Validation Loss: 0.8368 \n",
            "Hidden Dimension: 64, Epoch: 4, Loss: 0.7755, Validation Loss: 0.8576 \n",
            "Hidden Dimension: 64, Epoch: 5, Loss: 0.7692, Validation Loss: 0.8559 \n",
            "Hidden Dimension: 64, Epoch: 6, Loss: 0.6980, Validation Loss: 0.8728 \n",
            "Hidden Dimension: 64, Epoch: 7, Loss: 0.6419, Validation Loss: 0.8891 \n",
            "Early stopping at epoch 8 \n",
            "\n",
            " Hidden Dimension: 64, Validation Loss: 0.9031 \n",
            "\n",
            "Hidden Dimension: 128, Epoch: 0, Loss: 1.7980, Validation Loss: 1.2368 \n",
            "Hidden Dimension: 128, Epoch: 1, Loss: 1.2017, Validation Loss: 0.9722 \n",
            "Hidden Dimension: 128, Epoch: 2, Loss: 0.9658, Validation Loss: 0.8641 \n",
            "Hidden Dimension: 128, Epoch: 3, Loss: 0.8213, Validation Loss: 0.8389 \n",
            "Hidden Dimension: 128, Epoch: 4, Loss: 0.7212, Validation Loss: 0.8939 \n",
            "Hidden Dimension: 128, Epoch: 5, Loss: 0.7094, Validation Loss: 0.8934 \n",
            "Hidden Dimension: 128, Epoch: 6, Loss: 0.6587, Validation Loss: 0.8868 \n",
            "Hidden Dimension: 128, Epoch: 7, Loss: 0.6096, Validation Loss: 0.9053 \n",
            "Early stopping at epoch 8 \n",
            "\n",
            " Hidden Dimension: 128, Validation Loss: 0.9011 \n",
            "\n",
            "Hidden Dimension: 256, Epoch: 0, Loss: 1.7986, Validation Loss: 1.2713 \n",
            "Hidden Dimension: 256, Epoch: 1, Loss: 1.2917, Validation Loss: 1.1274 \n",
            "Hidden Dimension: 256, Epoch: 2, Loss: 1.0609, Validation Loss: 0.9799 \n",
            "Hidden Dimension: 256, Epoch: 3, Loss: 0.9102, Validation Loss: 0.8830 \n",
            "Hidden Dimension: 256, Epoch: 4, Loss: 0.7308, Validation Loss: 0.9037 \n",
            "Hidden Dimension: 256, Epoch: 5, Loss: 0.7389, Validation Loss: 0.8777 \n",
            "Hidden Dimension: 256, Epoch: 6, Loss: 0.7003, Validation Loss: 0.8786 \n",
            "Hidden Dimension: 256, Epoch: 7, Loss: 0.6495, Validation Loss: 0.8991 \n",
            "Hidden Dimension: 256, Epoch: 8, Loss: 0.6008, Validation Loss: 0.9089 \n",
            "Hidden Dimension: 256, Epoch: 9, Loss: 0.5800, Validation Loss: 0.9170 \n",
            "Early stopping at epoch 10 \n",
            "\n",
            " Hidden Dimension: 256, Validation Loss: 0.9245 \n",
            "\n",
            "Best Hidden Dimension: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for citeseer dataset (two layar )**"
      ],
      "metadata": {
        "id": "g5uT09PdSJZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = TwoLayerGAT_v2(input_dim=citeseer.num_node_features,hidden_dim= best_hidden_dim ,output_dim= citeseer_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cca9eef-2cad-4496-c0b3-f2ef0a150c03",
        "id": "b5UU_FcsSJZ3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 1.8322, Validation Loss: 1.6865 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 1.6909, Validation Loss: 1.5543 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 1.5944, Validation Loss: 1.4297 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 1.4490, Validation Loss: 1.3211 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 1.3460, Validation Loss: 1.2365 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 1.2650, Validation Loss: 1.1626 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 1.2073, Validation Loss: 1.1019 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 1.1911, Validation Loss: 1.0559 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 1.1235, Validation Loss: 1.0173 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 1.1003, Validation Loss: 0.9857 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 1.0428, Validation Loss: 0.9587 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 1.0688, Validation Loss: 0.9366 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 1.0114, Validation Loss: 0.9197 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 0.9962, Validation Loss: 0.9069 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 0.9678, Validation Loss: 0.8970 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 0.9408, Validation Loss: 0.8887 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 0.9339, Validation Loss: 0.8824 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 0.8947, Validation Loss: 0.8774 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 0.8924, Validation Loss: 0.8727 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 0.8991, Validation Loss: 0.8681 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 0.8708, Validation Loss: 0.8634 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 0.8664, Validation Loss: 0.8593 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 0.8669, Validation Loss: 0.8558 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 0.8494, Validation Loss: 0.8537 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 0.8313, Validation Loss: 0.8530 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 0.8386, Validation Loss: 0.8529 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 0.7790, Validation Loss: 0.8540 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 0.8287, Validation Loss: 0.8554 \n",
            "Early stopping at epoch 28 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 0.8559 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 1.7967, Validation Loss: 1.6161 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 1.6026, Validation Loss: 1.4018 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 1.4115, Validation Loss: 1.2217 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 1.2005, Validation Loss: 1.0991 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 1.1035, Validation Loss: 1.0183 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 1.0716, Validation Loss: 0.9521 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 1.0156, Validation Loss: 0.9055 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 0.9976, Validation Loss: 0.8778 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 0.9011, Validation Loss: 0.8612 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 0.8909, Validation Loss: 0.8490 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 0.8639, Validation Loss: 0.8368 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 0.8408, Validation Loss: 0.8286 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 0.7985, Validation Loss: 0.8261 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 0.8107, Validation Loss: 0.8264 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 0.7616, Validation Loss: 0.8277 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 0.7566, Validation Loss: 0.8306 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 0.7533, Validation Loss: 0.8342 \n",
            "Early stopping at epoch 17 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 0.8381 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 1.8155, Validation Loss: 1.5532 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 1.5350, Validation Loss: 1.2970 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 1.2793, Validation Loss: 1.0818 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 1.0913, Validation Loss: 0.9452 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 0.9803, Validation Loss: 0.8676 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 0.9250, Validation Loss: 0.8354 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 0.8646, Validation Loss: 0.8292 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 0.8172, Validation Loss: 0.8234 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 0.7996, Validation Loss: 0.8107 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 0.7827, Validation Loss: 0.8012 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 0.7500, Validation Loss: 0.7972 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 0.6839, Validation Loss: 0.7946 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 0.6782, Validation Loss: 0.7927 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 0.6543, Validation Loss: 0.7941 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 0.6413, Validation Loss: 0.7966 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 0.6160, Validation Loss: 0.8026 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 0.6116, Validation Loss: 0.8092 \n",
            "Early stopping at epoch 17 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 0.8158 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 1.7877, Validation Loss: 1.4212 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 1.4026, Validation Loss: 1.0786 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 1.0737, Validation Loss: 0.8896 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 0.9064, Validation Loss: 0.8116 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 0.8203, Validation Loss: 0.7960 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 0.7858, Validation Loss: 0.8063 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 0.7183, Validation Loss: 0.8166 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 0.6884, Validation Loss: 0.8208 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 0.6466, Validation Loss: 0.8300 \n",
            "Early stopping at epoch 9 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 0.8431 \n",
            "\n",
            "Best Number of Attention Heads: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = TwoLayerGAT_v2(input_dim=citeseer.num_node_features,hidden_dim=best_hidden_dim , output_dim=citeseer_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, citeseer)\n",
        "    val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, citeseer)\n",
        "print(f'Test Accuracy(two layer) on citeseer dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df790be-c7c7-459f-9c56-4ab34b9d0fa0",
        "id": "wB-qOzWeSJZ4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 12 \n",
            "\n",
            "Test Accuracy(two layer) on citeseer dataset: 0.7579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One layer GAT V2**"
      ],
      "metadata": {
        "id": "zWeNyFRczo97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATv2Conv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class OneLayerGAT_v2(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_heads, dropout=0.6, negative_slope=0.2 , share_weights = False):\n",
        "        super(OneLayerGAT_v2, self).__init__()\n",
        "        self.gat1 = GATv2Conv(input_dim, output_dim, heads=num_heads, dropout=dropout, negative_slope=negative_slope ,share_weights = share_weights)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.gat1(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "dfv1v2yvzxWh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Cora full dataset(One layer)**"
      ],
      "metadata": {
        "id": "sBx6MONE2J_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  cora full dataset(one layer)**"
      ],
      "metadata": {
        "id": "J_3s079bTcZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = OneLayerGAT_v2(input_dim= CoraFull.num_node_features , output_dim= CoraFull_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a210617-8ee1-473f-b5f7-b371099971c4",
        "id": "ZvQjZGwtTcZG"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 4.2528, Validation Loss: 3.8383 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 3.8225, Validation Loss: 3.4846 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 3.4909, Validation Loss: 3.1567 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 3.2231, Validation Loss: 2.8651 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 2.9754, Validation Loss: 2.6331 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 2.7408, Validation Loss: 2.4521 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 2.5664, Validation Loss: 2.3041 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 2.4458, Validation Loss: 2.1787 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 2.3273, Validation Loss: 2.0708 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 2.2164, Validation Loss: 1.9751 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 2.1577, Validation Loss: 1.8934 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 2.0681, Validation Loss: 1.8237 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 2.0119, Validation Loss: 1.7665 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 1.9641, Validation Loss: 1.7165 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 1.9087, Validation Loss: 1.6717 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 1.8514, Validation Loss: 1.6307 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 1.8126, Validation Loss: 1.5939 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 1.7798, Validation Loss: 1.5627 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 1.7633, Validation Loss: 1.5363 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 1.7298, Validation Loss: 1.5144 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 1.7003, Validation Loss: 1.4970 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 1.7034, Validation Loss: 1.4811 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 1.6718, Validation Loss: 1.4670 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 1.6763, Validation Loss: 1.4549 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 1.6469, Validation Loss: 1.4432 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 1.6318, Validation Loss: 1.4313 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 1.6255, Validation Loss: 1.4213 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 1.6180, Validation Loss: 1.4135 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 1.6357, Validation Loss: 1.4074 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 1.6054, Validation Loss: 1.4023 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 1.5945, Validation Loss: 1.3976 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 1.5774, Validation Loss: 1.3937 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 1.5645, Validation Loss: 1.3918 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 1.5862, Validation Loss: 1.3899 \n",
            "Attention Heads: 1, Epoch: 34, Loss: 1.5557, Validation Loss: 1.3880 \n",
            "Attention Heads: 1, Epoch: 35, Loss: 1.5667, Validation Loss: 1.3862 \n",
            "Attention Heads: 1, Epoch: 36, Loss: 1.5680, Validation Loss: 1.3842 \n",
            "Attention Heads: 1, Epoch: 37, Loss: 1.5746, Validation Loss: 1.3830 \n",
            "Attention Heads: 1, Epoch: 38, Loss: 1.5502, Validation Loss: 1.3822 \n",
            "Attention Heads: 1, Epoch: 39, Loss: 1.5576, Validation Loss: 1.3816 \n",
            "Attention Heads: 1, Epoch: 40, Loss: 1.5652, Validation Loss: 1.3794 \n",
            "Attention Heads: 1, Epoch: 41, Loss: 1.5514, Validation Loss: 1.3764 \n",
            "Attention Heads: 1, Epoch: 42, Loss: 1.5509, Validation Loss: 1.3728 \n",
            "Attention Heads: 1, Epoch: 43, Loss: 1.5573, Validation Loss: 1.3693 \n",
            "Attention Heads: 1, Epoch: 44, Loss: 1.5302, Validation Loss: 1.3664 \n",
            "Attention Heads: 1, Epoch: 45, Loss: 1.5403, Validation Loss: 1.3615 \n",
            "Attention Heads: 1, Epoch: 46, Loss: 1.5388, Validation Loss: 1.3561 \n",
            "Attention Heads: 1, Epoch: 47, Loss: 1.5309, Validation Loss: 1.3510 \n",
            "Attention Heads: 1, Epoch: 48, Loss: 1.5367, Validation Loss: 1.3498 \n",
            "Attention Heads: 1, Epoch: 49, Loss: 1.5183, Validation Loss: 1.3511 \n",
            "Attention Heads: 1, Epoch: 50, Loss: 1.5337, Validation Loss: 1.3528 \n",
            "Attention Heads: 1, Epoch: 51, Loss: 1.5341, Validation Loss: 1.3541 \n",
            "Attention Heads: 1, Epoch: 52, Loss: 1.5330, Validation Loss: 1.3543 \n",
            "Early stopping at epoch 53 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 1.3530 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 4.9430, Validation Loss: 4.2746 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 4.2799, Validation Loss: 3.7243 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 3.8174, Validation Loss: 3.3226 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 3.4922, Validation Loss: 3.0138 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 3.2018, Validation Loss: 2.7845 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 2.9913, Validation Loss: 2.6175 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 2.7959, Validation Loss: 2.4722 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 2.6708, Validation Loss: 2.3254 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 2.5517, Validation Loss: 2.1876 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 2.4386, Validation Loss: 2.0811 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 2.3367, Validation Loss: 1.9985 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 2.2481, Validation Loss: 1.9340 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 2.1998, Validation Loss: 1.8776 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 2.1241, Validation Loss: 1.8269 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 2.0705, Validation Loss: 1.7796 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 2.0242, Validation Loss: 1.7352 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 1.9680, Validation Loss: 1.6939 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 1.9480, Validation Loss: 1.6579 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 1.9013, Validation Loss: 1.6264 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 1.8784, Validation Loss: 1.5999 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 1.8469, Validation Loss: 1.5780 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 1.8058, Validation Loss: 1.5589 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 1.8153, Validation Loss: 1.5409 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 1.7663, Validation Loss: 1.5250 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 1.7557, Validation Loss: 1.5117 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 1.7698, Validation Loss: 1.5021 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 1.7589, Validation Loss: 1.4944 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 1.7210, Validation Loss: 1.4858 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 1.7047, Validation Loss: 1.4739 \n",
            "Attention Heads: 2, Epoch: 29, Loss: 1.7064, Validation Loss: 1.4638 \n",
            "Attention Heads: 2, Epoch: 30, Loss: 1.6781, Validation Loss: 1.4543 \n",
            "Attention Heads: 2, Epoch: 31, Loss: 1.6982, Validation Loss: 1.4492 \n",
            "Attention Heads: 2, Epoch: 32, Loss: 1.7021, Validation Loss: 1.4442 \n",
            "Attention Heads: 2, Epoch: 33, Loss: 1.6621, Validation Loss: 1.4371 \n",
            "Attention Heads: 2, Epoch: 34, Loss: 1.6696, Validation Loss: 1.4279 \n",
            "Attention Heads: 2, Epoch: 35, Loss: 1.6454, Validation Loss: 1.4214 \n",
            "Attention Heads: 2, Epoch: 36, Loss: 1.6772, Validation Loss: 1.4169 \n",
            "Attention Heads: 2, Epoch: 37, Loss: 1.6459, Validation Loss: 1.4139 \n",
            "Attention Heads: 2, Epoch: 38, Loss: 1.6488, Validation Loss: 1.4127 \n",
            "Attention Heads: 2, Epoch: 39, Loss: 1.6296, Validation Loss: 1.4114 \n",
            "Attention Heads: 2, Epoch: 40, Loss: 1.6425, Validation Loss: 1.4080 \n",
            "Attention Heads: 2, Epoch: 41, Loss: 1.6244, Validation Loss: 1.4003 \n",
            "Attention Heads: 2, Epoch: 42, Loss: 1.6230, Validation Loss: 1.3903 \n",
            "Attention Heads: 2, Epoch: 43, Loss: 1.6358, Validation Loss: 1.3868 \n",
            "Attention Heads: 2, Epoch: 44, Loss: 1.6192, Validation Loss: 1.3859 \n",
            "Attention Heads: 2, Epoch: 45, Loss: 1.6370, Validation Loss: 1.3854 \n",
            "Attention Heads: 2, Epoch: 46, Loss: 1.5862, Validation Loss: 1.3832 \n",
            "Attention Heads: 2, Epoch: 47, Loss: 1.6114, Validation Loss: 1.3781 \n",
            "Attention Heads: 2, Epoch: 48, Loss: 1.6120, Validation Loss: 1.3721 \n",
            "Attention Heads: 2, Epoch: 49, Loss: 1.6114, Validation Loss: 1.3693 \n",
            "Attention Heads: 2, Epoch: 50, Loss: 1.6225, Validation Loss: 1.3708 \n",
            "Attention Heads: 2, Epoch: 51, Loss: 1.5973, Validation Loss: 1.3719 \n",
            "Attention Heads: 2, Epoch: 52, Loss: 1.5886, Validation Loss: 1.3685 \n",
            "Attention Heads: 2, Epoch: 53, Loss: 1.6005, Validation Loss: 1.3636 \n",
            "Attention Heads: 2, Epoch: 54, Loss: 1.5786, Validation Loss: 1.3628 \n",
            "Attention Heads: 2, Epoch: 55, Loss: 1.5871, Validation Loss: 1.3645 \n",
            "Attention Heads: 2, Epoch: 56, Loss: 1.5948, Validation Loss: 1.3650 \n",
            "Attention Heads: 2, Epoch: 57, Loss: 1.6022, Validation Loss: 1.3638 \n",
            "Attention Heads: 2, Epoch: 58, Loss: 1.5828, Validation Loss: 1.3618 \n",
            "Attention Heads: 2, Epoch: 59, Loss: 1.5701, Validation Loss: 1.3583 \n",
            "Attention Heads: 2, Epoch: 60, Loss: 1.5887, Validation Loss: 1.3562 \n",
            "Attention Heads: 2, Epoch: 61, Loss: 1.5639, Validation Loss: 1.3522 \n",
            "Attention Heads: 2, Epoch: 62, Loss: 1.5907, Validation Loss: 1.3507 \n",
            "Attention Heads: 2, Epoch: 63, Loss: 1.5702, Validation Loss: 1.3485 \n",
            "Attention Heads: 2, Epoch: 64, Loss: 1.5656, Validation Loss: 1.3446 \n",
            "Attention Heads: 2, Epoch: 65, Loss: 1.5792, Validation Loss: 1.3404 \n",
            "Attention Heads: 2, Epoch: 66, Loss: 1.5795, Validation Loss: 1.3374 \n",
            "Attention Heads: 2, Epoch: 67, Loss: 1.5648, Validation Loss: 1.3326 \n",
            "Attention Heads: 2, Epoch: 68, Loss: 1.5685, Validation Loss: 1.3277 \n",
            "Attention Heads: 2, Epoch: 69, Loss: 1.5489, Validation Loss: 1.3241 \n",
            "Attention Heads: 2, Epoch: 70, Loss: 1.5533, Validation Loss: 1.3230 \n",
            "Attention Heads: 2, Epoch: 71, Loss: 1.5647, Validation Loss: 1.3230 \n",
            "Attention Heads: 2, Epoch: 72, Loss: 1.5380, Validation Loss: 1.3199 \n",
            "Attention Heads: 2, Epoch: 73, Loss: 1.5459, Validation Loss: 1.3164 \n",
            "Attention Heads: 2, Epoch: 74, Loss: 1.5529, Validation Loss: 1.3152 \n",
            "Attention Heads: 2, Epoch: 75, Loss: 1.5318, Validation Loss: 1.3159 \n",
            "Attention Heads: 2, Epoch: 76, Loss: 1.5219, Validation Loss: 1.3169 \n",
            "Attention Heads: 2, Epoch: 77, Loss: 1.5603, Validation Loss: 1.3170 \n",
            "Attention Heads: 2, Epoch: 78, Loss: 1.5383, Validation Loss: 1.3156 \n",
            "Attention Heads: 2, Epoch: 79, Loss: 1.5301, Validation Loss: 1.3137 \n",
            "Attention Heads: 2, Epoch: 80, Loss: 1.5155, Validation Loss: 1.3108 \n",
            "Attention Heads: 2, Epoch: 81, Loss: 1.5201, Validation Loss: 1.3069 \n",
            "Attention Heads: 2, Epoch: 82, Loss: 1.5317, Validation Loss: 1.3041 \n",
            "Attention Heads: 2, Epoch: 83, Loss: 1.5360, Validation Loss: 1.3032 \n",
            "Attention Heads: 2, Epoch: 84, Loss: 1.5217, Validation Loss: 1.3001 \n",
            "Attention Heads: 2, Epoch: 85, Loss: 1.5140, Validation Loss: 1.2931 \n",
            "Attention Heads: 2, Epoch: 86, Loss: 1.5239, Validation Loss: 1.2867 \n",
            "Attention Heads: 2, Epoch: 87, Loss: 1.5017, Validation Loss: 1.2877 \n",
            "Attention Heads: 2, Epoch: 88, Loss: 1.5286, Validation Loss: 1.2908 \n",
            "Attention Heads: 2, Epoch: 89, Loss: 1.5283, Validation Loss: 1.2930 \n",
            "Attention Heads: 2, Epoch: 90, Loss: 1.5144, Validation Loss: 1.2874 \n",
            "Attention Heads: 2, Epoch: 91, Loss: 1.5091, Validation Loss: 1.2843 \n",
            "Attention Heads: 2, Epoch: 92, Loss: 1.5041, Validation Loss: 1.2833 \n",
            "Attention Heads: 2, Epoch: 93, Loss: 1.5148, Validation Loss: 1.2849 \n",
            "Attention Heads: 2, Epoch: 94, Loss: 1.5198, Validation Loss: 1.2855 \n",
            "Attention Heads: 2, Epoch: 95, Loss: 1.5188, Validation Loss: 1.2832 \n",
            "Attention Heads: 2, Epoch: 96, Loss: 1.5253, Validation Loss: 1.2796 \n",
            "Attention Heads: 2, Epoch: 97, Loss: 1.5051, Validation Loss: 1.2767 \n",
            "Attention Heads: 2, Epoch: 98, Loss: 1.4986, Validation Loss: 1.2774 \n",
            "Attention Heads: 2, Epoch: 99, Loss: 1.5059, Validation Loss: 1.2788 \n",
            "Attention Heads: 2, Epoch: 100, Loss: 1.5101, Validation Loss: 1.2786 \n",
            "Attention Heads: 2, Epoch: 101, Loss: 1.5060, Validation Loss: 1.2771 \n",
            "Attention Heads: 2, Epoch: 102, Loss: 1.5018, Validation Loss: 1.2751 \n",
            "Attention Heads: 2, Epoch: 103, Loss: 1.5254, Validation Loss: 1.2734 \n",
            "Attention Heads: 2, Epoch: 104, Loss: 1.5053, Validation Loss: 1.2704 \n",
            "Attention Heads: 2, Epoch: 105, Loss: 1.5149, Validation Loss: 1.2668 \n",
            "Attention Heads: 2, Epoch: 106, Loss: 1.5164, Validation Loss: 1.2632 \n",
            "Attention Heads: 2, Epoch: 107, Loss: 1.4970, Validation Loss: 1.2596 \n",
            "Attention Heads: 2, Epoch: 108, Loss: 1.5171, Validation Loss: 1.2582 \n",
            "Attention Heads: 2, Epoch: 109, Loss: 1.5134, Validation Loss: 1.2589 \n",
            "Attention Heads: 2, Epoch: 110, Loss: 1.4829, Validation Loss: 1.2612 \n",
            "Attention Heads: 2, Epoch: 111, Loss: 1.4793, Validation Loss: 1.2629 \n",
            "Attention Heads: 2, Epoch: 112, Loss: 1.4656, Validation Loss: 1.2653 \n",
            "Early stopping at epoch 113 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 1.2666 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 5.6404, Validation Loss: 4.8357 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 4.8462, Validation Loss: 4.1246 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 4.2405, Validation Loss: 3.6396 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 3.8628, Validation Loss: 3.2711 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 3.5424, Validation Loss: 3.0092 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 3.2857, Validation Loss: 2.8352 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 3.0859, Validation Loss: 2.6838 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 2.9333, Validation Loss: 2.5124 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 2.8114, Validation Loss: 2.3554 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 2.6963, Validation Loss: 2.2487 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 2.5802, Validation Loss: 2.1730 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 2.4976, Validation Loss: 2.1198 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 2.3863, Validation Loss: 2.0586 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 2.3467, Validation Loss: 1.9761 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 2.2944, Validation Loss: 1.9082 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 2.2209, Validation Loss: 1.8655 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 2.1844, Validation Loss: 1.8400 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 2.1335, Validation Loss: 1.8027 \n",
            "Attention Heads: 4, Epoch: 18, Loss: 2.1076, Validation Loss: 1.7532 \n",
            "Attention Heads: 4, Epoch: 19, Loss: 2.0875, Validation Loss: 1.7206 \n",
            "Attention Heads: 4, Epoch: 20, Loss: 2.0474, Validation Loss: 1.7042 \n",
            "Attention Heads: 4, Epoch: 21, Loss: 2.0067, Validation Loss: 1.6837 \n",
            "Attention Heads: 4, Epoch: 22, Loss: 1.9987, Validation Loss: 1.6524 \n",
            "Attention Heads: 4, Epoch: 23, Loss: 1.9409, Validation Loss: 1.6208 \n",
            "Attention Heads: 4, Epoch: 24, Loss: 1.9308, Validation Loss: 1.5998 \n",
            "Attention Heads: 4, Epoch: 25, Loss: 1.9089, Validation Loss: 1.5883 \n",
            "Attention Heads: 4, Epoch: 26, Loss: 1.8912, Validation Loss: 1.5750 \n",
            "Attention Heads: 4, Epoch: 27, Loss: 1.8540, Validation Loss: 1.5539 \n",
            "Attention Heads: 4, Epoch: 28, Loss: 1.8719, Validation Loss: 1.5359 \n",
            "Attention Heads: 4, Epoch: 29, Loss: 1.8502, Validation Loss: 1.5271 \n",
            "Attention Heads: 4, Epoch: 30, Loss: 1.8478, Validation Loss: 1.5221 \n",
            "Attention Heads: 4, Epoch: 31, Loss: 1.8318, Validation Loss: 1.5131 \n",
            "Attention Heads: 4, Epoch: 32, Loss: 1.8060, Validation Loss: 1.4994 \n",
            "Attention Heads: 4, Epoch: 33, Loss: 1.7853, Validation Loss: 1.4887 \n",
            "Attention Heads: 4, Epoch: 34, Loss: 1.7960, Validation Loss: 1.4852 \n",
            "Attention Heads: 4, Epoch: 35, Loss: 1.7862, Validation Loss: 1.4801 \n",
            "Attention Heads: 4, Epoch: 36, Loss: 1.7844, Validation Loss: 1.4731 \n",
            "Attention Heads: 4, Epoch: 37, Loss: 1.7784, Validation Loss: 1.4640 \n",
            "Attention Heads: 4, Epoch: 38, Loss: 1.7738, Validation Loss: 1.4524 \n",
            "Attention Heads: 4, Epoch: 39, Loss: 1.7697, Validation Loss: 1.4511 \n",
            "Attention Heads: 4, Epoch: 40, Loss: 1.7509, Validation Loss: 1.4497 \n",
            "Attention Heads: 4, Epoch: 41, Loss: 1.7470, Validation Loss: 1.4409 \n",
            "Attention Heads: 4, Epoch: 42, Loss: 1.7302, Validation Loss: 1.4289 \n",
            "Attention Heads: 4, Epoch: 43, Loss: 1.7104, Validation Loss: 1.4248 \n",
            "Attention Heads: 4, Epoch: 44, Loss: 1.7303, Validation Loss: 1.4317 \n",
            "Attention Heads: 4, Epoch: 45, Loss: 1.7223, Validation Loss: 1.4323 \n",
            "Attention Heads: 4, Epoch: 46, Loss: 1.7246, Validation Loss: 1.4181 \n",
            "Attention Heads: 4, Epoch: 47, Loss: 1.7079, Validation Loss: 1.4116 \n",
            "Attention Heads: 4, Epoch: 48, Loss: 1.7139, Validation Loss: 1.4177 \n",
            "Attention Heads: 4, Epoch: 49, Loss: 1.7110, Validation Loss: 1.4187 \n",
            "Attention Heads: 4, Epoch: 50, Loss: 1.7077, Validation Loss: 1.4087 \n",
            "Attention Heads: 4, Epoch: 51, Loss: 1.6888, Validation Loss: 1.4058 \n",
            "Attention Heads: 4, Epoch: 52, Loss: 1.6969, Validation Loss: 1.4091 \n",
            "Attention Heads: 4, Epoch: 53, Loss: 1.6882, Validation Loss: 1.4113 \n",
            "Attention Heads: 4, Epoch: 54, Loss: 1.6709, Validation Loss: 1.4022 \n",
            "Attention Heads: 4, Epoch: 55, Loss: 1.6901, Validation Loss: 1.3895 \n",
            "Attention Heads: 4, Epoch: 56, Loss: 1.6712, Validation Loss: 1.3898 \n",
            "Attention Heads: 4, Epoch: 57, Loss: 1.6750, Validation Loss: 1.3966 \n",
            "Attention Heads: 4, Epoch: 58, Loss: 1.6682, Validation Loss: 1.3913 \n",
            "Attention Heads: 4, Epoch: 59, Loss: 1.6683, Validation Loss: 1.3815 \n",
            "Attention Heads: 4, Epoch: 60, Loss: 1.6633, Validation Loss: 1.3769 \n",
            "Attention Heads: 4, Epoch: 61, Loss: 1.6330, Validation Loss: 1.3816 \n",
            "Attention Heads: 4, Epoch: 62, Loss: 1.6751, Validation Loss: 1.3797 \n",
            "Attention Heads: 4, Epoch: 63, Loss: 1.6642, Validation Loss: 1.3703 \n",
            "Attention Heads: 4, Epoch: 64, Loss: 1.6620, Validation Loss: 1.3678 \n",
            "Attention Heads: 4, Epoch: 65, Loss: 1.6362, Validation Loss: 1.3736 \n",
            "Attention Heads: 4, Epoch: 66, Loss: 1.6498, Validation Loss: 1.3678 \n",
            "Attention Heads: 4, Epoch: 67, Loss: 1.6338, Validation Loss: 1.3581 \n",
            "Attention Heads: 4, Epoch: 68, Loss: 1.6177, Validation Loss: 1.3536 \n",
            "Attention Heads: 4, Epoch: 69, Loss: 1.6426, Validation Loss: 1.3568 \n",
            "Attention Heads: 4, Epoch: 70, Loss: 1.6592, Validation Loss: 1.3606 \n",
            "Attention Heads: 4, Epoch: 71, Loss: 1.6414, Validation Loss: 1.3490 \n",
            "Attention Heads: 4, Epoch: 72, Loss: 1.6442, Validation Loss: 1.3402 \n",
            "Attention Heads: 4, Epoch: 73, Loss: 1.5950, Validation Loss: 1.3460 \n",
            "Attention Heads: 4, Epoch: 74, Loss: 1.6267, Validation Loss: 1.3474 \n",
            "Attention Heads: 4, Epoch: 75, Loss: 1.6346, Validation Loss: 1.3365 \n",
            "Attention Heads: 4, Epoch: 76, Loss: 1.6185, Validation Loss: 1.3290 \n",
            "Attention Heads: 4, Epoch: 77, Loss: 1.6316, Validation Loss: 1.3352 \n",
            "Attention Heads: 4, Epoch: 78, Loss: 1.6242, Validation Loss: 1.3370 \n",
            "Attention Heads: 4, Epoch: 79, Loss: 1.6257, Validation Loss: 1.3259 \n",
            "Attention Heads: 4, Epoch: 80, Loss: 1.6306, Validation Loss: 1.3202 \n",
            "Attention Heads: 4, Epoch: 81, Loss: 1.6155, Validation Loss: 1.3301 \n",
            "Attention Heads: 4, Epoch: 82, Loss: 1.5966, Validation Loss: 1.3272 \n",
            "Attention Heads: 4, Epoch: 83, Loss: 1.6141, Validation Loss: 1.3197 \n",
            "Attention Heads: 4, Epoch: 84, Loss: 1.5986, Validation Loss: 1.3152 \n",
            "Attention Heads: 4, Epoch: 85, Loss: 1.6311, Validation Loss: 1.3175 \n",
            "Attention Heads: 4, Epoch: 86, Loss: 1.6167, Validation Loss: 1.3196 \n",
            "Attention Heads: 4, Epoch: 87, Loss: 1.5839, Validation Loss: 1.3157 \n",
            "Attention Heads: 4, Epoch: 88, Loss: 1.6056, Validation Loss: 1.3118 \n",
            "Attention Heads: 4, Epoch: 89, Loss: 1.6030, Validation Loss: 1.3115 \n",
            "Attention Heads: 4, Epoch: 90, Loss: 1.5784, Validation Loss: 1.3128 \n",
            "Attention Heads: 4, Epoch: 91, Loss: 1.5938, Validation Loss: 1.3074 \n",
            "Attention Heads: 4, Epoch: 92, Loss: 1.5966, Validation Loss: 1.3029 \n",
            "Attention Heads: 4, Epoch: 93, Loss: 1.6005, Validation Loss: 1.3042 \n",
            "Attention Heads: 4, Epoch: 94, Loss: 1.5688, Validation Loss: 1.3036 \n",
            "Attention Heads: 4, Epoch: 95, Loss: 1.5869, Validation Loss: 1.3044 \n",
            "Attention Heads: 4, Epoch: 96, Loss: 1.5769, Validation Loss: 1.3056 \n",
            "Early stopping at epoch 97 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 1.3043 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 6.3339, Validation Loss: 5.3758 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 5.4091, Validation Loss: 4.4938 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 4.7075, Validation Loss: 3.9198 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 4.2478, Validation Loss: 3.4921 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 3.9049, Validation Loss: 3.1707 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 3.6488, Validation Loss: 2.9454 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 3.4300, Validation Loss: 2.8069 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 3.2571, Validation Loss: 2.7307 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 3.1206, Validation Loss: 2.6377 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 2.9754, Validation Loss: 2.4825 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 2.8664, Validation Loss: 2.3336 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 2.7850, Validation Loss: 2.2342 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 2.6856, Validation Loss: 2.1863 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 2.6109, Validation Loss: 2.1641 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 2.5389, Validation Loss: 2.1070 \n",
            "Attention Heads: 8, Epoch: 15, Loss: 2.4599, Validation Loss: 2.0136 \n",
            "Attention Heads: 8, Epoch: 16, Loss: 2.4491, Validation Loss: 1.9530 \n",
            "Attention Heads: 8, Epoch: 17, Loss: 2.3740, Validation Loss: 1.9277 \n",
            "Attention Heads: 8, Epoch: 18, Loss: 2.3707, Validation Loss: 1.9122 \n",
            "Attention Heads: 8, Epoch: 19, Loss: 2.3064, Validation Loss: 1.8635 \n",
            "Attention Heads: 8, Epoch: 20, Loss: 2.2619, Validation Loss: 1.7971 \n",
            "Attention Heads: 8, Epoch: 21, Loss: 2.1932, Validation Loss: 1.7575 \n",
            "Attention Heads: 8, Epoch: 22, Loss: 2.1867, Validation Loss: 1.7525 \n",
            "Attention Heads: 8, Epoch: 23, Loss: 2.1683, Validation Loss: 1.7466 \n",
            "Attention Heads: 8, Epoch: 24, Loss: 2.1314, Validation Loss: 1.7094 \n",
            "Attention Heads: 8, Epoch: 25, Loss: 2.0723, Validation Loss: 1.6603 \n",
            "Attention Heads: 8, Epoch: 26, Loss: 2.0786, Validation Loss: 1.6489 \n",
            "Attention Heads: 8, Epoch: 27, Loss: 2.0755, Validation Loss: 1.6546 \n",
            "Attention Heads: 8, Epoch: 28, Loss: 2.0300, Validation Loss: 1.6382 \n",
            "Attention Heads: 8, Epoch: 29, Loss: 2.0136, Validation Loss: 1.6001 \n",
            "Attention Heads: 8, Epoch: 30, Loss: 1.9893, Validation Loss: 1.5752 \n",
            "Attention Heads: 8, Epoch: 31, Loss: 1.9916, Validation Loss: 1.5776 \n",
            "Attention Heads: 8, Epoch: 32, Loss: 1.9747, Validation Loss: 1.5853 \n",
            "Attention Heads: 8, Epoch: 33, Loss: 1.9581, Validation Loss: 1.5701 \n",
            "Attention Heads: 8, Epoch: 34, Loss: 1.9452, Validation Loss: 1.5365 \n",
            "Attention Heads: 8, Epoch: 35, Loss: 1.9350, Validation Loss: 1.5239 \n",
            "Attention Heads: 8, Epoch: 36, Loss: 1.9052, Validation Loss: 1.5314 \n",
            "Attention Heads: 8, Epoch: 37, Loss: 1.9057, Validation Loss: 1.5315 \n",
            "Attention Heads: 8, Epoch: 38, Loss: 1.9125, Validation Loss: 1.5142 \n",
            "Attention Heads: 8, Epoch: 39, Loss: 1.9296, Validation Loss: 1.4959 \n",
            "Attention Heads: 8, Epoch: 40, Loss: 1.8837, Validation Loss: 1.4915 \n",
            "Attention Heads: 8, Epoch: 41, Loss: 1.8875, Validation Loss: 1.4961 \n",
            "Attention Heads: 8, Epoch: 42, Loss: 1.8813, Validation Loss: 1.4931 \n",
            "Attention Heads: 8, Epoch: 43, Loss: 1.8747, Validation Loss: 1.4778 \n",
            "Attention Heads: 8, Epoch: 44, Loss: 1.8550, Validation Loss: 1.4692 \n",
            "Attention Heads: 8, Epoch: 45, Loss: 1.8437, Validation Loss: 1.4691 \n",
            "Attention Heads: 8, Epoch: 46, Loss: 1.8201, Validation Loss: 1.4652 \n",
            "Attention Heads: 8, Epoch: 47, Loss: 1.8433, Validation Loss: 1.4620 \n",
            "Attention Heads: 8, Epoch: 48, Loss: 1.8293, Validation Loss: 1.4530 \n",
            "Attention Heads: 8, Epoch: 49, Loss: 1.8213, Validation Loss: 1.4457 \n",
            "Attention Heads: 8, Epoch: 50, Loss: 1.8069, Validation Loss: 1.4489 \n",
            "Attention Heads: 8, Epoch: 51, Loss: 1.8176, Validation Loss: 1.4499 \n",
            "Attention Heads: 8, Epoch: 52, Loss: 1.8119, Validation Loss: 1.4353 \n",
            "Attention Heads: 8, Epoch: 53, Loss: 1.7833, Validation Loss: 1.4275 \n",
            "Attention Heads: 8, Epoch: 54, Loss: 1.8120, Validation Loss: 1.4339 \n",
            "Attention Heads: 8, Epoch: 55, Loss: 1.7787, Validation Loss: 1.4315 \n",
            "Attention Heads: 8, Epoch: 56, Loss: 1.8055, Validation Loss: 1.4197 \n",
            "Attention Heads: 8, Epoch: 57, Loss: 1.7938, Validation Loss: 1.4188 \n",
            "Attention Heads: 8, Epoch: 58, Loss: 1.7677, Validation Loss: 1.4324 \n",
            "Attention Heads: 8, Epoch: 59, Loss: 1.7931, Validation Loss: 1.4241 \n",
            "Attention Heads: 8, Epoch: 60, Loss: 1.7861, Validation Loss: 1.4125 \n",
            "Attention Heads: 8, Epoch: 61, Loss: 1.7811, Validation Loss: 1.4163 \n",
            "Attention Heads: 8, Epoch: 62, Loss: 1.7501, Validation Loss: 1.4190 \n",
            "Attention Heads: 8, Epoch: 63, Loss: 1.7563, Validation Loss: 1.4073 \n",
            "Attention Heads: 8, Epoch: 64, Loss: 1.7671, Validation Loss: 1.4002 \n",
            "Attention Heads: 8, Epoch: 65, Loss: 1.7594, Validation Loss: 1.4043 \n",
            "Attention Heads: 8, Epoch: 66, Loss: 1.7630, Validation Loss: 1.4045 \n",
            "Attention Heads: 8, Epoch: 67, Loss: 1.7482, Validation Loss: 1.3943 \n",
            "Attention Heads: 8, Epoch: 68, Loss: 1.7672, Validation Loss: 1.3837 \n",
            "Attention Heads: 8, Epoch: 69, Loss: 1.7337, Validation Loss: 1.3860 \n",
            "Attention Heads: 8, Epoch: 70, Loss: 1.7370, Validation Loss: 1.3829 \n",
            "Attention Heads: 8, Epoch: 71, Loss: 1.7420, Validation Loss: 1.3791 \n",
            "Attention Heads: 8, Epoch: 72, Loss: 1.7290, Validation Loss: 1.3723 \n",
            "Attention Heads: 8, Epoch: 73, Loss: 1.7234, Validation Loss: 1.3660 \n",
            "Attention Heads: 8, Epoch: 74, Loss: 1.7300, Validation Loss: 1.3697 \n",
            "Attention Heads: 8, Epoch: 75, Loss: 1.7490, Validation Loss: 1.3737 \n",
            "Attention Heads: 8, Epoch: 76, Loss: 1.7509, Validation Loss: 1.3696 \n",
            "Attention Heads: 8, Epoch: 77, Loss: 1.7479, Validation Loss: 1.3566 \n",
            "Attention Heads: 8, Epoch: 78, Loss: 1.7280, Validation Loss: 1.3608 \n",
            "Attention Heads: 8, Epoch: 79, Loss: 1.7379, Validation Loss: 1.3659 \n",
            "Attention Heads: 8, Epoch: 80, Loss: 1.6921, Validation Loss: 1.3563 \n",
            "Attention Heads: 8, Epoch: 81, Loss: 1.7029, Validation Loss: 1.3451 \n",
            "Attention Heads: 8, Epoch: 82, Loss: 1.7036, Validation Loss: 1.3512 \n",
            "Attention Heads: 8, Epoch: 83, Loss: 1.6949, Validation Loss: 1.3525 \n",
            "Attention Heads: 8, Epoch: 84, Loss: 1.6948, Validation Loss: 1.3404 \n",
            "Attention Heads: 8, Epoch: 85, Loss: 1.7084, Validation Loss: 1.3346 \n",
            "Attention Heads: 8, Epoch: 86, Loss: 1.6931, Validation Loss: 1.3451 \n",
            "Attention Heads: 8, Epoch: 87, Loss: 1.6925, Validation Loss: 1.3404 \n",
            "Attention Heads: 8, Epoch: 88, Loss: 1.6759, Validation Loss: 1.3315 \n",
            "Attention Heads: 8, Epoch: 89, Loss: 1.6844, Validation Loss: 1.3318 \n",
            "Attention Heads: 8, Epoch: 90, Loss: 1.6706, Validation Loss: 1.3397 \n",
            "Attention Heads: 8, Epoch: 91, Loss: 1.6788, Validation Loss: 1.3353 \n",
            "Attention Heads: 8, Epoch: 92, Loss: 1.6803, Validation Loss: 1.3277 \n",
            "Attention Heads: 8, Epoch: 93, Loss: 1.7001, Validation Loss: 1.3340 \n",
            "Attention Heads: 8, Epoch: 94, Loss: 1.6693, Validation Loss: 1.3356 \n",
            "Attention Heads: 8, Epoch: 95, Loss: 1.6709, Validation Loss: 1.3266 \n",
            "Attention Heads: 8, Epoch: 96, Loss: 1.6743, Validation Loss: 1.3157 \n",
            "Attention Heads: 8, Epoch: 97, Loss: 1.6714, Validation Loss: 1.3210 \n",
            "Attention Heads: 8, Epoch: 98, Loss: 1.6625, Validation Loss: 1.3249 \n",
            "Attention Heads: 8, Epoch: 99, Loss: 1.6855, Validation Loss: 1.3150 \n",
            "Attention Heads: 8, Epoch: 100, Loss: 1.6861, Validation Loss: 1.3058 \n",
            "Attention Heads: 8, Epoch: 101, Loss: 1.6544, Validation Loss: 1.3102 \n",
            "Attention Heads: 8, Epoch: 102, Loss: 1.6473, Validation Loss: 1.3177 \n",
            "Attention Heads: 8, Epoch: 103, Loss: 1.6385, Validation Loss: 1.3083 \n",
            "Attention Heads: 8, Epoch: 104, Loss: 1.6527, Validation Loss: 1.3017 \n",
            "Attention Heads: 8, Epoch: 105, Loss: 1.6497, Validation Loss: 1.3019 \n",
            "Attention Heads: 8, Epoch: 106, Loss: 1.6613, Validation Loss: 1.3103 \n",
            "Attention Heads: 8, Epoch: 107, Loss: 1.6498, Validation Loss: 1.3058 \n",
            "Attention Heads: 8, Epoch: 108, Loss: 1.6656, Validation Loss: 1.2987 \n",
            "Attention Heads: 8, Epoch: 109, Loss: 1.6460, Validation Loss: 1.3009 \n",
            "Attention Heads: 8, Epoch: 110, Loss: 1.6492, Validation Loss: 1.3089 \n",
            "Attention Heads: 8, Epoch: 111, Loss: 1.6680, Validation Loss: 1.3025 \n",
            "Attention Heads: 8, Epoch: 112, Loss: 1.6195, Validation Loss: 1.2935 \n",
            "Attention Heads: 8, Epoch: 113, Loss: 1.6692, Validation Loss: 1.2951 \n",
            "Attention Heads: 8, Epoch: 114, Loss: 1.6570, Validation Loss: 1.3004 \n",
            "Attention Heads: 8, Epoch: 115, Loss: 1.6557, Validation Loss: 1.2975 \n",
            "Attention Heads: 8, Epoch: 116, Loss: 1.6442, Validation Loss: 1.2917 \n",
            "Attention Heads: 8, Epoch: 117, Loss: 1.6334, Validation Loss: 1.2917 \n",
            "Attention Heads: 8, Epoch: 118, Loss: 1.6267, Validation Loss: 1.2961 \n",
            "Attention Heads: 8, Epoch: 119, Loss: 1.6253, Validation Loss: 1.2969 \n",
            "Attention Heads: 8, Epoch: 120, Loss: 1.6032, Validation Loss: 1.2922 \n",
            "Early stopping at epoch 121 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 1.2935 \n",
            "\n",
            "Best Number of Attention Heads: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = OneLayerGAT_v2(input_dim=CoraFull.num_node_features, output_dim=CoraFull_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, CoraFull)\n",
        "    val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, CoraFull)\n",
        "print(f'Test Accuracy(one layer) on CoraFull dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHNyaoJ8URN_",
        "outputId": "2978be7a-51e2-40ef-e4d0-266cac5b1a22"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 85 \n",
            "\n",
            "Test Accuracy(one layer) on CoraFull dataset: 0.6998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Citeseer dataset(One layer)**"
      ],
      "metadata": {
        "id": "I5xR9XLk1SF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  citeseer dataset(one layer)**"
      ],
      "metadata": {
        "id": "7ybUEYxPUjxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = OneLayerGAT_v2(input_dim= citeseer.num_node_features , output_dim= citeseer_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863a566c-293b-4441-ff4f-4d71e36b13e6",
        "id": "nFLYTHEPUjxz"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 1.7895, Validation Loss: 1.6489 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 1.6214, Validation Loss: 1.5264 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 1.4957, Validation Loss: 1.4191 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 1.3919, Validation Loss: 1.3242 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 1.2975, Validation Loss: 1.2424 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 1.2049, Validation Loss: 1.1723 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 1.1517, Validation Loss: 1.1125 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 1.0823, Validation Loss: 1.0620 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 1.0646, Validation Loss: 1.0201 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 1.0139, Validation Loss: 0.9853 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 0.9657, Validation Loss: 0.9558 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 0.9426, Validation Loss: 0.9308 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 0.9243, Validation Loss: 0.9090 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 0.9134, Validation Loss: 0.8906 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 0.8763, Validation Loss: 0.8749 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 0.8684, Validation Loss: 0.8618 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 0.8679, Validation Loss: 0.8511 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 0.8337, Validation Loss: 0.8425 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 0.8205, Validation Loss: 0.8351 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 0.8104, Validation Loss: 0.8293 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 0.8000, Validation Loss: 0.8247 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 0.7955, Validation Loss: 0.8211 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 0.7868, Validation Loss: 0.8183 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 0.8057, Validation Loss: 0.8166 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 0.7872, Validation Loss: 0.8155 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 0.7585, Validation Loss: 0.8146 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 0.7480, Validation Loss: 0.8141 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 0.7482, Validation Loss: 0.8138 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 0.7559, Validation Loss: 0.8134 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 0.7392, Validation Loss: 0.8129 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 0.7352, Validation Loss: 0.8125 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 0.7386, Validation Loss: 0.8121 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 0.7289, Validation Loss: 0.8119 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 0.7365, Validation Loss: 0.8117 \n",
            "Attention Heads: 1, Epoch: 34, Loss: 0.7068, Validation Loss: 0.8119 \n",
            "Attention Heads: 1, Epoch: 35, Loss: 0.7258, Validation Loss: 0.8123 \n",
            "Attention Heads: 1, Epoch: 36, Loss: 0.7293, Validation Loss: 0.8128 \n",
            "Early stopping at epoch 37 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 0.8136 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 2.5059, Validation Loss: 2.1778 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 2.1629, Validation Loss: 1.9249 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 1.9202, Validation Loss: 1.7263 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 1.7258, Validation Loss: 1.5683 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 1.5705, Validation Loss: 1.4410 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 1.4629, Validation Loss: 1.3371 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 1.3734, Validation Loss: 1.2519 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 1.2837, Validation Loss: 1.1808 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 1.2086, Validation Loss: 1.1221 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 1.1555, Validation Loss: 1.0736 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 1.1095, Validation Loss: 1.0335 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 1.0928, Validation Loss: 1.0007 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 1.0316, Validation Loss: 0.9734 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 0.9873, Validation Loss: 0.9503 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 0.9813, Validation Loss: 0.9305 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 0.9440, Validation Loss: 0.9134 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 0.9456, Validation Loss: 0.8986 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 0.9466, Validation Loss: 0.8859 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 0.9145, Validation Loss: 0.8753 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 0.9140, Validation Loss: 0.8662 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 0.8671, Validation Loss: 0.8586 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 0.8959, Validation Loss: 0.8526 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 0.8767, Validation Loss: 0.8475 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 0.8509, Validation Loss: 0.8433 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 0.8443, Validation Loss: 0.8402 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 0.8553, Validation Loss: 0.8373 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 0.8214, Validation Loss: 0.8352 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 0.7832, Validation Loss: 0.8336 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 0.8115, Validation Loss: 0.8323 \n",
            "Attention Heads: 2, Epoch: 29, Loss: 0.8159, Validation Loss: 0.8316 \n",
            "Attention Heads: 2, Epoch: 30, Loss: 0.8151, Validation Loss: 0.8310 \n",
            "Attention Heads: 2, Epoch: 31, Loss: 0.8101, Validation Loss: 0.8305 \n",
            "Attention Heads: 2, Epoch: 32, Loss: 0.8149, Validation Loss: 0.8304 \n",
            "Attention Heads: 2, Epoch: 33, Loss: 0.7895, Validation Loss: 0.8306 \n",
            "Attention Heads: 2, Epoch: 34, Loss: 0.7776, Validation Loss: 0.8306 \n",
            "Early stopping at epoch 35 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 0.8303 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 3.1798, Validation Loss: 2.7183 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 2.7255, Validation Loss: 2.3414 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 2.3523, Validation Loss: 2.0445 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 2.1100, Validation Loss: 1.8163 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 1.9026, Validation Loss: 1.6397 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 1.7518, Validation Loss: 1.5005 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 1.6115, Validation Loss: 1.3886 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 1.5085, Validation Loss: 1.2966 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 1.4306, Validation Loss: 1.2203 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 1.3723, Validation Loss: 1.1569 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 1.3205, Validation Loss: 1.1042 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 1.2342, Validation Loss: 1.0599 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 1.2181, Validation Loss: 1.0232 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 1.1595, Validation Loss: 0.9920 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 1.1506, Validation Loss: 0.9662 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 1.1409, Validation Loss: 0.9439 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 1.1050, Validation Loss: 0.9251 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 1.0613, Validation Loss: 0.9092 \n",
            "Attention Heads: 4, Epoch: 18, Loss: 1.0366, Validation Loss: 0.8961 \n",
            "Attention Heads: 4, Epoch: 19, Loss: 1.0478, Validation Loss: 0.8853 \n",
            "Attention Heads: 4, Epoch: 20, Loss: 1.0434, Validation Loss: 0.8760 \n",
            "Attention Heads: 4, Epoch: 21, Loss: 1.0420, Validation Loss: 0.8682 \n",
            "Attention Heads: 4, Epoch: 22, Loss: 0.9673, Validation Loss: 0.8615 \n",
            "Attention Heads: 4, Epoch: 23, Loss: 0.9551, Validation Loss: 0.8557 \n",
            "Attention Heads: 4, Epoch: 24, Loss: 0.9725, Validation Loss: 0.8506 \n",
            "Attention Heads: 4, Epoch: 25, Loss: 0.9319, Validation Loss: 0.8464 \n",
            "Attention Heads: 4, Epoch: 26, Loss: 0.9091, Validation Loss: 0.8427 \n",
            "Attention Heads: 4, Epoch: 27, Loss: 0.9197, Validation Loss: 0.8391 \n",
            "Attention Heads: 4, Epoch: 28, Loss: 0.9496, Validation Loss: 0.8356 \n",
            "Attention Heads: 4, Epoch: 29, Loss: 0.9326, Validation Loss: 0.8323 \n",
            "Attention Heads: 4, Epoch: 30, Loss: 0.9286, Validation Loss: 0.8293 \n",
            "Attention Heads: 4, Epoch: 31, Loss: 0.9039, Validation Loss: 0.8266 \n",
            "Attention Heads: 4, Epoch: 32, Loss: 0.9142, Validation Loss: 0.8239 \n",
            "Attention Heads: 4, Epoch: 33, Loss: 0.9237, Validation Loss: 0.8216 \n",
            "Attention Heads: 4, Epoch: 34, Loss: 0.8975, Validation Loss: 0.8195 \n",
            "Attention Heads: 4, Epoch: 35, Loss: 0.8621, Validation Loss: 0.8178 \n",
            "Attention Heads: 4, Epoch: 36, Loss: 0.9030, Validation Loss: 0.8162 \n",
            "Attention Heads: 4, Epoch: 37, Loss: 0.8946, Validation Loss: 0.8147 \n",
            "Attention Heads: 4, Epoch: 38, Loss: 0.8499, Validation Loss: 0.8137 \n",
            "Attention Heads: 4, Epoch: 39, Loss: 0.8710, Validation Loss: 0.8129 \n",
            "Attention Heads: 4, Epoch: 40, Loss: 0.8442, Validation Loss: 0.8119 \n",
            "Attention Heads: 4, Epoch: 41, Loss: 0.8805, Validation Loss: 0.8110 \n",
            "Attention Heads: 4, Epoch: 42, Loss: 0.8827, Validation Loss: 0.8102 \n",
            "Attention Heads: 4, Epoch: 43, Loss: 0.8749, Validation Loss: 0.8095 \n",
            "Attention Heads: 4, Epoch: 44, Loss: 0.8893, Validation Loss: 0.8087 \n",
            "Attention Heads: 4, Epoch: 45, Loss: 0.8637, Validation Loss: 0.8079 \n",
            "Attention Heads: 4, Epoch: 46, Loss: 0.8192, Validation Loss: 0.8073 \n",
            "Attention Heads: 4, Epoch: 47, Loss: 0.8360, Validation Loss: 0.8068 \n",
            "Attention Heads: 4, Epoch: 48, Loss: 0.8267, Validation Loss: 0.8064 \n",
            "Attention Heads: 4, Epoch: 49, Loss: 0.8402, Validation Loss: 0.8059 \n",
            "Attention Heads: 4, Epoch: 50, Loss: 0.8413, Validation Loss: 0.8055 \n",
            "Attention Heads: 4, Epoch: 51, Loss: 0.8365, Validation Loss: 0.8050 \n",
            "Attention Heads: 4, Epoch: 52, Loss: 0.8416, Validation Loss: 0.8046 \n",
            "Attention Heads: 4, Epoch: 53, Loss: 0.8200, Validation Loss: 0.8042 \n",
            "Attention Heads: 4, Epoch: 54, Loss: 0.8268, Validation Loss: 0.8041 \n",
            "Attention Heads: 4, Epoch: 55, Loss: 0.8306, Validation Loss: 0.8042 \n",
            "Attention Heads: 4, Epoch: 56, Loss: 0.8196, Validation Loss: 0.8043 \n",
            "Early stopping at epoch 57 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 0.8044 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 3.8692, Validation Loss: 3.3183 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 3.3333, Validation Loss: 2.8363 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 2.8997, Validation Loss: 2.4373 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 2.5497, Validation Loss: 2.1232 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 2.2754, Validation Loss: 1.8825 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 2.1007, Validation Loss: 1.6982 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 1.9591, Validation Loss: 1.5555 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 1.7911, Validation Loss: 1.4416 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 1.6950, Validation Loss: 1.3493 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 1.6082, Validation Loss: 1.2726 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 1.5025, Validation Loss: 1.2083 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 1.4504, Validation Loss: 1.1542 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 1.3801, Validation Loss: 1.1084 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 1.3348, Validation Loss: 1.0696 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 1.3029, Validation Loss: 1.0367 \n",
            "Attention Heads: 8, Epoch: 15, Loss: 1.2615, Validation Loss: 1.0088 \n",
            "Attention Heads: 8, Epoch: 16, Loss: 1.2254, Validation Loss: 0.9852 \n",
            "Attention Heads: 8, Epoch: 17, Loss: 1.2279, Validation Loss: 0.9653 \n",
            "Attention Heads: 8, Epoch: 18, Loss: 1.1906, Validation Loss: 0.9479 \n",
            "Attention Heads: 8, Epoch: 19, Loss: 1.1946, Validation Loss: 0.9331 \n",
            "Attention Heads: 8, Epoch: 20, Loss: 1.1429, Validation Loss: 0.9204 \n",
            "Attention Heads: 8, Epoch: 21, Loss: 1.1174, Validation Loss: 0.9093 \n",
            "Attention Heads: 8, Epoch: 22, Loss: 1.1152, Validation Loss: 0.8994 \n",
            "Attention Heads: 8, Epoch: 23, Loss: 1.1024, Validation Loss: 0.8908 \n",
            "Attention Heads: 8, Epoch: 24, Loss: 1.0888, Validation Loss: 0.8835 \n",
            "Attention Heads: 8, Epoch: 25, Loss: 1.0543, Validation Loss: 0.8770 \n",
            "Attention Heads: 8, Epoch: 26, Loss: 1.0897, Validation Loss: 0.8713 \n",
            "Attention Heads: 8, Epoch: 27, Loss: 1.0631, Validation Loss: 0.8660 \n",
            "Attention Heads: 8, Epoch: 28, Loss: 1.0633, Validation Loss: 0.8614 \n",
            "Attention Heads: 8, Epoch: 29, Loss: 1.0384, Validation Loss: 0.8574 \n",
            "Attention Heads: 8, Epoch: 30, Loss: 1.0153, Validation Loss: 0.8540 \n",
            "Attention Heads: 8, Epoch: 31, Loss: 1.0312, Validation Loss: 0.8509 \n",
            "Attention Heads: 8, Epoch: 32, Loss: 0.9922, Validation Loss: 0.8482 \n",
            "Attention Heads: 8, Epoch: 33, Loss: 1.0031, Validation Loss: 0.8456 \n",
            "Attention Heads: 8, Epoch: 34, Loss: 1.0086, Validation Loss: 0.8432 \n",
            "Attention Heads: 8, Epoch: 35, Loss: 0.9798, Validation Loss: 0.8411 \n",
            "Attention Heads: 8, Epoch: 36, Loss: 0.9749, Validation Loss: 0.8392 \n",
            "Attention Heads: 8, Epoch: 37, Loss: 1.0310, Validation Loss: 0.8376 \n",
            "Attention Heads: 8, Epoch: 38, Loss: 0.9953, Validation Loss: 0.8364 \n",
            "Attention Heads: 8, Epoch: 39, Loss: 0.9944, Validation Loss: 0.8353 \n",
            "Attention Heads: 8, Epoch: 40, Loss: 0.9591, Validation Loss: 0.8345 \n",
            "Attention Heads: 8, Epoch: 41, Loss: 0.9352, Validation Loss: 0.8338 \n",
            "Attention Heads: 8, Epoch: 42, Loss: 0.9630, Validation Loss: 0.8330 \n",
            "Attention Heads: 8, Epoch: 43, Loss: 0.9556, Validation Loss: 0.8323 \n",
            "Attention Heads: 8, Epoch: 44, Loss: 0.9686, Validation Loss: 0.8317 \n",
            "Attention Heads: 8, Epoch: 45, Loss: 1.0042, Validation Loss: 0.8311 \n",
            "Attention Heads: 8, Epoch: 46, Loss: 0.9462, Validation Loss: 0.8307 \n",
            "Attention Heads: 8, Epoch: 47, Loss: 0.9468, Validation Loss: 0.8305 \n",
            "Attention Heads: 8, Epoch: 48, Loss: 0.9284, Validation Loss: 0.8304 \n",
            "Attention Heads: 8, Epoch: 49, Loss: 0.9502, Validation Loss: 0.8304 \n",
            "Early stopping at epoch 50 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 0.8306 \n",
            "\n",
            "Best Number of Attention Heads: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = OneLayerGAT_v2(input_dim=citeseer.num_node_features, output_dim=citeseer_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, citeseer)\n",
        "    val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, citeseer)\n",
        "print(f'Test Accuracy(one layer) on citeseer dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtffybfXUjx0",
        "outputId": "edbda530-5bf5-4227-eacc-c9c9dea7a3e8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 53 \n",
            "\n",
            "Test Accuracy(one layer) on citeseer dataset: 0.7414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Three layer GAT V2**"
      ],
      "metadata": {
        "id": "tS1CLFR3Ngd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATv2Conv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ThreeLayerGAT_v2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, dropout=0.6, negative_slope=0.2 , share_weights = False):\n",
        "        super(ThreeLayerGAT_v2, self).__init__()\n",
        "        self.gat1 = GATv2Conv(input_dim, hidden_dim, heads=num_heads, dropout=dropout, concat=True, negative_slope=negative_slope ,share_weights = share_weights)\n",
        "        self.gat2 = GATv2Conv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout, concat=True, negative_slope=negative_slope ,share_weights = share_weights)\n",
        "        self.gat3 = GATv2Conv(hidden_dim * num_heads, output_dim, heads=1, dropout=dropout, negative_slope=negative_slope ,share_weights = share_weights)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.gat2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.gat3(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "J7ogpjPoNgq4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Corafull dataset(three layer)**"
      ],
      "metadata": {
        "id": "zT_uXsLQ3CCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "hidden_dims = [16 , 32, 64, 128, 256]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    model = ThreeLayerGAT_v2(input_dim=CoraFull.num_node_features, hidden_dim= hidden_dim , output_dim=CoraFull_dataset.num_classes , num_heads = 4).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "        print(f'Hidden Dimension: {hidden_dim}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Hidden Dimension: {hidden_dim}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_hidden_dim = hidden_dim\n",
        "\n",
        "print('Best Hidden Dimension: {}'.format(best_hidden_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aaaf0d5-a1b7-48f1-90ef-8eab9e0f0509",
        "id": "B_5HJG6m3CCZ"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 16, Epoch: 0, Loss: 4.2539, Validation Loss: 4.1139 \n",
            "Hidden Dimension: 16, Epoch: 1, Loss: 4.1296, Validation Loss: 3.8635 \n",
            "Hidden Dimension: 16, Epoch: 2, Loss: 3.9587, Validation Loss: 3.6388 \n",
            "Hidden Dimension: 16, Epoch: 3, Loss: 3.8032, Validation Loss: 3.4437 \n",
            "Hidden Dimension: 16, Epoch: 4, Loss: 3.6337, Validation Loss: 3.2344 \n",
            "Hidden Dimension: 16, Epoch: 5, Loss: 3.4698, Validation Loss: 3.0136 \n",
            "Hidden Dimension: 16, Epoch: 6, Loss: 3.2952, Validation Loss: 2.7770 \n",
            "Hidden Dimension: 16, Epoch: 7, Loss: 3.1333, Validation Loss: 2.5796 \n",
            "Hidden Dimension: 16, Epoch: 8, Loss: 2.9944, Validation Loss: 2.4109 \n",
            "Hidden Dimension: 16, Epoch: 9, Loss: 2.9150, Validation Loss: 2.2829 \n",
            "Hidden Dimension: 16, Epoch: 10, Loss: 2.7563, Validation Loss: 2.1444 \n",
            "Hidden Dimension: 16, Epoch: 11, Loss: 2.6510, Validation Loss: 1.9985 \n",
            "Hidden Dimension: 16, Epoch: 12, Loss: 2.5571, Validation Loss: 1.8635 \n",
            "Hidden Dimension: 16, Epoch: 13, Loss: 2.4879, Validation Loss: 1.7655 \n",
            "Hidden Dimension: 16, Epoch: 14, Loss: 2.4030, Validation Loss: 1.6873 \n",
            "Hidden Dimension: 16, Epoch: 15, Loss: 2.3320, Validation Loss: 1.6206 \n",
            "Hidden Dimension: 16, Epoch: 16, Loss: 2.2630, Validation Loss: 1.5656 \n",
            "Hidden Dimension: 16, Epoch: 17, Loss: 2.2218, Validation Loss: 1.5173 \n",
            "Hidden Dimension: 16, Epoch: 18, Loss: 2.1849, Validation Loss: 1.4775 \n",
            "Hidden Dimension: 16, Epoch: 19, Loss: 2.1279, Validation Loss: 1.4529 \n",
            "Hidden Dimension: 16, Epoch: 20, Loss: 2.0923, Validation Loss: 1.4267 \n",
            "Hidden Dimension: 16, Epoch: 21, Loss: 2.0624, Validation Loss: 1.4008 \n",
            "Hidden Dimension: 16, Epoch: 22, Loss: 2.0260, Validation Loss: 1.3777 \n",
            "Hidden Dimension: 16, Epoch: 23, Loss: 2.0044, Validation Loss: 1.3489 \n",
            "Hidden Dimension: 16, Epoch: 24, Loss: 1.9664, Validation Loss: 1.3270 \n",
            "Hidden Dimension: 16, Epoch: 25, Loss: 1.9650, Validation Loss: 1.3101 \n",
            "Hidden Dimension: 16, Epoch: 26, Loss: 1.9223, Validation Loss: 1.2947 \n",
            "Hidden Dimension: 16, Epoch: 27, Loss: 1.8992, Validation Loss: 1.2878 \n",
            "Hidden Dimension: 16, Epoch: 28, Loss: 1.8700, Validation Loss: 1.2777 \n",
            "Hidden Dimension: 16, Epoch: 29, Loss: 1.8739, Validation Loss: 1.2626 \n",
            "Hidden Dimension: 16, Epoch: 30, Loss: 1.8557, Validation Loss: 1.2488 \n",
            "Hidden Dimension: 16, Epoch: 31, Loss: 1.8322, Validation Loss: 1.2383 \n",
            "Hidden Dimension: 16, Epoch: 32, Loss: 1.8235, Validation Loss: 1.2307 \n",
            "Hidden Dimension: 16, Epoch: 33, Loss: 1.8078, Validation Loss: 1.2180 \n",
            "Hidden Dimension: 16, Epoch: 34, Loss: 1.7963, Validation Loss: 1.2100 \n",
            "Hidden Dimension: 16, Epoch: 35, Loss: 1.7658, Validation Loss: 1.2004 \n",
            "Hidden Dimension: 16, Epoch: 36, Loss: 1.7681, Validation Loss: 1.1988 \n",
            "Hidden Dimension: 16, Epoch: 37, Loss: 1.7423, Validation Loss: 1.1930 \n",
            "Hidden Dimension: 16, Epoch: 38, Loss: 1.7575, Validation Loss: 1.1891 \n",
            "Hidden Dimension: 16, Epoch: 39, Loss: 1.7569, Validation Loss: 1.1859 \n",
            "Hidden Dimension: 16, Epoch: 40, Loss: 1.7287, Validation Loss: 1.1861 \n",
            "Hidden Dimension: 16, Epoch: 41, Loss: 1.7342, Validation Loss: 1.1870 \n",
            "Hidden Dimension: 16, Epoch: 42, Loss: 1.7300, Validation Loss: 1.1798 \n",
            "Hidden Dimension: 16, Epoch: 43, Loss: 1.7003, Validation Loss: 1.1863 \n",
            "Hidden Dimension: 16, Epoch: 44, Loss: 1.7193, Validation Loss: 1.1790 \n",
            "Hidden Dimension: 16, Epoch: 45, Loss: 1.7049, Validation Loss: 1.1793 \n",
            "Hidden Dimension: 16, Epoch: 46, Loss: 1.6789, Validation Loss: 1.1669 \n",
            "Hidden Dimension: 16, Epoch: 47, Loss: 1.6628, Validation Loss: 1.1654 \n",
            "Hidden Dimension: 16, Epoch: 48, Loss: 1.6504, Validation Loss: 1.1653 \n",
            "Hidden Dimension: 16, Epoch: 49, Loss: 1.6632, Validation Loss: 1.1712 \n",
            "Hidden Dimension: 16, Epoch: 50, Loss: 1.6634, Validation Loss: 1.1650 \n",
            "Hidden Dimension: 16, Epoch: 51, Loss: 1.6505, Validation Loss: 1.1588 \n",
            "Hidden Dimension: 16, Epoch: 52, Loss: 1.6439, Validation Loss: 1.1544 \n",
            "Hidden Dimension: 16, Epoch: 53, Loss: 1.6401, Validation Loss: 1.1504 \n",
            "Hidden Dimension: 16, Epoch: 54, Loss: 1.6580, Validation Loss: 1.1540 \n",
            "Hidden Dimension: 16, Epoch: 55, Loss: 1.6586, Validation Loss: 1.1528 \n",
            "Hidden Dimension: 16, Epoch: 56, Loss: 1.6245, Validation Loss: 1.1580 \n",
            "Hidden Dimension: 16, Epoch: 57, Loss: 1.6054, Validation Loss: 1.1598 \n",
            "Early stopping at epoch 58 \n",
            "\n",
            " Hidden Dimension: 16, Validation Loss: 1.1531 \n",
            "\n",
            "Hidden Dimension: 32, Epoch: 0, Loss: 4.2536, Validation Loss: 4.0094 \n",
            "Hidden Dimension: 32, Epoch: 1, Loss: 4.0427, Validation Loss: 3.6908 \n",
            "Hidden Dimension: 32, Epoch: 2, Loss: 3.8726, Validation Loss: 3.6144 \n",
            "Hidden Dimension: 32, Epoch: 3, Loss: 3.7304, Validation Loss: 3.3888 \n",
            "Hidden Dimension: 32, Epoch: 4, Loss: 3.5286, Validation Loss: 3.0334 \n",
            "Hidden Dimension: 32, Epoch: 5, Loss: 3.2982, Validation Loss: 2.7051 \n",
            "Hidden Dimension: 32, Epoch: 6, Loss: 3.1047, Validation Loss: 2.4967 \n",
            "Hidden Dimension: 32, Epoch: 7, Loss: 2.9343, Validation Loss: 2.3151 \n",
            "Hidden Dimension: 32, Epoch: 8, Loss: 2.7701, Validation Loss: 2.1351 \n",
            "Hidden Dimension: 32, Epoch: 9, Loss: 2.6457, Validation Loss: 1.9749 \n",
            "Hidden Dimension: 32, Epoch: 10, Loss: 2.4969, Validation Loss: 1.8236 \n",
            "Hidden Dimension: 32, Epoch: 11, Loss: 2.4178, Validation Loss: 1.6955 \n",
            "Hidden Dimension: 32, Epoch: 12, Loss: 2.3169, Validation Loss: 1.6080 \n",
            "Hidden Dimension: 32, Epoch: 13, Loss: 2.2610, Validation Loss: 1.5534 \n",
            "Hidden Dimension: 32, Epoch: 14, Loss: 2.1873, Validation Loss: 1.4745 \n",
            "Hidden Dimension: 32, Epoch: 15, Loss: 2.1088, Validation Loss: 1.4164 \n",
            "Hidden Dimension: 32, Epoch: 16, Loss: 2.0642, Validation Loss: 1.3853 \n",
            "Hidden Dimension: 32, Epoch: 17, Loss: 2.0112, Validation Loss: 1.3579 \n",
            "Hidden Dimension: 32, Epoch: 18, Loss: 1.9777, Validation Loss: 1.3141 \n",
            "Hidden Dimension: 32, Epoch: 19, Loss: 1.9251, Validation Loss: 1.2788 \n",
            "Hidden Dimension: 32, Epoch: 20, Loss: 1.8890, Validation Loss: 1.2550 \n",
            "Hidden Dimension: 32, Epoch: 21, Loss: 1.8618, Validation Loss: 1.2407 \n",
            "Hidden Dimension: 32, Epoch: 22, Loss: 1.8080, Validation Loss: 1.2343 \n",
            "Hidden Dimension: 32, Epoch: 23, Loss: 1.8150, Validation Loss: 1.2138 \n",
            "Hidden Dimension: 32, Epoch: 24, Loss: 1.7684, Validation Loss: 1.2005 \n",
            "Hidden Dimension: 32, Epoch: 25, Loss: 1.7731, Validation Loss: 1.1952 \n",
            "Hidden Dimension: 32, Epoch: 26, Loss: 1.7322, Validation Loss: 1.1944 \n",
            "Hidden Dimension: 32, Epoch: 27, Loss: 1.7387, Validation Loss: 1.1855 \n",
            "Hidden Dimension: 32, Epoch: 28, Loss: 1.7120, Validation Loss: 1.1705 \n",
            "Hidden Dimension: 32, Epoch: 29, Loss: 1.6912, Validation Loss: 1.1653 \n",
            "Hidden Dimension: 32, Epoch: 30, Loss: 1.6885, Validation Loss: 1.1594 \n",
            "Hidden Dimension: 32, Epoch: 31, Loss: 1.6782, Validation Loss: 1.1566 \n",
            "Hidden Dimension: 32, Epoch: 32, Loss: 1.6531, Validation Loss: 1.1542 \n",
            "Hidden Dimension: 32, Epoch: 33, Loss: 1.6366, Validation Loss: 1.1501 \n",
            "Hidden Dimension: 32, Epoch: 34, Loss: 1.6255, Validation Loss: 1.1426 \n",
            "Hidden Dimension: 32, Epoch: 35, Loss: 1.6259, Validation Loss: 1.1391 \n",
            "Hidden Dimension: 32, Epoch: 36, Loss: 1.6123, Validation Loss: 1.1447 \n",
            "Hidden Dimension: 32, Epoch: 37, Loss: 1.5974, Validation Loss: 1.1386 \n",
            "Hidden Dimension: 32, Epoch: 38, Loss: 1.5804, Validation Loss: 1.1481 \n",
            "Hidden Dimension: 32, Epoch: 39, Loss: 1.5989, Validation Loss: 1.1359 \n",
            "Hidden Dimension: 32, Epoch: 40, Loss: 1.5839, Validation Loss: 1.1271 \n",
            "Hidden Dimension: 32, Epoch: 41, Loss: 1.5865, Validation Loss: 1.1489 \n",
            "Hidden Dimension: 32, Epoch: 42, Loss: 1.5833, Validation Loss: 1.1240 \n",
            "Hidden Dimension: 32, Epoch: 43, Loss: 1.5418, Validation Loss: 1.1212 \n",
            "Hidden Dimension: 32, Epoch: 44, Loss: 1.5544, Validation Loss: 1.1225 \n",
            "Hidden Dimension: 32, Epoch: 45, Loss: 1.5501, Validation Loss: 1.1239 \n",
            "Hidden Dimension: 32, Epoch: 46, Loss: 1.5353, Validation Loss: 1.1107 \n",
            "Hidden Dimension: 32, Epoch: 47, Loss: 1.5398, Validation Loss: 1.0965 \n",
            "Hidden Dimension: 32, Epoch: 48, Loss: 1.5075, Validation Loss: 1.1133 \n",
            "Hidden Dimension: 32, Epoch: 49, Loss: 1.5189, Validation Loss: 1.1196 \n",
            "Hidden Dimension: 32, Epoch: 50, Loss: 1.5033, Validation Loss: 1.1090 \n",
            "Hidden Dimension: 32, Epoch: 51, Loss: 1.5059, Validation Loss: 1.1118 \n",
            "Early stopping at epoch 52 \n",
            "\n",
            " Hidden Dimension: 32, Validation Loss: 1.1106 \n",
            "\n",
            "Hidden Dimension: 64, Epoch: 0, Loss: 4.2529, Validation Loss: 3.9155 \n",
            "Hidden Dimension: 64, Epoch: 1, Loss: 4.0722, Validation Loss: 3.8335 \n",
            "Hidden Dimension: 64, Epoch: 2, Loss: 3.8764, Validation Loss: 3.5070 \n",
            "Hidden Dimension: 64, Epoch: 3, Loss: 3.6537, Validation Loss: 3.0926 \n",
            "Hidden Dimension: 64, Epoch: 4, Loss: 3.3780, Validation Loss: 2.7900 \n",
            "Hidden Dimension: 64, Epoch: 5, Loss: 3.1150, Validation Loss: 2.4248 \n",
            "Hidden Dimension: 64, Epoch: 6, Loss: 2.8862, Validation Loss: 2.2317 \n",
            "Hidden Dimension: 64, Epoch: 7, Loss: 2.7335, Validation Loss: 2.0330 \n",
            "Hidden Dimension: 64, Epoch: 8, Loss: 2.5606, Validation Loss: 1.8474 \n",
            "Hidden Dimension: 64, Epoch: 9, Loss: 2.4139, Validation Loss: 1.7063 \n",
            "Hidden Dimension: 64, Epoch: 10, Loss: 2.3025, Validation Loss: 1.5809 \n",
            "Hidden Dimension: 64, Epoch: 11, Loss: 2.2124, Validation Loss: 1.4945 \n",
            "Hidden Dimension: 64, Epoch: 12, Loss: 2.1290, Validation Loss: 1.4447 \n",
            "Hidden Dimension: 64, Epoch: 13, Loss: 2.0384, Validation Loss: 1.3968 \n",
            "Hidden Dimension: 64, Epoch: 14, Loss: 1.9778, Validation Loss: 1.3398 \n",
            "Hidden Dimension: 64, Epoch: 15, Loss: 1.9132, Validation Loss: 1.3055 \n",
            "Hidden Dimension: 64, Epoch: 16, Loss: 1.8673, Validation Loss: 1.2799 \n",
            "Hidden Dimension: 64, Epoch: 17, Loss: 1.8446, Validation Loss: 1.2499 \n",
            "Hidden Dimension: 64, Epoch: 18, Loss: 1.8011, Validation Loss: 1.2291 \n",
            "Hidden Dimension: 64, Epoch: 19, Loss: 1.7334, Validation Loss: 1.2129 \n",
            "Hidden Dimension: 64, Epoch: 20, Loss: 1.7251, Validation Loss: 1.2057 \n",
            "Hidden Dimension: 64, Epoch: 21, Loss: 1.7164, Validation Loss: 1.1949 \n",
            "Hidden Dimension: 64, Epoch: 22, Loss: 1.7067, Validation Loss: 1.1779 \n",
            "Hidden Dimension: 64, Epoch: 23, Loss: 1.6391, Validation Loss: 1.1649 \n",
            "Hidden Dimension: 64, Epoch: 24, Loss: 1.6645, Validation Loss: 1.1602 \n",
            "Hidden Dimension: 64, Epoch: 25, Loss: 1.6119, Validation Loss: 1.1458 \n",
            "Hidden Dimension: 64, Epoch: 26, Loss: 1.6022, Validation Loss: 1.1472 \n",
            "Hidden Dimension: 64, Epoch: 27, Loss: 1.5841, Validation Loss: 1.1477 \n",
            "Hidden Dimension: 64, Epoch: 28, Loss: 1.5661, Validation Loss: 1.1447 \n",
            "Hidden Dimension: 64, Epoch: 29, Loss: 1.5656, Validation Loss: 1.1363 \n",
            "Hidden Dimension: 64, Epoch: 30, Loss: 1.5443, Validation Loss: 1.1188 \n",
            "Hidden Dimension: 64, Epoch: 31, Loss: 1.5348, Validation Loss: 1.1347 \n",
            "Hidden Dimension: 64, Epoch: 32, Loss: 1.5259, Validation Loss: 1.1314 \n",
            "Hidden Dimension: 64, Epoch: 33, Loss: 1.5353, Validation Loss: 1.1263 \n",
            "Hidden Dimension: 64, Epoch: 34, Loss: 1.5184, Validation Loss: 1.1333 \n",
            "Hidden Dimension: 64, Epoch: 35, Loss: 1.5033, Validation Loss: 1.1129 \n",
            "Hidden Dimension: 64, Epoch: 36, Loss: 1.4730, Validation Loss: 1.1152 \n",
            "Hidden Dimension: 64, Epoch: 37, Loss: 1.5002, Validation Loss: 1.1207 \n",
            "Hidden Dimension: 64, Epoch: 38, Loss: 1.5082, Validation Loss: 1.1427 \n",
            "Hidden Dimension: 64, Epoch: 39, Loss: 1.4838, Validation Loss: 1.1515 \n",
            "Early stopping at epoch 40 \n",
            "\n",
            " Hidden Dimension: 64, Validation Loss: 1.1371 \n",
            "\n",
            "Hidden Dimension: 128, Epoch: 0, Loss: 4.2577, Validation Loss: 4.4224 \n",
            "Hidden Dimension: 128, Epoch: 1, Loss: 4.9312, Validation Loss: 3.9851 \n",
            "Hidden Dimension: 128, Epoch: 2, Loss: 4.0180, Validation Loss: 3.8612 \n",
            "Hidden Dimension: 128, Epoch: 3, Loss: 3.9445, Validation Loss: 3.5883 \n",
            "Hidden Dimension: 128, Epoch: 4, Loss: 3.7696, Validation Loss: 3.3001 \n",
            "Hidden Dimension: 128, Epoch: 5, Loss: 3.5129, Validation Loss: 2.9421 \n",
            "Hidden Dimension: 128, Epoch: 6, Loss: 3.2025, Validation Loss: 2.5735 \n",
            "Hidden Dimension: 128, Epoch: 7, Loss: 2.9916, Validation Loss: 2.4273 \n",
            "Hidden Dimension: 128, Epoch: 8, Loss: 2.8647, Validation Loss: 2.2222 \n",
            "Hidden Dimension: 128, Epoch: 9, Loss: 2.6522, Validation Loss: 2.0283 \n",
            "Hidden Dimension: 128, Epoch: 10, Loss: 2.5256, Validation Loss: 1.8688 \n",
            "Hidden Dimension: 128, Epoch: 11, Loss: 2.4042, Validation Loss: 1.7549 \n",
            "Hidden Dimension: 128, Epoch: 12, Loss: 2.2873, Validation Loss: 1.6742 \n",
            "Hidden Dimension: 128, Epoch: 13, Loss: 2.2185, Validation Loss: 1.5657 \n",
            "Hidden Dimension: 128, Epoch: 14, Loss: 2.1216, Validation Loss: 1.4989 \n",
            "Hidden Dimension: 128, Epoch: 15, Loss: 2.0730, Validation Loss: 1.4515 \n",
            "Hidden Dimension: 128, Epoch: 16, Loss: 2.0106, Validation Loss: 1.4104 \n",
            "Hidden Dimension: 128, Epoch: 17, Loss: 1.9291, Validation Loss: 1.3820 \n",
            "Hidden Dimension: 128, Epoch: 18, Loss: 1.9030, Validation Loss: 1.3507 \n",
            "Hidden Dimension: 128, Epoch: 19, Loss: 1.8739, Validation Loss: 1.3078 \n",
            "Hidden Dimension: 128, Epoch: 20, Loss: 1.8501, Validation Loss: 1.2709 \n",
            "Hidden Dimension: 128, Epoch: 21, Loss: 1.7956, Validation Loss: 1.2566 \n",
            "Hidden Dimension: 128, Epoch: 22, Loss: 1.7525, Validation Loss: 1.2477 \n",
            "Hidden Dimension: 128, Epoch: 23, Loss: 1.7329, Validation Loss: 1.2338 \n",
            "Hidden Dimension: 128, Epoch: 24, Loss: 1.6953, Validation Loss: 1.2209 \n",
            "Hidden Dimension: 128, Epoch: 25, Loss: 1.6949, Validation Loss: 1.2070 \n",
            "Hidden Dimension: 128, Epoch: 26, Loss: 1.6619, Validation Loss: 1.1978 \n",
            "Hidden Dimension: 128, Epoch: 27, Loss: 1.6380, Validation Loss: 1.1833 \n",
            "Hidden Dimension: 128, Epoch: 28, Loss: 1.6251, Validation Loss: 1.1723 \n",
            "Hidden Dimension: 128, Epoch: 29, Loss: 1.6133, Validation Loss: 1.1503 \n",
            "Hidden Dimension: 128, Epoch: 30, Loss: 1.5977, Validation Loss: 1.1416 \n",
            "Hidden Dimension: 128, Epoch: 31, Loss: 1.5618, Validation Loss: 1.1483 \n",
            "Hidden Dimension: 128, Epoch: 32, Loss: 1.5704, Validation Loss: 1.1893 \n",
            "Hidden Dimension: 128, Epoch: 33, Loss: 1.5642, Validation Loss: 1.1577 \n",
            "Hidden Dimension: 128, Epoch: 34, Loss: 1.5256, Validation Loss: 1.1360 \n",
            "Hidden Dimension: 128, Epoch: 35, Loss: 1.5431, Validation Loss: 1.1475 \n",
            "Hidden Dimension: 128, Epoch: 36, Loss: 1.5155, Validation Loss: 1.1411 \n",
            "Hidden Dimension: 128, Epoch: 37, Loss: 1.5175, Validation Loss: 1.1414 \n",
            "Hidden Dimension: 128, Epoch: 38, Loss: 1.5230, Validation Loss: 1.1579 \n",
            "Early stopping at epoch 39 \n",
            "\n",
            " Hidden Dimension: 128, Validation Loss: 1.1492 \n",
            "\n",
            "Hidden Dimension: 256, Epoch: 0, Loss: 4.2578, Validation Loss: 8.6144 \n",
            "Hidden Dimension: 256, Epoch: 1, Loss: 11.9426, Validation Loss: 4.0381 \n",
            "Hidden Dimension: 256, Epoch: 2, Loss: 4.1240, Validation Loss: 4.0124 \n",
            "Hidden Dimension: 256, Epoch: 3, Loss: 4.0625, Validation Loss: 3.8519 \n",
            "Hidden Dimension: 256, Epoch: 4, Loss: 3.9629, Validation Loss: 3.6356 \n",
            "Hidden Dimension: 256, Epoch: 5, Loss: 3.7485, Validation Loss: 3.2410 \n",
            "Hidden Dimension: 256, Epoch: 6, Loss: 3.5206, Validation Loss: 3.0736 \n",
            "Hidden Dimension: 256, Epoch: 7, Loss: 3.4260, Validation Loss: 2.8632 \n",
            "Hidden Dimension: 256, Epoch: 8, Loss: 3.1387, Validation Loss: 2.6038 \n",
            "Hidden Dimension: 256, Epoch: 9, Loss: 2.9818, Validation Loss: 2.4080 \n",
            "Hidden Dimension: 256, Epoch: 10, Loss: 2.8355, Validation Loss: 2.2053 \n",
            "Hidden Dimension: 256, Epoch: 11, Loss: 2.7086, Validation Loss: 2.0162 \n",
            "Hidden Dimension: 256, Epoch: 12, Loss: 2.5629, Validation Loss: 1.9051 \n",
            "Hidden Dimension: 256, Epoch: 13, Loss: 2.4727, Validation Loss: 1.8052 \n",
            "Hidden Dimension: 256, Epoch: 14, Loss: 2.4108, Validation Loss: 1.7542 \n",
            "Hidden Dimension: 256, Epoch: 15, Loss: 2.2930, Validation Loss: 1.6874 \n",
            "Hidden Dimension: 256, Epoch: 16, Loss: 2.2247, Validation Loss: 1.6376 \n",
            "Hidden Dimension: 256, Epoch: 17, Loss: 2.1844, Validation Loss: 1.5582 \n",
            "Hidden Dimension: 256, Epoch: 18, Loss: 2.0924, Validation Loss: 1.4921 \n",
            "Hidden Dimension: 256, Epoch: 19, Loss: 2.0454, Validation Loss: 1.4388 \n",
            "Hidden Dimension: 256, Epoch: 20, Loss: 1.9985, Validation Loss: 1.3948 \n",
            "Hidden Dimension: 256, Epoch: 21, Loss: 1.9404, Validation Loss: 1.3842 \n",
            "Hidden Dimension: 256, Epoch: 22, Loss: 1.8979, Validation Loss: 1.3423 \n",
            "Hidden Dimension: 256, Epoch: 23, Loss: 1.8459, Validation Loss: 1.3226 \n",
            "Hidden Dimension: 256, Epoch: 24, Loss: 1.8324, Validation Loss: 1.3078 \n",
            "Hidden Dimension: 256, Epoch: 25, Loss: 1.7845, Validation Loss: 1.2782 \n",
            "Hidden Dimension: 256, Epoch: 26, Loss: 1.7618, Validation Loss: 1.2720 \n",
            "Hidden Dimension: 256, Epoch: 27, Loss: 1.7034, Validation Loss: 1.2558 \n",
            "Hidden Dimension: 256, Epoch: 28, Loss: 1.6723, Validation Loss: 1.2388 \n",
            "Hidden Dimension: 256, Epoch: 29, Loss: 1.6962, Validation Loss: 1.2207 \n",
            "Hidden Dimension: 256, Epoch: 30, Loss: 1.6813, Validation Loss: 1.2233 \n",
            "Hidden Dimension: 256, Epoch: 31, Loss: 1.6534, Validation Loss: 1.2164 \n",
            "Hidden Dimension: 256, Epoch: 32, Loss: 1.6170, Validation Loss: 1.2034 \n",
            "Hidden Dimension: 256, Epoch: 33, Loss: 1.6058, Validation Loss: 1.2020 \n",
            "Hidden Dimension: 256, Epoch: 34, Loss: 1.5952, Validation Loss: 1.2036 \n",
            "Hidden Dimension: 256, Epoch: 35, Loss: 1.5700, Validation Loss: 1.1753 \n",
            "Hidden Dimension: 256, Epoch: 36, Loss: 1.5730, Validation Loss: 1.1712 \n",
            "Hidden Dimension: 256, Epoch: 37, Loss: 1.5468, Validation Loss: 1.1728 \n",
            "Hidden Dimension: 256, Epoch: 38, Loss: 1.5382, Validation Loss: 1.1640 \n",
            "Hidden Dimension: 256, Epoch: 39, Loss: 1.5129, Validation Loss: 1.1674 \n",
            "Hidden Dimension: 256, Epoch: 40, Loss: 1.5205, Validation Loss: 1.1449 \n",
            "Hidden Dimension: 256, Epoch: 41, Loss: 1.5046, Validation Loss: 1.1636 \n",
            "Hidden Dimension: 256, Epoch: 42, Loss: 1.5202, Validation Loss: 1.1873 \n",
            "Hidden Dimension: 256, Epoch: 43, Loss: 1.5082, Validation Loss: 1.1717 \n",
            "Hidden Dimension: 256, Epoch: 44, Loss: 1.4947, Validation Loss: 1.1591 \n",
            "Early stopping at epoch 45 \n",
            "\n",
            " Hidden Dimension: 256, Validation Loss: 1.1665 \n",
            "\n",
            "Best Hidden Dimension: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  cora full dataset(three layer)**"
      ],
      "metadata": {
        "id": "fz8ZHRwbWLz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = ThreeLayerGAT_v2(input_dim= CoraFull.num_node_features ,hidden_dim=best_hidden_dim , output_dim= CoraFull_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, CoraFull)\n",
        "        val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05f3531-ff85-42d0-acf1-a4c3c8dbcd13",
        "id": "v-FS6TEgWLz7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 4.2597, Validation Loss: 4.1553 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 4.1931, Validation Loss: 4.0958 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 4.1233, Validation Loss: 3.9845 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 4.0446, Validation Loss: 3.8314 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 3.9572, Validation Loss: 3.6874 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 3.9724, Validation Loss: 3.6211 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 3.8283, Validation Loss: 3.5761 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 3.7966, Validation Loss: 3.5179 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 3.7632, Validation Loss: 3.4421 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 3.6884, Validation Loss: 3.3614 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 3.6250, Validation Loss: 3.2808 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 3.5758, Validation Loss: 3.2174 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 3.5227, Validation Loss: 3.1698 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 3.4461, Validation Loss: 3.1151 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 3.3723, Validation Loss: 3.0344 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 3.3286, Validation Loss: 2.9425 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 3.2602, Validation Loss: 2.8497 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 3.1944, Validation Loss: 2.7796 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 3.1490, Validation Loss: 2.7234 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 3.1018, Validation Loss: 2.6447 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 3.0484, Validation Loss: 2.5566 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 2.9951, Validation Loss: 2.4702 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 2.9568, Validation Loss: 2.4031 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 2.9258, Validation Loss: 2.3494 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 2.8722, Validation Loss: 2.2837 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 2.8027, Validation Loss: 2.2073 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 2.7970, Validation Loss: 2.1690 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 2.7706, Validation Loss: 2.1331 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 2.7227, Validation Loss: 2.0881 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 2.7196, Validation Loss: 2.0407 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 2.6354, Validation Loss: 1.9997 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 2.6397, Validation Loss: 1.9706 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 2.6105, Validation Loss: 1.9417 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 2.5793, Validation Loss: 1.9033 \n",
            "Attention Heads: 1, Epoch: 34, Loss: 2.5594, Validation Loss: 1.8637 \n",
            "Attention Heads: 1, Epoch: 35, Loss: 2.5561, Validation Loss: 1.8275 \n",
            "Attention Heads: 1, Epoch: 36, Loss: 2.4927, Validation Loss: 1.7983 \n",
            "Attention Heads: 1, Epoch: 37, Loss: 2.4960, Validation Loss: 1.7789 \n",
            "Attention Heads: 1, Epoch: 38, Loss: 2.4685, Validation Loss: 1.7523 \n",
            "Attention Heads: 1, Epoch: 39, Loss: 2.4664, Validation Loss: 1.7177 \n",
            "Attention Heads: 1, Epoch: 40, Loss: 2.4220, Validation Loss: 1.6774 \n",
            "Attention Heads: 1, Epoch: 41, Loss: 2.4219, Validation Loss: 1.6471 \n",
            "Attention Heads: 1, Epoch: 42, Loss: 2.3823, Validation Loss: 1.6306 \n",
            "Attention Heads: 1, Epoch: 43, Loss: 2.3580, Validation Loss: 1.6132 \n",
            "Attention Heads: 1, Epoch: 44, Loss: 2.3638, Validation Loss: 1.5943 \n",
            "Attention Heads: 1, Epoch: 45, Loss: 2.3366, Validation Loss: 1.5728 \n",
            "Attention Heads: 1, Epoch: 46, Loss: 2.3176, Validation Loss: 1.5552 \n",
            "Attention Heads: 1, Epoch: 47, Loss: 2.3301, Validation Loss: 1.5387 \n",
            "Attention Heads: 1, Epoch: 48, Loss: 2.2948, Validation Loss: 1.5231 \n",
            "Attention Heads: 1, Epoch: 49, Loss: 2.2771, Validation Loss: 1.5185 \n",
            "Attention Heads: 1, Epoch: 50, Loss: 2.2844, Validation Loss: 1.5162 \n",
            "Attention Heads: 1, Epoch: 51, Loss: 2.3041, Validation Loss: 1.5119 \n",
            "Attention Heads: 1, Epoch: 52, Loss: 2.2643, Validation Loss: 1.5006 \n",
            "Attention Heads: 1, Epoch: 53, Loss: 2.2572, Validation Loss: 1.4903 \n",
            "Attention Heads: 1, Epoch: 54, Loss: 2.2674, Validation Loss: 1.4796 \n",
            "Attention Heads: 1, Epoch: 55, Loss: 2.2147, Validation Loss: 1.4657 \n",
            "Attention Heads: 1, Epoch: 56, Loss: 2.2232, Validation Loss: 1.4462 \n",
            "Attention Heads: 1, Epoch: 57, Loss: 2.2135, Validation Loss: 1.4343 \n",
            "Attention Heads: 1, Epoch: 58, Loss: 2.2441, Validation Loss: 1.4293 \n",
            "Attention Heads: 1, Epoch: 59, Loss: 2.2129, Validation Loss: 1.4310 \n",
            "Attention Heads: 1, Epoch: 60, Loss: 2.1959, Validation Loss: 1.4314 \n",
            "Attention Heads: 1, Epoch: 61, Loss: 2.2024, Validation Loss: 1.4238 \n",
            "Attention Heads: 1, Epoch: 62, Loss: 2.1762, Validation Loss: 1.4178 \n",
            "Attention Heads: 1, Epoch: 63, Loss: 2.1891, Validation Loss: 1.4096 \n",
            "Attention Heads: 1, Epoch: 64, Loss: 2.1800, Validation Loss: 1.4053 \n",
            "Attention Heads: 1, Epoch: 65, Loss: 2.1428, Validation Loss: 1.4016 \n",
            "Attention Heads: 1, Epoch: 66, Loss: 2.1485, Validation Loss: 1.3930 \n",
            "Attention Heads: 1, Epoch: 67, Loss: 2.1515, Validation Loss: 1.3846 \n",
            "Attention Heads: 1, Epoch: 68, Loss: 2.1628, Validation Loss: 1.3788 \n",
            "Attention Heads: 1, Epoch: 69, Loss: 2.1619, Validation Loss: 1.3788 \n",
            "Attention Heads: 1, Epoch: 70, Loss: 2.1401, Validation Loss: 1.3747 \n",
            "Attention Heads: 1, Epoch: 71, Loss: 2.1378, Validation Loss: 1.3677 \n",
            "Attention Heads: 1, Epoch: 72, Loss: 2.1031, Validation Loss: 1.3576 \n",
            "Attention Heads: 1, Epoch: 73, Loss: 2.1185, Validation Loss: 1.3553 \n",
            "Attention Heads: 1, Epoch: 74, Loss: 2.1116, Validation Loss: 1.3594 \n",
            "Attention Heads: 1, Epoch: 75, Loss: 2.1120, Validation Loss: 1.3533 \n",
            "Attention Heads: 1, Epoch: 76, Loss: 2.0873, Validation Loss: 1.3461 \n",
            "Attention Heads: 1, Epoch: 77, Loss: 2.0941, Validation Loss: 1.3424 \n",
            "Attention Heads: 1, Epoch: 78, Loss: 2.0943, Validation Loss: 1.3334 \n",
            "Attention Heads: 1, Epoch: 79, Loss: 2.0822, Validation Loss: 1.3298 \n",
            "Attention Heads: 1, Epoch: 80, Loss: 2.0708, Validation Loss: 1.3287 \n",
            "Attention Heads: 1, Epoch: 81, Loss: 2.0960, Validation Loss: 1.3276 \n",
            "Attention Heads: 1, Epoch: 82, Loss: 2.1117, Validation Loss: 1.3234 \n",
            "Attention Heads: 1, Epoch: 83, Loss: 2.0589, Validation Loss: 1.3189 \n",
            "Attention Heads: 1, Epoch: 84, Loss: 2.0706, Validation Loss: 1.3164 \n",
            "Attention Heads: 1, Epoch: 85, Loss: 2.0835, Validation Loss: 1.3103 \n",
            "Attention Heads: 1, Epoch: 86, Loss: 2.0661, Validation Loss: 1.3051 \n",
            "Attention Heads: 1, Epoch: 87, Loss: 2.0456, Validation Loss: 1.2996 \n",
            "Attention Heads: 1, Epoch: 88, Loss: 2.0582, Validation Loss: 1.2962 \n",
            "Attention Heads: 1, Epoch: 89, Loss: 2.0555, Validation Loss: 1.2998 \n",
            "Attention Heads: 1, Epoch: 90, Loss: 2.0536, Validation Loss: 1.2975 \n",
            "Attention Heads: 1, Epoch: 91, Loss: 2.0289, Validation Loss: 1.2958 \n",
            "Attention Heads: 1, Epoch: 92, Loss: 2.0554, Validation Loss: 1.2960 \n",
            "Early stopping at epoch 93 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 1.2957 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 4.2458, Validation Loss: 4.1070 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 4.1177, Validation Loss: 3.8766 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 3.9799, Validation Loss: 3.6990 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 3.8165, Validation Loss: 3.5012 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 3.6501, Validation Loss: 3.2721 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 3.5111, Validation Loss: 3.1200 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 3.3856, Validation Loss: 2.9660 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 3.2554, Validation Loss: 2.8126 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 3.1387, Validation Loss: 2.6486 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 3.0450, Validation Loss: 2.4950 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 2.9455, Validation Loss: 2.3629 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 2.8306, Validation Loss: 2.2346 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 2.7440, Validation Loss: 2.1301 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 2.6820, Validation Loss: 2.0412 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 2.5919, Validation Loss: 1.9411 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 2.5276, Validation Loss: 1.8498 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 2.4830, Validation Loss: 1.7708 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 2.4218, Validation Loss: 1.7023 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 2.3562, Validation Loss: 1.6521 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 2.3339, Validation Loss: 1.6020 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 2.2789, Validation Loss: 1.5612 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 2.2381, Validation Loss: 1.5235 \n",
            "Attention Heads: 2, Epoch: 22, Loss: 2.2198, Validation Loss: 1.4916 \n",
            "Attention Heads: 2, Epoch: 23, Loss: 2.1742, Validation Loss: 1.4710 \n",
            "Attention Heads: 2, Epoch: 24, Loss: 2.1482, Validation Loss: 1.4459 \n",
            "Attention Heads: 2, Epoch: 25, Loss: 2.1209, Validation Loss: 1.4244 \n",
            "Attention Heads: 2, Epoch: 26, Loss: 2.0969, Validation Loss: 1.4045 \n",
            "Attention Heads: 2, Epoch: 27, Loss: 2.0975, Validation Loss: 1.3806 \n",
            "Attention Heads: 2, Epoch: 28, Loss: 2.0495, Validation Loss: 1.3610 \n",
            "Attention Heads: 2, Epoch: 29, Loss: 1.9980, Validation Loss: 1.3458 \n",
            "Attention Heads: 2, Epoch: 30, Loss: 2.0075, Validation Loss: 1.3308 \n",
            "Attention Heads: 2, Epoch: 31, Loss: 1.9769, Validation Loss: 1.3200 \n",
            "Attention Heads: 2, Epoch: 32, Loss: 1.9625, Validation Loss: 1.3090 \n",
            "Attention Heads: 2, Epoch: 33, Loss: 1.9797, Validation Loss: 1.2908 \n",
            "Attention Heads: 2, Epoch: 34, Loss: 1.9369, Validation Loss: 1.2785 \n",
            "Attention Heads: 2, Epoch: 35, Loss: 1.9443, Validation Loss: 1.2682 \n",
            "Attention Heads: 2, Epoch: 36, Loss: 1.9159, Validation Loss: 1.2569 \n",
            "Attention Heads: 2, Epoch: 37, Loss: 1.8854, Validation Loss: 1.2545 \n",
            "Attention Heads: 2, Epoch: 38, Loss: 1.8843, Validation Loss: 1.2469 \n",
            "Attention Heads: 2, Epoch: 39, Loss: 1.8526, Validation Loss: 1.2357 \n",
            "Attention Heads: 2, Epoch: 40, Loss: 1.8446, Validation Loss: 1.2241 \n",
            "Attention Heads: 2, Epoch: 41, Loss: 1.8443, Validation Loss: 1.2254 \n",
            "Attention Heads: 2, Epoch: 42, Loss: 1.8445, Validation Loss: 1.2241 \n",
            "Attention Heads: 2, Epoch: 43, Loss: 1.8442, Validation Loss: 1.2211 \n",
            "Attention Heads: 2, Epoch: 44, Loss: 1.8366, Validation Loss: 1.2150 \n",
            "Attention Heads: 2, Epoch: 45, Loss: 1.8158, Validation Loss: 1.2089 \n",
            "Attention Heads: 2, Epoch: 46, Loss: 1.7946, Validation Loss: 1.1956 \n",
            "Attention Heads: 2, Epoch: 47, Loss: 1.8128, Validation Loss: 1.1948 \n",
            "Attention Heads: 2, Epoch: 48, Loss: 1.8019, Validation Loss: 1.1865 \n",
            "Attention Heads: 2, Epoch: 49, Loss: 1.7901, Validation Loss: 1.1893 \n",
            "Attention Heads: 2, Epoch: 50, Loss: 1.7715, Validation Loss: 1.1877 \n",
            "Attention Heads: 2, Epoch: 51, Loss: 1.7704, Validation Loss: 1.1844 \n",
            "Attention Heads: 2, Epoch: 52, Loss: 1.7775, Validation Loss: 1.1827 \n",
            "Attention Heads: 2, Epoch: 53, Loss: 1.7510, Validation Loss: 1.1856 \n",
            "Attention Heads: 2, Epoch: 54, Loss: 1.7788, Validation Loss: 1.1649 \n",
            "Attention Heads: 2, Epoch: 55, Loss: 1.7515, Validation Loss: 1.1507 \n",
            "Attention Heads: 2, Epoch: 56, Loss: 1.7403, Validation Loss: 1.1538 \n",
            "Attention Heads: 2, Epoch: 57, Loss: 1.7475, Validation Loss: 1.1579 \n",
            "Attention Heads: 2, Epoch: 58, Loss: 1.7057, Validation Loss: 1.1571 \n",
            "Attention Heads: 2, Epoch: 59, Loss: 1.7153, Validation Loss: 1.1582 \n",
            "Early stopping at epoch 60 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 1.1570 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 4.2558, Validation Loss: 4.0124 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 4.0575, Validation Loss: 3.6993 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 3.8293, Validation Loss: 3.4657 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 3.6259, Validation Loss: 3.1697 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 3.3867, Validation Loss: 2.8553 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 3.2363, Validation Loss: 2.6721 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 3.0241, Validation Loss: 2.4529 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 2.8581, Validation Loss: 2.2234 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 2.7027, Validation Loss: 2.0488 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 2.5610, Validation Loss: 1.8956 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 2.4368, Validation Loss: 1.7596 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 2.3271, Validation Loss: 1.6488 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 2.2716, Validation Loss: 1.5673 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 2.2098, Validation Loss: 1.5017 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 2.1129, Validation Loss: 1.4446 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 2.0625, Validation Loss: 1.3836 \n",
            "Attention Heads: 4, Epoch: 16, Loss: 2.0206, Validation Loss: 1.3365 \n",
            "Attention Heads: 4, Epoch: 17, Loss: 1.9629, Validation Loss: 1.3080 \n",
            "Attention Heads: 4, Epoch: 18, Loss: 1.9317, Validation Loss: 1.2829 \n",
            "Attention Heads: 4, Epoch: 19, Loss: 1.9134, Validation Loss: 1.2613 \n",
            "Attention Heads: 4, Epoch: 20, Loss: 1.8431, Validation Loss: 1.2480 \n",
            "Attention Heads: 4, Epoch: 21, Loss: 1.8369, Validation Loss: 1.2313 \n",
            "Attention Heads: 4, Epoch: 22, Loss: 1.8039, Validation Loss: 1.2091 \n",
            "Attention Heads: 4, Epoch: 23, Loss: 1.7903, Validation Loss: 1.1974 \n",
            "Attention Heads: 4, Epoch: 24, Loss: 1.7570, Validation Loss: 1.1893 \n",
            "Attention Heads: 4, Epoch: 25, Loss: 1.7273, Validation Loss: 1.1790 \n",
            "Attention Heads: 4, Epoch: 26, Loss: 1.7139, Validation Loss: 1.1739 \n",
            "Attention Heads: 4, Epoch: 27, Loss: 1.6883, Validation Loss: 1.1624 \n",
            "Attention Heads: 4, Epoch: 28, Loss: 1.6656, Validation Loss: 1.1664 \n",
            "Attention Heads: 4, Epoch: 29, Loss: 1.6686, Validation Loss: 1.1561 \n",
            "Attention Heads: 4, Epoch: 30, Loss: 1.6370, Validation Loss: 1.1472 \n",
            "Attention Heads: 4, Epoch: 31, Loss: 1.6422, Validation Loss: 1.1515 \n",
            "Attention Heads: 4, Epoch: 32, Loss: 1.6340, Validation Loss: 1.1509 \n",
            "Attention Heads: 4, Epoch: 33, Loss: 1.6366, Validation Loss: 1.1519 \n",
            "Attention Heads: 4, Epoch: 34, Loss: 1.6270, Validation Loss: 1.1457 \n",
            "Attention Heads: 4, Epoch: 35, Loss: 1.5932, Validation Loss: 1.1601 \n",
            "Attention Heads: 4, Epoch: 36, Loss: 1.6033, Validation Loss: 1.1260 \n",
            "Attention Heads: 4, Epoch: 37, Loss: 1.5844, Validation Loss: 1.1254 \n",
            "Attention Heads: 4, Epoch: 38, Loss: 1.5705, Validation Loss: 1.1315 \n",
            "Attention Heads: 4, Epoch: 39, Loss: 1.5517, Validation Loss: 1.1458 \n",
            "Attention Heads: 4, Epoch: 40, Loss: 1.5758, Validation Loss: 1.1296 \n",
            "Attention Heads: 4, Epoch: 41, Loss: 1.5615, Validation Loss: 1.1226 \n",
            "Attention Heads: 4, Epoch: 42, Loss: 1.5375, Validation Loss: 1.1184 \n",
            "Attention Heads: 4, Epoch: 43, Loss: 1.5529, Validation Loss: 1.1329 \n",
            "Attention Heads: 4, Epoch: 44, Loss: 1.5403, Validation Loss: 1.1273 \n",
            "Attention Heads: 4, Epoch: 45, Loss: 1.5122, Validation Loss: 1.1260 \n",
            "Attention Heads: 4, Epoch: 46, Loss: 1.5404, Validation Loss: 1.1247 \n",
            "Attention Heads: 4, Epoch: 47, Loss: 1.5361, Validation Loss: 1.1103 \n",
            "Attention Heads: 4, Epoch: 48, Loss: 1.5190, Validation Loss: 1.1198 \n",
            "Attention Heads: 4, Epoch: 49, Loss: 1.5027, Validation Loss: 1.1300 \n",
            "Attention Heads: 4, Epoch: 50, Loss: 1.4999, Validation Loss: 1.1248 \n",
            "Attention Heads: 4, Epoch: 51, Loss: 1.4976, Validation Loss: 1.1296 \n",
            "Early stopping at epoch 52 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 1.1320 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 4.2578, Validation Loss: 3.9325 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 4.0637, Validation Loss: 3.8401 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 3.8639, Validation Loss: 3.4685 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 3.6483, Validation Loss: 3.1759 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 3.3904, Validation Loss: 2.8400 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 3.0972, Validation Loss: 2.4806 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 2.9015, Validation Loss: 2.2160 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 2.6901, Validation Loss: 1.9897 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 2.4943, Validation Loss: 1.8164 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 2.3873, Validation Loss: 1.6784 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 2.2340, Validation Loss: 1.5762 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 2.1315, Validation Loss: 1.4859 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 2.0617, Validation Loss: 1.4232 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 1.9798, Validation Loss: 1.3685 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 1.9199, Validation Loss: 1.3396 \n",
            "Attention Heads: 8, Epoch: 15, Loss: 1.8518, Validation Loss: 1.2897 \n",
            "Attention Heads: 8, Epoch: 16, Loss: 1.8366, Validation Loss: 1.2497 \n",
            "Attention Heads: 8, Epoch: 17, Loss: 1.7709, Validation Loss: 1.2172 \n",
            "Attention Heads: 8, Epoch: 18, Loss: 1.7475, Validation Loss: 1.1958 \n",
            "Attention Heads: 8, Epoch: 19, Loss: 1.7049, Validation Loss: 1.1963 \n",
            "Attention Heads: 8, Epoch: 20, Loss: 1.6767, Validation Loss: 1.1965 \n",
            "Attention Heads: 8, Epoch: 21, Loss: 1.6586, Validation Loss: 1.1871 \n",
            "Attention Heads: 8, Epoch: 22, Loss: 1.6374, Validation Loss: 1.1671 \n",
            "Attention Heads: 8, Epoch: 23, Loss: 1.5837, Validation Loss: 1.1707 \n",
            "Attention Heads: 8, Epoch: 24, Loss: 1.5792, Validation Loss: 1.1540 \n",
            "Attention Heads: 8, Epoch: 25, Loss: 1.5764, Validation Loss: 1.1594 \n",
            "Attention Heads: 8, Epoch: 26, Loss: 1.5448, Validation Loss: 1.1380 \n",
            "Attention Heads: 8, Epoch: 27, Loss: 1.5244, Validation Loss: 1.1454 \n",
            "Attention Heads: 8, Epoch: 28, Loss: 1.5383, Validation Loss: 1.1532 \n",
            "Attention Heads: 8, Epoch: 29, Loss: 1.5045, Validation Loss: 1.1320 \n",
            "Attention Heads: 8, Epoch: 30, Loss: 1.4858, Validation Loss: 1.1266 \n",
            "Attention Heads: 8, Epoch: 31, Loss: 1.4880, Validation Loss: 1.1420 \n",
            "Attention Heads: 8, Epoch: 32, Loss: 1.4636, Validation Loss: 1.1480 \n",
            "Attention Heads: 8, Epoch: 33, Loss: 1.4863, Validation Loss: 1.1397 \n",
            "Attention Heads: 8, Epoch: 34, Loss: 1.4566, Validation Loss: 1.1417 \n",
            "Early stopping at epoch 35 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 1.1662 \n",
            "\n",
            "Best Number of Attention Heads: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = ThreeLayerGAT_v2(input_dim=CoraFull.num_node_features, hidden_dim=best_hidden_dim , output_dim=CoraFull_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, CoraFull)\n",
        "    val_loss = validate(model, criterion, CoraFull)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, CoraFull)\n",
        "print(f'Test Accuracy(three layer) on CoraFull dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsYiHP6qWLz9",
        "outputId": "8f5a9328-8a99-4b6a-c6f6-71422bafc87c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 50 \n",
            "\n",
            "Test Accuracy(three layer) on CoraFull dataset: 0.6789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and evaluate on Citeseer dataset(three layer)**"
      ],
      "metadata": {
        "id": "_sajPny13CCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "hidden_dims = [16 , 32, 64, 128, 256]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    model = ThreeLayerGAT_v2(input_dim=citeseer.num_node_features, hidden_dim= hidden_dim , output_dim= citeseer_dataset.num_classes , num_heads = 4).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "        # Print the loss\n",
        "        print(f'Hidden Dimension: {hidden_dim}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Hidden Dimension: {hidden_dim}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_hidden_dim = hidden_dim\n",
        "\n",
        "print('Best Hidden Dimension: {}'.format(best_hidden_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d57674-769e-42e5-9c2d-eadfd9358308",
        "id": "M0bdwXQXs2Gy"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Dimension: 16, Epoch: 0, Loss: 1.7866, Validation Loss: 1.6352 \n",
            "Hidden Dimension: 16, Epoch: 1, Loss: 1.6530, Validation Loss: 1.4322 \n",
            "Hidden Dimension: 16, Epoch: 2, Loss: 1.4975, Validation Loss: 1.2163 \n",
            "Hidden Dimension: 16, Epoch: 3, Loss: 1.3310, Validation Loss: 1.0526 \n",
            "Hidden Dimension: 16, Epoch: 4, Loss: 1.2642, Validation Loss: 0.9695 \n",
            "Hidden Dimension: 16, Epoch: 5, Loss: 1.1919, Validation Loss: 0.9299 \n",
            "Hidden Dimension: 16, Epoch: 6, Loss: 1.1570, Validation Loss: 0.9215 \n",
            "Hidden Dimension: 16, Epoch: 7, Loss: 1.1268, Validation Loss: 0.9208 \n",
            "Hidden Dimension: 16, Epoch: 8, Loss: 1.1033, Validation Loss: 0.9187 \n",
            "Hidden Dimension: 16, Epoch: 9, Loss: 1.1043, Validation Loss: 0.9128 \n",
            "Hidden Dimension: 16, Epoch: 10, Loss: 1.0620, Validation Loss: 0.9091 \n",
            "Hidden Dimension: 16, Epoch: 11, Loss: 1.0512, Validation Loss: 0.9052 \n",
            "Hidden Dimension: 16, Epoch: 12, Loss: 1.0243, Validation Loss: 0.9054 \n",
            "Hidden Dimension: 16, Epoch: 13, Loss: 1.0000, Validation Loss: 0.9148 \n",
            "Hidden Dimension: 16, Epoch: 14, Loss: 0.9495, Validation Loss: 0.9222 \n",
            "Hidden Dimension: 16, Epoch: 15, Loss: 1.0041, Validation Loss: 0.9216 \n",
            "Early stopping at epoch 16 \n",
            "\n",
            " Hidden Dimension: 16, Validation Loss: 0.9171 \n",
            "\n",
            "Hidden Dimension: 32, Epoch: 0, Loss: 1.8095, Validation Loss: 1.6305 \n",
            "Hidden Dimension: 32, Epoch: 1, Loss: 1.6471, Validation Loss: 1.3805 \n",
            "Hidden Dimension: 32, Epoch: 2, Loss: 1.4470, Validation Loss: 1.0936 \n",
            "Hidden Dimension: 32, Epoch: 3, Loss: 1.2564, Validation Loss: 0.9494 \n",
            "Hidden Dimension: 32, Epoch: 4, Loss: 1.1504, Validation Loss: 0.9209 \n",
            "Hidden Dimension: 32, Epoch: 5, Loss: 1.1171, Validation Loss: 0.9196 \n",
            "Hidden Dimension: 32, Epoch: 6, Loss: 1.0792, Validation Loss: 0.8895 \n",
            "Hidden Dimension: 32, Epoch: 7, Loss: 1.0429, Validation Loss: 0.8614 \n",
            "Hidden Dimension: 32, Epoch: 8, Loss: 1.0378, Validation Loss: 0.8543 \n",
            "Hidden Dimension: 32, Epoch: 9, Loss: 0.9701, Validation Loss: 0.8660 \n",
            "Hidden Dimension: 32, Epoch: 10, Loss: 0.9406, Validation Loss: 0.8744 \n",
            "Hidden Dimension: 32, Epoch: 11, Loss: 0.9227, Validation Loss: 0.8622 \n",
            "Hidden Dimension: 32, Epoch: 12, Loss: 0.9148, Validation Loss: 0.8640 \n",
            "Early stopping at epoch 13 \n",
            "\n",
            " Hidden Dimension: 32, Validation Loss: 0.8674 \n",
            "\n",
            "Hidden Dimension: 64, Epoch: 0, Loss: 1.8105, Validation Loss: 1.6155 \n",
            "Hidden Dimension: 64, Epoch: 1, Loss: 1.6987, Validation Loss: 1.3297 \n",
            "Hidden Dimension: 64, Epoch: 2, Loss: 1.3901, Validation Loss: 1.0441 \n",
            "Hidden Dimension: 64, Epoch: 3, Loss: 1.2268, Validation Loss: 0.9376 \n",
            "Hidden Dimension: 64, Epoch: 4, Loss: 1.1638, Validation Loss: 0.9391 \n",
            "Hidden Dimension: 64, Epoch: 5, Loss: 1.0804, Validation Loss: 0.9051 \n",
            "Hidden Dimension: 64, Epoch: 6, Loss: 1.0287, Validation Loss: 0.8930 \n",
            "Hidden Dimension: 64, Epoch: 7, Loss: 1.0386, Validation Loss: 0.8978 \n",
            "Hidden Dimension: 64, Epoch: 8, Loss: 1.0098, Validation Loss: 0.8877 \n",
            "Hidden Dimension: 64, Epoch: 9, Loss: 0.9432, Validation Loss: 0.8858 \n",
            "Hidden Dimension: 64, Epoch: 10, Loss: 0.9269, Validation Loss: 0.8915 \n",
            "Hidden Dimension: 64, Epoch: 11, Loss: 0.9283, Validation Loss: 0.8766 \n",
            "Hidden Dimension: 64, Epoch: 12, Loss: 0.9121, Validation Loss: 0.8643 \n",
            "Hidden Dimension: 64, Epoch: 13, Loss: 0.9095, Validation Loss: 0.8640 \n",
            "Hidden Dimension: 64, Epoch: 14, Loss: 0.8793, Validation Loss: 0.8668 \n",
            "Hidden Dimension: 64, Epoch: 15, Loss: 0.8966, Validation Loss: 0.8687 \n",
            "Hidden Dimension: 64, Epoch: 16, Loss: 0.8597, Validation Loss: 0.8703 \n",
            "Early stopping at epoch 17 \n",
            "\n",
            " Hidden Dimension: 64, Validation Loss: 0.8806 \n",
            "\n",
            "Hidden Dimension: 128, Epoch: 0, Loss: 1.7962, Validation Loss: 1.9913 \n",
            "Hidden Dimension: 128, Epoch: 1, Loss: 2.3294, Validation Loss: 1.5369 \n",
            "Hidden Dimension: 128, Epoch: 2, Loss: 1.6637, Validation Loss: 1.4157 \n",
            "Hidden Dimension: 128, Epoch: 3, Loss: 1.4834, Validation Loss: 1.2618 \n",
            "Hidden Dimension: 128, Epoch: 4, Loss: 1.3138, Validation Loss: 1.1272 \n",
            "Hidden Dimension: 128, Epoch: 5, Loss: 1.2244, Validation Loss: 1.0416 \n",
            "Hidden Dimension: 128, Epoch: 6, Loss: 1.1926, Validation Loss: 0.9754 \n",
            "Hidden Dimension: 128, Epoch: 7, Loss: 1.0944, Validation Loss: 0.9271 \n",
            "Hidden Dimension: 128, Epoch: 8, Loss: 1.0643, Validation Loss: 0.9139 \n",
            "Hidden Dimension: 128, Epoch: 9, Loss: 1.0254, Validation Loss: 0.9157 \n",
            "Hidden Dimension: 128, Epoch: 10, Loss: 0.9749, Validation Loss: 0.9221 \n",
            "Hidden Dimension: 128, Epoch: 11, Loss: 0.9217, Validation Loss: 0.9339 \n",
            "Hidden Dimension: 128, Epoch: 12, Loss: 0.8811, Validation Loss: 0.9437 \n",
            "Early stopping at epoch 13 \n",
            "\n",
            " Hidden Dimension: 128, Validation Loss: 0.9485 \n",
            "\n",
            "Hidden Dimension: 256, Epoch: 0, Loss: 1.8121, Validation Loss: 5.6235 \n",
            "Hidden Dimension: 256, Epoch: 1, Loss: 7.3621, Validation Loss: 1.9246 \n",
            "Hidden Dimension: 256, Epoch: 2, Loss: 2.1166, Validation Loss: 1.6458 \n",
            "Hidden Dimension: 256, Epoch: 3, Loss: 1.6950, Validation Loss: 1.5882 \n",
            "Hidden Dimension: 256, Epoch: 4, Loss: 1.5934, Validation Loss: 1.4856 \n",
            "Hidden Dimension: 256, Epoch: 5, Loss: 1.5005, Validation Loss: 1.3494 \n",
            "Hidden Dimension: 256, Epoch: 6, Loss: 1.3977, Validation Loss: 1.2356 \n",
            "Hidden Dimension: 256, Epoch: 7, Loss: 1.3666, Validation Loss: 1.1719 \n",
            "Hidden Dimension: 256, Epoch: 8, Loss: 1.3135, Validation Loss: 1.1297 \n",
            "Hidden Dimension: 256, Epoch: 9, Loss: 1.2682, Validation Loss: 1.0925 \n",
            "Hidden Dimension: 256, Epoch: 10, Loss: 1.1806, Validation Loss: 1.0585 \n",
            "Hidden Dimension: 256, Epoch: 11, Loss: 1.1359, Validation Loss: 1.0206 \n",
            "Hidden Dimension: 256, Epoch: 12, Loss: 1.1101, Validation Loss: 0.9808 \n",
            "Hidden Dimension: 256, Epoch: 13, Loss: 1.0594, Validation Loss: 0.9663 \n",
            "Hidden Dimension: 256, Epoch: 14, Loss: 1.0372, Validation Loss: 0.9745 \n",
            "Hidden Dimension: 256, Epoch: 15, Loss: 1.0204, Validation Loss: 0.9768 \n",
            "Hidden Dimension: 256, Epoch: 16, Loss: 1.0132, Validation Loss: 0.9765 \n",
            "Hidden Dimension: 256, Epoch: 17, Loss: 0.9625, Validation Loss: 0.9734 \n",
            "Early stopping at epoch 18 \n",
            "\n",
            " Hidden Dimension: 256, Validation Loss: 0.9724 \n",
            "\n",
            "Best Hidden Dimension: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **find best number of heads for  citeseer dataset(three layer)**"
      ],
      "metadata": {
        "id": "H-hYd9-eWY9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "num_heads_list = [1, 2, 4 , 8]\n",
        "\n",
        "best_validation_loss = float('inf')\n",
        "best_hidden_dims = None\n",
        "\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "\n",
        "    model = ThreeLayerGAT_v2(input_dim= citeseer.num_node_features ,hidden_dim=best_hidden_dim , output_dim= citeseer_dataset.num_classes , num_heads = num_heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    min_delta = 0.001\n",
        "    patience_counter = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, optimizer, criterion, citeseer)\n",
        "        val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "        # Check for improvement in validation loss\n",
        "        if val_loss < best_loss - min_delta:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch} \\n')\n",
        "                break\n",
        "\n",
        "\n",
        "        print(f'Attention Heads: {num_heads}, Epoch: {epoch}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} ')\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "\n",
        "    print(f' Attention Heads: {num_heads}, Validation Loss: {validation_loss:.4f} \\n')\n",
        "    # Check if this number of attention heads is the best so far\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        best_num_heads = num_heads\n",
        "\n",
        "print('Best Number of Attention Heads: {}'.format(best_num_heads))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354501a8-e6c6-4669-bace-1d99d6f6e268",
        "id": "FulOmEvrWY9n"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Heads: 1, Epoch: 0, Loss: 1.8107, Validation Loss: 1.6966 \n",
            "Attention Heads: 1, Epoch: 1, Loss: 1.7179, Validation Loss: 1.5796 \n",
            "Attention Heads: 1, Epoch: 2, Loss: 1.6169, Validation Loss: 1.4568 \n",
            "Attention Heads: 1, Epoch: 3, Loss: 1.5220, Validation Loss: 1.3314 \n",
            "Attention Heads: 1, Epoch: 4, Loss: 1.4583, Validation Loss: 1.2249 \n",
            "Attention Heads: 1, Epoch: 5, Loss: 1.4576, Validation Loss: 1.1446 \n",
            "Attention Heads: 1, Epoch: 6, Loss: 1.3632, Validation Loss: 1.0893 \n",
            "Attention Heads: 1, Epoch: 7, Loss: 1.3762, Validation Loss: 1.0514 \n",
            "Attention Heads: 1, Epoch: 8, Loss: 1.3484, Validation Loss: 1.0279 \n",
            "Attention Heads: 1, Epoch: 9, Loss: 1.2946, Validation Loss: 1.0098 \n",
            "Attention Heads: 1, Epoch: 10, Loss: 1.2634, Validation Loss: 0.9864 \n",
            "Attention Heads: 1, Epoch: 11, Loss: 1.2650, Validation Loss: 0.9606 \n",
            "Attention Heads: 1, Epoch: 12, Loss: 1.2452, Validation Loss: 0.9374 \n",
            "Attention Heads: 1, Epoch: 13, Loss: 1.2247, Validation Loss: 0.9233 \n",
            "Attention Heads: 1, Epoch: 14, Loss: 1.2169, Validation Loss: 0.9127 \n",
            "Attention Heads: 1, Epoch: 15, Loss: 1.2222, Validation Loss: 0.9031 \n",
            "Attention Heads: 1, Epoch: 16, Loss: 1.2217, Validation Loss: 0.8959 \n",
            "Attention Heads: 1, Epoch: 17, Loss: 1.1704, Validation Loss: 0.8879 \n",
            "Attention Heads: 1, Epoch: 18, Loss: 1.1899, Validation Loss: 0.8822 \n",
            "Attention Heads: 1, Epoch: 19, Loss: 1.1519, Validation Loss: 0.8777 \n",
            "Attention Heads: 1, Epoch: 20, Loss: 1.1945, Validation Loss: 0.8750 \n",
            "Attention Heads: 1, Epoch: 21, Loss: 1.1651, Validation Loss: 0.8730 \n",
            "Attention Heads: 1, Epoch: 22, Loss: 1.1367, Validation Loss: 0.8716 \n",
            "Attention Heads: 1, Epoch: 23, Loss: 1.1538, Validation Loss: 0.8710 \n",
            "Attention Heads: 1, Epoch: 24, Loss: 1.1179, Validation Loss: 0.8701 \n",
            "Attention Heads: 1, Epoch: 25, Loss: 1.1249, Validation Loss: 0.8653 \n",
            "Attention Heads: 1, Epoch: 26, Loss: 1.1413, Validation Loss: 0.8606 \n",
            "Attention Heads: 1, Epoch: 27, Loss: 1.1452, Validation Loss: 0.8544 \n",
            "Attention Heads: 1, Epoch: 28, Loss: 1.1166, Validation Loss: 0.8512 \n",
            "Attention Heads: 1, Epoch: 29, Loss: 1.1178, Validation Loss: 0.8481 \n",
            "Attention Heads: 1, Epoch: 30, Loss: 1.1182, Validation Loss: 0.8471 \n",
            "Attention Heads: 1, Epoch: 31, Loss: 1.0857, Validation Loss: 0.8487 \n",
            "Attention Heads: 1, Epoch: 32, Loss: 1.0646, Validation Loss: 0.8497 \n",
            "Attention Heads: 1, Epoch: 33, Loss: 1.0866, Validation Loss: 0.8545 \n",
            "Early stopping at epoch 34 \n",
            "\n",
            " Attention Heads: 1, Validation Loss: 0.8591 \n",
            "\n",
            "Attention Heads: 2, Epoch: 0, Loss: 1.8012, Validation Loss: 1.6658 \n",
            "Attention Heads: 2, Epoch: 1, Loss: 1.6843, Validation Loss: 1.4855 \n",
            "Attention Heads: 2, Epoch: 2, Loss: 1.5425, Validation Loss: 1.2749 \n",
            "Attention Heads: 2, Epoch: 3, Loss: 1.4064, Validation Loss: 1.0957 \n",
            "Attention Heads: 2, Epoch: 4, Loss: 1.2948, Validation Loss: 0.9767 \n",
            "Attention Heads: 2, Epoch: 5, Loss: 1.2520, Validation Loss: 0.9203 \n",
            "Attention Heads: 2, Epoch: 6, Loss: 1.1805, Validation Loss: 0.9077 \n",
            "Attention Heads: 2, Epoch: 7, Loss: 1.1698, Validation Loss: 0.8943 \n",
            "Attention Heads: 2, Epoch: 8, Loss: 1.1098, Validation Loss: 0.8753 \n",
            "Attention Heads: 2, Epoch: 9, Loss: 1.0979, Validation Loss: 0.8576 \n",
            "Attention Heads: 2, Epoch: 10, Loss: 1.0921, Validation Loss: 0.8417 \n",
            "Attention Heads: 2, Epoch: 11, Loss: 1.0372, Validation Loss: 0.8407 \n",
            "Attention Heads: 2, Epoch: 12, Loss: 1.0777, Validation Loss: 0.8409 \n",
            "Attention Heads: 2, Epoch: 13, Loss: 1.0529, Validation Loss: 0.8382 \n",
            "Attention Heads: 2, Epoch: 14, Loss: 1.0368, Validation Loss: 0.8336 \n",
            "Attention Heads: 2, Epoch: 15, Loss: 1.0027, Validation Loss: 0.8272 \n",
            "Attention Heads: 2, Epoch: 16, Loss: 0.9817, Validation Loss: 0.8236 \n",
            "Attention Heads: 2, Epoch: 17, Loss: 0.9934, Validation Loss: 0.8208 \n",
            "Attention Heads: 2, Epoch: 18, Loss: 0.9916, Validation Loss: 0.8204 \n",
            "Attention Heads: 2, Epoch: 19, Loss: 1.0097, Validation Loss: 0.8198 \n",
            "Attention Heads: 2, Epoch: 20, Loss: 0.9329, Validation Loss: 0.8264 \n",
            "Attention Heads: 2, Epoch: 21, Loss: 0.9554, Validation Loss: 0.8340 \n",
            "Early stopping at epoch 22 \n",
            "\n",
            " Attention Heads: 2, Validation Loss: 0.8373 \n",
            "\n",
            "Attention Heads: 4, Epoch: 0, Loss: 1.7938, Validation Loss: 1.6069 \n",
            "Attention Heads: 4, Epoch: 1, Loss: 1.6604, Validation Loss: 1.4087 \n",
            "Attention Heads: 4, Epoch: 2, Loss: 1.4685, Validation Loss: 1.1421 \n",
            "Attention Heads: 4, Epoch: 3, Loss: 1.2625, Validation Loss: 0.9458 \n",
            "Attention Heads: 4, Epoch: 4, Loss: 1.1397, Validation Loss: 0.9037 \n",
            "Attention Heads: 4, Epoch: 5, Loss: 1.1215, Validation Loss: 0.8862 \n",
            "Attention Heads: 4, Epoch: 6, Loss: 1.1003, Validation Loss: 0.8888 \n",
            "Attention Heads: 4, Epoch: 7, Loss: 1.0595, Validation Loss: 0.8879 \n",
            "Attention Heads: 4, Epoch: 8, Loss: 1.0412, Validation Loss: 0.8755 \n",
            "Attention Heads: 4, Epoch: 9, Loss: 0.9834, Validation Loss: 0.8761 \n",
            "Attention Heads: 4, Epoch: 10, Loss: 1.0109, Validation Loss: 0.8566 \n",
            "Attention Heads: 4, Epoch: 11, Loss: 0.9670, Validation Loss: 0.8508 \n",
            "Attention Heads: 4, Epoch: 12, Loss: 0.9243, Validation Loss: 0.8526 \n",
            "Attention Heads: 4, Epoch: 13, Loss: 0.9475, Validation Loss: 0.8549 \n",
            "Attention Heads: 4, Epoch: 14, Loss: 0.9764, Validation Loss: 0.8572 \n",
            "Attention Heads: 4, Epoch: 15, Loss: 0.9022, Validation Loss: 0.8630 \n",
            "Early stopping at epoch 16 \n",
            "\n",
            " Attention Heads: 4, Validation Loss: 0.8747 \n",
            "\n",
            "Attention Heads: 8, Epoch: 0, Loss: 1.7998, Validation Loss: 1.5643 \n",
            "Attention Heads: 8, Epoch: 1, Loss: 1.6389, Validation Loss: 1.3293 \n",
            "Attention Heads: 8, Epoch: 2, Loss: 1.4269, Validation Loss: 1.0510 \n",
            "Attention Heads: 8, Epoch: 3, Loss: 1.2214, Validation Loss: 0.9191 \n",
            "Attention Heads: 8, Epoch: 4, Loss: 1.0767, Validation Loss: 0.9273 \n",
            "Attention Heads: 8, Epoch: 5, Loss: 1.0410, Validation Loss: 0.9453 \n",
            "Attention Heads: 8, Epoch: 6, Loss: 1.0355, Validation Loss: 0.9169 \n",
            "Attention Heads: 8, Epoch: 7, Loss: 1.0349, Validation Loss: 0.8934 \n",
            "Attention Heads: 8, Epoch: 8, Loss: 0.9425, Validation Loss: 0.8842 \n",
            "Attention Heads: 8, Epoch: 9, Loss: 0.9746, Validation Loss: 0.8714 \n",
            "Attention Heads: 8, Epoch: 10, Loss: 0.9496, Validation Loss: 0.8658 \n",
            "Attention Heads: 8, Epoch: 11, Loss: 0.8598, Validation Loss: 0.8651 \n",
            "Attention Heads: 8, Epoch: 12, Loss: 0.8648, Validation Loss: 0.8680 \n",
            "Attention Heads: 8, Epoch: 13, Loss: 0.8721, Validation Loss: 0.8695 \n",
            "Attention Heads: 8, Epoch: 14, Loss: 0.8218, Validation Loss: 0.8697 \n",
            "Early stopping at epoch 15 \n",
            "\n",
            " Attention Heads: 8, Validation Loss: 0.8745 \n",
            "\n",
            "Best Number of Attention Heads: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Train the best model\n",
        "model = ThreeLayerGAT_v2(input_dim=citeseer.num_node_features, hidden_dim=best_hidden_dim , output_dim=citeseer_dataset.num_classes, num_heads=best_num_heads).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "min_delta = 0.001\n",
        "patience_counter = 0\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, criterion, citeseer)\n",
        "    val_loss = validate(model, criterion, citeseer)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch} \\n')\n",
        "            break\n",
        "\n",
        "# Evaluate the model\n",
        "test_accuracy = test(model, criterion, citeseer)\n",
        "print(f'Test Accuracy(three layer) on citeseer dataset: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSI6zJYaWY9o",
        "outputId": "4e42fd40-9c05-426c-d93b-a11f06380e1d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 20 \n",
            "\n",
            "Test Accuracy(three layer) on citeseer dataset: 0.7323\n"
          ]
        }
      ]
    }
  ]
}